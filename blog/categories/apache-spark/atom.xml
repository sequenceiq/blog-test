<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Apache Spark | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/apache-spark/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-01-07T14:28:40+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark on Tez execution context - running in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez/"/>
    <updated>2014-11-02T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez</id>
    <content type="html"><![CDATA[<p>Last week Hortonworks <a href="http://hortonworks.com/blog/improving-spark-data-pipelines-native-yarn-integration/">announced</a> improvements for running Apache Spark at scale by introducing a new pluggable <code>execution context</code> and has <a href="https://github.com/hortonworks/spark-native-yarn-samples">open sourced</a> it.</p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are always trying to work and offer the latest technology solutions for our clients and help them to choose their favorite technology/option. We are running a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; to be open sourced soon &ndash; with the goal (among many others) to abstract and allow our customers to use their favorite big data runtime: MR2, Spark or Tez. Along this process we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well (all of these containers have made <strong>top</strong> downloads on the official Docker repository). For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>We have always been big fans on Apache Spark &ndash; due to the simplicity of development and at the same time we are big fans of Apache Tez, for reasons we have <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">blogged before</a>.</p>

<p>When the <a href="https://issues.apache.org/jira/browse/SPARK-3561">SPARK-3561</a> has been submitted we were eager to get our hands on the WIP and early implementation &ndash; and this time we&rsquo;d like to help you with a quick ramp-up and easy solution to have a Spark Docker <a href="https://github.com/sequenceiq/docker-spark-native-yarn">container</a> where the <code>execution context</code> has been changed to <a href="http://tez.apache.org/">Apache Tez</a> and everything is preconfigured. The only thing you will need to do is to follow these easy steps.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/spark-native-yarn
</code></p>

<p>Once you have pulled the container you are ready to run the image.</p>

<h3>Run the image</h3>

<p><code>
docker run -i -t -h sandbox sequenceiq/spark-native-yarn /etc/bootstrap.sh -bash
</code></p>

<!-- more -->


<p>You have now a fully configured Apache Spark, where the <code>execution context</code> is <a href="http://tez.apache.org/">Apache Tez</a>.</p>

<h3>Test the container</h3>

<p>We have pushed sample data and tests from the <a href="https://github.com/hortonworks/spark-native-yarn-samples">code repository</a> into the Docker container, thus you can start experimenting right away without writing one line of code.</p>

<h4>Calculate PI</h4>

<p>Simplest example to test with is the <code>PI calculation</code>.</p>

<p><code>
cd /usr/local/spark
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-examples-1.1.0.2.1.5.0-702-hadoop2.4.0.2.1.5.0-695.jar
</code></p>

<p>You should expect something like the following as the result:
<code>
Pi is roughly 3.14668
</code></p>

<h4>Run a KMeans example</h4>

<p>Run the <code>KMeans</code> example using the sample dataset.</p>

<p><code>
./bin/spark-submit --class sample.KMeans --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/kmeans_data.txt
</code></p>

<p>You should expect something like the following as the result:
<code>
Finished iteration (delta = 0.0)
Final centers:
DenseVector(0.15000000000000002, 0.15000000000000002, 0.15000000000000002)
DenseVector(9.2, 9.2, 9.2)
DenseVector(0.0, 0.0, 0.0)
DenseVector(9.05, 9.05, 9.05)
</code></p>

<h4>Other examples (Join, Partition By, Source count, Word count)</h4>

<p>Join
<code>
./bin/spark-submit --class sample.Join --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/join1.txt /sample-data/join2.txt
</code>
Partition By
<code>
./bin/spark-submit --class sample.PartitionBy --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/partitioning.txt
</code>
Source count
<code>
./bin/spark-submit --class sample.SourceCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt
</code>
Word count
<code>
./bin/spark-submit --class sample.WordCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt 1
</code>
Note that the last argument (1) is the number of <code>reducers</code>.</p>

<h3>Using the Spark Shell</h3>

<p>The Spark shell works out of the box with the new Tez <code>executor context</code>, the only thing you will need to do is run:</p>

<p><code>
./bin/spark-shell --master execution-context:org.apache.spark.tez.TezJobExecutionContext
</code></p>

<h3>Summary</h3>

<p>Right after the next day that <a href="https://github.com/hortonworks/spark-native-yarn-samples">SPARK-3561</a> has been made available we have started to test at scale using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and run performance tests by using the same Spark jobs developed in Banzai (over 50 individual jobs) using the same input sets, cluster size and Scala code &ndash; but changing the default <code>Spark context</code> to a <code>Tez context</code>. Follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> as we will release these test results and the lessons we have learned in the coming weeks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark 1.1.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker/"/>
    <updated>2014-09-17T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker</id>
    <content type="html"><![CDATA[<p>As you might be already familiar, we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well. For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>In this current post we’d like to help you to start with the <code>latest - 1.1.0</code> Spark release in minutes &ndash; using Docker. Docker and Spark are two technologies which are very <code>hyped</code> these days. At <a href="http://sequenceiq.com/">SequenceIQ</a> we use both quite a lot, thus we put together a Docker container and sharing it with the community.</p>

<p>The container’s code is available in our <a href="https://github.com/sequenceiq/docker-spark/blob/v1.1onHadoop-2.5.1/README.md">GitHub</a> repository.</p>

<h3>Pull the image from Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/spark:1.1.0
</code></p>

<!-- more -->


<h2>Building the image</h2>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<p><code>
docker build --rm -t sequenceiq/spark:1.1.0 .
</code></p>

<h2>Running the image</h2>

<p>Once you have pulled or built the container, you are ready to start with Spark.</p>

<p><code>
docker run -i -t -h sandbox sequenceiq/spark /etc/bootstrap.sh -bash
</code></p>

<h3>Testing</h3>

<p>In order to check whether everything is OK, you can run one of the stock examples, coming with Spark. Check our previous blog posts and examples about Spark <a href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/">here</a> and <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">here</a>.</p>

<p>```
cd /usr/local/spark</p>

<h1>run the spark shell</h1>

<p>./bin/spark-shell &mdash;master yarn-client &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1</p>

<h1>execute the the following command which should return 1000</h1>

<p>scala> sc.parallelize(1 to 1000).count()
```</p>

<p>There are two deploy modes that can be used to launch Spark applications on YARN. In yarn-cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>

<p>Estimating Pi (yarn-cluster mode):</p>

<p>```
cd /usr/local/spark</p>

<h1>execute the the following command which should write the &ldquo;Pi is roughly 3.1418&rdquo; into the logs</h1>

<p>./bin/spark-submit &mdash;class org.apache.spark.examples.SparkPi &mdash;master yarn-cluster &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar
```</p>

<p>Estimating Pi (yarn-client mode):</p>

<p>```
cd /usr/local/spark</p>

<h1>execute the the following command which should print the &ldquo;Pi is roughly 3.1418&rdquo; to the screen</h1>

<p>./bin/spark-submit &mdash;class org.apache.spark.examples.SparkPi &mdash;master yarn-client &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar
```</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
