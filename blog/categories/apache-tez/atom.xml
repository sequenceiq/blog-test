<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Apache Tez | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/apache-tez/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-16T14:59:00+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[TopK on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/"/>
    <updated>2014-09-23T17:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez</id>
    <content type="html"><![CDATA[<p>The Apache Tez community draw attention last week with their latest release <a href="http://tez.apache.org/releases/0.5.0/release-notes.txt">0.5.0</a>
of the application framework. At SequenceIQ we always try to find and provide the best solutions to our customers and share the experience we gain by
being involved in many open source Apache projects. We are always looking for the latest innovations, and try to apply them to our projects.
For a while we have been working hard on a new project called
<a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> which we&rsquo;ll open source in the near future. One handy feature of the projects is the ability to run the same pipes on <code>MR2</code>, <code>Spark</code> and <code>Tez</code> &ndash; your choice.
In the next couple of posts we&rsquo;ll compare these runtimes using different jobs and as the first example to implement we chose TopK. Before going into
details let&rsquo;s revisit what Apache Tez is made of.</p>

<h2>Apache Tez key concepts</h2>

<ul>
<li>One of the most important feature is that there is no heavy deployment phase which otherwise could go wrong in many ways &ndash; probably sounds familiar
for most of us. There is a nice <a href="http://tez.apache.org/install.html">install guide</a> on the project&rsquo;s page which you can follow, but basically
you have to copy a bunch of jars to HDFS and you&rsquo;re almost good to go.</li>
<li>Multiple versions of Tez can be used at the same time which solves a common problem, the rolling upgrades.</li>
<li>Distributed data processing jobs typically look like <code>DAGs</code> (directed acyclic graphs) and Tez relies on this concept to define your jobs.
DAGs are made from <code>Vertices</code> and <code>Edges</code>. Vertices in the graph represent data transformations while edges represent the data movement
from producers to consumers. The DAG itself defines the structure of the data processing and the relationship between producers and consumers.</li>
</ul>


<p>Tez provides faster execution and higher predictability because:</p>

<ul>
<li>Eliminates replicated write barriers between successive computations</li>
<li>Eliminates the job launch overhead</li>
<li>Eliminates the extra stage of map reads in every workflow job</li>
<li>Provides better locality</li>
<li>Capable to re-use containers which reduces the scheduling time and speeds up incredibly the short running tasks</li>
<li>Can share in-memory data across tasks</li>
<li>Can run multiple DAGs in one session</li>
<li>The core engine can be customized (vertex manager, DAG scheduler, task scheduler)</li>
<li>Provides an event mechanism to communicate between tasks (data movement events to inform consumers by the data location)</li>
</ul>


<p>If you&rsquo;d like to try Tez on a fully functional multi-node cluster we put together an Ambari based Docker image. Click
<a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a> for details.</p>

<!-- more -->


<h2>TopK</h2>

<p>The goal is to find the top K elements of a dataset. In this example&rsquo;s case is a simple CSV and we&rsquo;re looking for the top elements in a given column.
In order to do that we need to <code>group</code> and <code>sort</code> them to <code>take</code> the K elements. The implementation can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a> repository. The important part starts
with the <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L136">DAG creation</a>.
```java</p>

<pre><code>    DataSourceDescriptor dataSource = MRInput.createConfigBuilder(new Configuration(tezConf),
            TextInputFormat.class, inputPath).build();

    DataSinkDescriptor dataSink = MROutput.createConfigBuilder(new Configuration(tezConf),
            TextOutputFormat.class, outputPath).build();

    Vertex tokenizerVertex = Vertex.create(TOKENIZER,
            ProcessorDescriptor.create(TokenProcessor.class.getName())
                    .setUserPayload(createPayload(Integer.valueOf(columnIndex))))
            .addDataSource(INPUT, dataSource);

    int topK = Integer.valueOf(top);
    Vertex sumVertex = Vertex.create(SUM,
            ProcessorDescriptor.create(SumProcessor.class.getName())
                    .setUserPayload(createPayload(topK)), Integer.valueOf(numPartitions));

    Vertex writerVertex = Vertex.create(WRITER,
            ProcessorDescriptor.create(Writer.class.getName())
                    .setUserPayload(createPayload(topK)), 1)
            .addDataSink(OUTPUT, dataSink);

    OrderedPartitionedKVEdgeConfig tokenSumEdge = OrderedPartitionedKVEdgeConfig
            .newBuilder(Text.class.getName(), IntWritable.class.getName(),
                    HashPartitioner.class.getName()).build();

    UnorderedKVEdgeConfig sumWriterEdge = UnorderedKVEdgeConfig
            .newBuilder(IntWritable.class.getName(), Text.class.getName()).build();
</code></pre>

<p><code>
First of all we define a `DataSourceDescriptor` which represents our dataset and a `DataSinkDescriptor` where we'll
write the results to. As you can see there are plenty of utility classes to help you define your DAGs. Now that the input and output is
ready let's define our `Vertices`. You'll see the actual data transformation is really easy as Hadoop will take care of the heavy
lifting. The first Vertex is a
[tokenizer](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L198)
which does nothing more than splitting the rows of the CSV and emit a record with the selected column as the key and `1` as the value.
</code>java</p>

<pre><code>@Override
public void initialize() throws Exception {
  byte[] payload = getContext().getUserPayload().deepCopyAsArray();
  ByteArrayInputStream bis = new ByteArrayInputStream(payload);
  DataInputStream dis = new DataInputStream(bis);
  columnIndex = dis.readInt();
  dis.close();
  bis.close();
}
@Override
public void run() throws Exception {
  KeyValueWriter kvWriter = (KeyValueWriter) getOutputs().get(WRITER).getWriter();
  KeyValuesReader kvReader = (KeyValuesReader) getInputs().get(TOKENIZER).getReader();
  while (kvReader.next()) {
    Text word = (Text) kvReader.getCurrentKey();
    int sum = 0;
    for (Object value : kvReader.getCurrentValues()) {
      sum += ((IntWritable) value).get();
    }
    kvWriter.write(new IntWritable(sum), word);
  }
}
</code></pre>

<p><code>
The interesting part here is the `initialize` method which reads the `UserPayload` to find out in which column we're looking for
the top K elements. What happens after the first Vertex is that Hadoop will `group` the records by key, so we'll have all the keys
with a bunch of 1s. In the next Vertex we
[sum](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L243)
these values so we'll have all the words in the given column counted. We could emit all the values to make Hadoop sort them for us,
but we can optimize this process by reducing the data we need to send over the network. And how we are going to do that? If we
[maintain](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L340)
a `local` top K of the data which the sum tasks process and emit only these values all we have left in the last Vertex is to
select the top K results from a much smaller data set. Another important improvement here is that we can use the `UnorderedKVEdgeConfig`
to avoid sorting the data at the task output and the merge sort at the next tasks input. We used the `OrderedPartitionedKVEdgeConfig`
between the first 2 Vertices to demonstrate the different edges and as partition can be a huge asset.
</code>java</p>

<pre><code>    @Override
    public void run() throws Exception {
        Preconditions.checkArgument(getInputs().size() == 1);
        Preconditions.checkArgument(getOutputs().size() == 1);
        KeyValueWriter kvWriter = (KeyValueWriter) getOutputs().get(WRITER).getWriter();
        KeyValuesReader kvReader = (KeyValuesReader) getInputs().get(TOKENIZER).getReader();
        while (kvReader.next()) {
            Text currentWord = (Text) kvReader.getCurrentKey();
            int sum = 0;
            for (Object value : kvReader.getCurrentValues()) {
                sum += ((IntWritable) value).get();
            }
            localTop.store(sum, currentWord.toString());
        }

        Map&lt;Integer, List&lt;String&gt;&gt; result = localTop.getTopK();
        for (int top : result.keySet()) {
            IntWritable topWritable = new IntWritable(top);
            for (String string : result.get(top)) {
                word.set(string);
                kvWriter.write(topWritable, word);
            }
        }
    }
</code></pre>

<p><code>
All we have left is to [take](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L318)
the first K elements of the reduced data set (from the local top Ks) and write it to HDFS and we're done. Except that we have to
define the data movements with edges.
</code>java</p>

<pre><code>DAG dag = DAG.create("topk");
dag
    .addVertex(tokenizerVertex)
    .addVertex(sumVertex)
    .addVertex(writerVertex)
    .addEdge(Edge.create(tokenizerVertex, sumVertex, tokenSumEdge.createDefaultEdgeProperty()))
    .addEdge(Edge.create(sumVertex, writerVertex, sumWriterEdge.createDefaultBroadcastEdgeProperty()));
</code></pre>

<p>```
The execution of this DAG looks something like this:</p>

<p><img src="http://yuml.me/b6bf74a3" alt="" /></p>

<p>In the last Vertex we start collecting the grouped sorted data so we can take the first K elements. This part kills the parallelism as
we need to see the global picture here, that&rsquo;s why you can see that the parallelism is
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L129">set</a> to <code>1</code>.
Leaving the parallelism undefined or -1 results that the number of launched tasks will be determined by the <code>ApplicationMaster</code>.</p>

<h3>TopK DataGen</h3>

<p>You also can generate an arbitrary size of dataset with the
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDataGen.java">TopKDataGen</a>
job. This is a special DAG which has only 1 Vertex and no Edges.</p>

<h3>How to run the examples</h3>

<p>First of all you will need a Tez cluster &ndash; we have put together a real one, you can get it from <a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a>. Pull the container, and follow the instructions below.</p>

<p>Build the project <code>mvn clean install</code> which will generate a jar. Copy this jar to HDFS and you are good to go. In order to make this jar
runnable we also created a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDriver.java">driver</a>
class.
<code>
hadoop jar tez-topk-1.0.jar topkgen /data 1000000
hadoop jar tez-topk-1.0.jar topk /data /result 0 10
</code></p>

<h2>What&rsquo;s next</h2>

<p>In the next post we&rsquo;ll see how we can achieve the same with Spark and we&rsquo;ll do a performance comparison on a large dataset.
Cascading also works on the Tez integration, so we&rsquo;ll definitely report on that too.
If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Tez cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/"/>
    <updated>2014-09-19T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster</id>
    <content type="html"><![CDATA[<p>This week the <a href="http://tez.apache.org/">Apache Tez</a> community announced the release of the 0.5 version of the project. At <a href="http://sequenceiq.com/">SequenceIQ</a> first time we came across Tez was in 2013 &ndash; after <a href="http://hortonworks.com/">Hortonworks</a> launched the <code>Stinger Initiative</code>. Though we were not using Hive (that might change soon) we have quickly realized the <code>other</code> capabilities of Tez &ndash; the expressive data flow API, data movement patterns, dynamic graph reconfiguration, etc &ndash; to name a few.</p>

<p>We quickly became <code>fans</code> of Tez &ndash; and have started to run internal PoC projects, rewrite ML algorithms and legacy MR2 code to run/leverage Tez. The new release comes with a stable developer API and a proven stability track, and this has triggered a <code>major</code> re-architecture/refactoring project at SequenceIQ. While I don’t want to enter into deep details, we are building a Platform as a Service API &ndash; with the first stages of the project already released, open sourced and in public beta:</p>

<p><a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; our Docker based cloud agnostic Hadoop as a Service API (AWS, Azure, Google Cloud, DigitalOcean);
<a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; an SLA policy based autoscaling API for Hadoop YARN</p>

<p>One of the unreleased component is a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; a big data pipeline API (with 50+ pre-built data and job pipes), running on <strong>MR2, Tez and Spark</strong>.</p>

<p>With all these said, we have put together a <code>Tez Ready</code> Docker based Hadoop cluster to share our excitement and allow you to quickly start and get familiar with the nice features of the Tez API. The cluster is built on our widely used Apache Ambari Docker <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">container</a>, with some additional features. The containers are <code>service discovery</code> aware. You don’t need to setup anything beforehand, configure IP addresses or DNS names &ndash; the only thing you will need to do is just specify the number of nodes desired in your cluster, and you are ready to go. If you are interested on the underlying architecture (using Docker, Serf and dnsmasq) you can check my slides/presentation from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit</a>.</p>

<p>I&rsquo;d like to highlight one important feature of Tez &ndash; us being crazy about automation/DevOps &ndash; the simplicity and the capability of running multiple versions of Tez on the same YARN cluster. We are contributors to many Apache projects (Hadoop, YARN, Ambari, etc) and since we have started to use Tez we consider to contribute there as well (at the end of the day will be a core part of our platform). Adding new features, changing code or fixing bugs always introduce undesired <code>features</code> &ndash; nevertheless, the Tez binaries built by different colleagues can be tested at scale, using the same cluster without affecting each others work. Check Gopal V&rsquo;s good <a href="http://bit.ly/tez-devops">introduction</a> about Tez and DevOps.</p>

<h2>Apache Tez cluster on Docker</h2>

<p>The container’s code is available on our <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea-tez">GitHub</a> repository.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/ambari:1.7.0-ea-tez
</code></p>

<!-- more -->


<h3>Building the image</h3>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<p><code>
docker build --rm -t sequenceiq/ambari:1.7.0-ea-tez ambari-server/
</code></p>

<h2>Running the cluster</h2>

<p>We have put together a few shell functions to simplify your work, so before you start make sure you get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea-tez/ambari-functions">file</a>.</p>

<p><code>
curl -Lo .amb j.mp/docker-ambari-tez &amp;&amp; . .amb
</code></p>

<h3>Create your Apache Tez cluster</h3>

<p>You are almost there. The only thing you will need to do is to specify the number of nodes you need in your cluster. We will launch the containers, they will dynamically join the cluster and apply the Tez specific configurations.</p>

<p><code>
amb-deploy-cluster 4
</code></p>

<p>Once the cluster is started you can <a href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/">enter</a> in the container and submit your custom Tez application or use one of the stock Tez examples.</p>

<p>Check back next week, as we are releasing <code>real world</code> examples running on three different big data fabrics: Tez, MR2 and Spark.</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
