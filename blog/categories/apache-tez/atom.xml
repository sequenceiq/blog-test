<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Apache Tez | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/apache-tez/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-11-17T13:58:28+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark on Tez execution context - running in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez/"/>
    <updated>2014-11-02T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez</id>
    <content type="html"><![CDATA[<p>Last week Hortonworks <a href="http://hortonworks.com/blog/improving-spark-data-pipelines-native-yarn-integration/">announced</a> improvements for running Apache Spark at scale by introducing a new pluggable <code>execution context</code> and has <a href="https://github.com/hortonworks/spark-native-yarn-samples">open sourced</a> it.</p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are always trying to work and offer the latest technology solutions for our clients and help them to choose their favorite technology/option. We are running a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; to be open sourced soon &ndash; with the goal (among many others) to abstract and allow our customers to use their favorite big data runtime: MR2, Spark or Tez. Along this process we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well (all of these containers have made <strong>top</strong> downloads on the official Docker repository). For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>We have always been big fans on Apache Spark &ndash; due to the simplicity of development and at the same time we are big fans of Apache Tez, for reasons we have <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">blogged before</a>.</p>

<p>When the <a href="https://issues.apache.org/jira/browse/SPARK-3561">SPARK-3561</a> has been submitted we were eager to get our hands on the WIP and early implementation &ndash; and this time we&rsquo;d like to help you with a quick ramp-up and easy solution to have a Spark Docker <a href="https://github.com/sequenceiq/docker-spark-native-yarn">container</a> where the <code>execution context</code> has been changed to <a href="http://tez.apache.org/">Apache Tez</a> and everything is preconfigured. The only thing you will need to do is to follow these easy steps.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/spark-native-yarn
</code></p>

<p>Once you have pulled the container you are ready to run the image.</p>

<h3>Run the image</h3>

<p><code>
docker run -i -t -h sandbox sequenceiq/spark-native-yarn /etc/bootstrap.sh -bash
</code></p>

<!-- more -->


<p>You have now a fully configured Apache Spark, where the <code>execution context</code> is <a href="http://tez.apache.org/">Apache Tez</a>.</p>

<h3>Test the container</h3>

<p>We have pushed sample data and tests from the <a href="https://github.com/hortonworks/spark-native-yarn-samples">code repository</a> into the Docker container, thus you can start experimenting right away without writing one line of code.</p>

<h4>Calculate PI</h4>

<p>Simplest example to test with is the <code>PI calculation</code>.</p>

<p><code>
cd /usr/local/spark
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-examples-1.1.0.2.1.5.0-702-hadoop2.4.0.2.1.5.0-695.jar
</code></p>

<p>You should expect something like the following as the result:
<code>
Pi is roughly 3.14668
</code></p>

<h4>Run a KMeans example</h4>

<p>Run the <code>KMeans</code> example using the sample dataset.</p>

<p><code>
./bin/spark-submit --class sample.KMeans --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/kmeans_data.txt
</code></p>

<p>You should expect something like the following as the result:
<code>
Finished iteration (delta = 0.0)
Final centers:
DenseVector(0.15000000000000002, 0.15000000000000002, 0.15000000000000002)
DenseVector(9.2, 9.2, 9.2)
DenseVector(0.0, 0.0, 0.0)
DenseVector(9.05, 9.05, 9.05)
</code></p>

<h4>Other examples (Join, Partition By, Source count, Word count)</h4>

<p>Join
<code>
./bin/spark-submit --class sample.Join --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/join1.txt /sample-data/join2.txt
</code>
Partition By
<code>
./bin/spark-submit --class sample.PartitionBy --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/partitioning.txt
</code>
Source count
<code>
./bin/spark-submit --class sample.SourceCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt
</code>
Word count
<code>
./bin/spark-submit --class sample.WordCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt 1
</code>
Note that the last argument (1) is the number of <code>reducers</code>.</p>

<h3>Using the Spark Shell</h3>

<p>The Spark shell works out of the box with the new Tez <code>executor context</code>, the only thing you will need to do is run:</p>

<p><code>
./bin/spark-shell --master execution-context:org.apache.spark.tez.TezJobExecutionContext
</code></p>

<h3>Summary</h3>

<p>Right after the next day that <a href="https://github.com/hortonworks/spark-native-yarn-samples">SPARK-3561</a> has been made available we have started to test at scale using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and run performance tests by using the same Spark jobs developed in Banzai (over 50 individual jobs) using the same input sets, cluster size and Scala code &ndash; but changing the default <code>Spark context</code> to a <code>Tez context</code>. Follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> as we will release these test results and the lessons we have learned in the coming weeks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cascading on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez/"/>
    <updated>2014-10-20T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez</id>
    <content type="html"><![CDATA[<p>In one of our previous <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">posts</a> we showed how to do a TopK using directly the Apache Tez API. In this post we’d like to show how to do a similarly complex algorithm with Cascading &ndash; running on Apache Tez.
At <a href="http://sequenceiq.com">SequenceIQ</a> we use Scalding, Cascading and Spark to write most of our jobs. For a while our big data pipeline API called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> offers a unified API over different runtimes: MR2, Spark and Tez; recently Cascading has announced support for Apache Tez and we’d like to show you that by writing a detailed example.</p>

<h2>Cascading Application &ndash; GroupBy, Each, Every</h2>

<p>Cascading data flows are to be constructed from Source taps (input), Sink taps (output) and Pipes.
At first, we have to setup our properties for the Cascading flow.</p>

<p>``` java</p>

<pre><code>    Properties properties = AppProps.appProps()
            .setJarClass(Main.class)
            .buildProperties();

    properties = FlowRuntimeProps.flowRuntimeProps()
            .setGatherPartitions(1)
            .buildProperties(properties);
</code></pre>

<p>```</p>

<p>Then in order to use Apache Tez, setup the Tez specific <code>Flow Connector</code>.</p>

<!-- more -->


<p><code>java
FlowConnector flowConnector = new Hadoop2TezFlowConnector(properties);
</code></p>

<p>After that we do the algorithm part of the flow. We need an input and output which comes as command-line arguments.
We are going to work on CSV files for the sake of simplicity, so we will use the <code>TextDelimited</code> scheme. Also we need to define our input pipe and taps (<code>source/sink</code>).
Suppose that we want to count the occurrences of users and keep them only if they occur more than once. We can compute this with 2 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N205A3">GroupBy</a>, 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Every</a> and 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Each</a> operation.
First, we group by user ids (count them with every operation), then in the second grouping we need to sort on the whole data set (by <code>count</code>) and use the <a href="http://docs.cascading.org/cascading/2.5/javadoc/cascading/operation/Filter.html">Filter</a> operation to remove the unneeded lines. (here we grouping by <code>Fields.NONE</code>, that means we take all data into 1 group, in other words we force to use 1 reducer)</p>

<p>``` java</p>

<pre><code>    final String inputPath = args[0];
    final String outputPath = args[1];

    final Fields fields = new Fields("userId", "data1", "data2", "data3");
    final Scheme scheme = new TextDelimited(fields, false, true, ",");

    final Pipe inPipe = new Pipe("inPipe");
    final Tap inTap = new Hfs(scheme, inputPath);
    final Fields groupFields = new Fields("userId");

    Pipe usersPipe = new GroupBy("usersWithCount", inPipe, groupFields);
    usersPipe = new Every(usersPipe, groupFields, new Count(), Fields.ALL);
    usersPipe = new GroupBy(usersPipe, Fields.NONE, new Fields("count", "userId"), true);
    usersPipe = new Each(usersPipe, new Fields("count"), new RegexFilter( "^(?:[2-9]|(?:[1-9][0-9]+))" ));

    final Fields resultFields = new Fields("userId", "count");
    final Scheme outputScheme = new TextDelimited(resultFields, false, true, ",");
    Tap sinkTap = new Hfs(outputScheme, outputPath);
</code></pre>

<p>```</p>

<p>Finally, setup the flow:</p>

<p>``` java</p>

<pre><code>    FlowDef flowDef = FlowDef.flowDef()
            .setName("Cascading-TEZ")
            .addSource(inPipe, inTap)
            .addTailSink(usersPipe, sinkTap);

    Flow flow = flowConnector.connect(flowDef);
    flow.complete();
</code></pre>

<p>```</p>

<p>As you can see the codebase is a bit simpler than using directly the Apache Tez API, however you loose the low level features of the expressive data flow API. Basically it&rsquo;s up to the personal preference of a developer whether to use and build directly on top of the Tez API or use Cascading (we have our own internal debate among colleagues) &ndash; as Apache Tez improves the performance by multiple times.</p>

<p>Get the code from our GitHub repository <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> and build the project inside the <code>cascading-tez-sample</code> directory:</p>

<p><code>bash
./gradlew clean build
</code>
Once your jar is ready upload it onto a Tez cluster and run the following command:
<code>bash
hadoop jar cascading-tez-sample-1.0.jar /input /output
</code></p>

<p>Sample data can be generated in the same way as in <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez">this</a> example.</p>

<p>We have put together a Tez enabled Docker container, you can get it from <a href="https://github.com/sequenceiq/docker-tez">here</a>. Pull the container, and follow the instructions.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TopK on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/"/>
    <updated>2014-09-23T17:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez</id>
    <content type="html"><![CDATA[<p>The Apache Tez community draw attention last week with their latest release <a href="http://tez.apache.org/releases/0.5.0/release-notes.txt">0.5.0</a>
of the application framework. At SequenceIQ we always try to find and provide the best solutions to our customers and share the experience we gain by
being involved in many open source Apache projects. We are always looking for the latest innovations, and try to apply them to our projects.
For a while we have been working hard on a new project called
<a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> which we&rsquo;ll open source in the near future. One handy feature of the projects is the ability to run the same pipes on <code>MR2</code>, <code>Spark</code> and <code>Tez</code> &ndash; your choice.
In the next couple of posts we&rsquo;ll compare these runtimes using different jobs and as the first example to implement we chose TopK. Before going into
details let&rsquo;s revisit what Apache Tez is made of.</p>

<h2>Apache Tez key concepts</h2>

<ul>
<li>One of the most important feature is that there is no heavy deployment phase which otherwise could go wrong in many ways &ndash; probably sounds familiar
for most of us. There is a nice <a href="http://tez.apache.org/install.html">install guide</a> on the project&rsquo;s page which you can follow, but basically
you have to copy a bunch of jars to HDFS and you&rsquo;re almost good to go.</li>
<li>Multiple versions of Tez can be used at the same time which solves a common problem, the rolling upgrades.</li>
<li>Distributed data processing jobs typically look like <code>DAGs</code> (directed acyclic graphs) and Tez relies on this concept to define your jobs.
DAGs are made from <code>Vertices</code> and <code>Edges</code>. Vertices in the graph represent data transformations while edges represent the data movement
from producers to consumers. The DAG itself defines the structure of the data processing and the relationship between producers and consumers.</li>
</ul>


<p>Tez provides faster execution and higher predictability because:</p>

<ul>
<li>Eliminates replicated write barriers between successive computations</li>
<li>Eliminates the job launch overhead</li>
<li>Eliminates the extra stage of map reads in every workflow job</li>
<li>Provides better locality</li>
<li>Capable to re-use containers which reduces the scheduling time and speeds up incredibly the short running tasks</li>
<li>Can share in-memory data across tasks</li>
<li>Can run multiple DAGs in one session</li>
<li>The core engine can be customized (vertex manager, DAG scheduler, task scheduler)</li>
<li>Provides an event mechanism to communicate between tasks (data movement events to inform consumers by the data location)</li>
</ul>


<p>If you&rsquo;d like to try Tez on a fully functional multi-node cluster we put together an Ambari based Docker image. Click
<a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a> for details.</p>

<!-- more -->


<h2>TopK</h2>

<p>The goal is to find the top K elements of a dataset. In this example&rsquo;s case is a simple CSV and we&rsquo;re looking for the top elements in a given column.
In order to do that we need to <code>group</code> and <code>sort</code> them to <code>take</code> the K elements. The implementation can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a> repository. The important part starts
with the <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L136">DAG creation</a>.
```java</p>

<pre><code>    DataSourceDescriptor dataSource = MRInput.createConfigBuilder(new Configuration(tezConf),
            TextInputFormat.class, inputPath).build();

    DataSinkDescriptor dataSink = MROutput.createConfigBuilder(new Configuration(tezConf),
            TextOutputFormat.class, outputPath).build();

    Vertex tokenizerVertex = Vertex.create(TOKENIZER,
            ProcessorDescriptor.create(TokenProcessor.class.getName())
                    .setUserPayload(createPayload(Integer.valueOf(columnIndex))))
            .addDataSource(INPUT, dataSource);

    int topK = Integer.valueOf(top);
    Vertex sumVertex = Vertex.create(SUM,
            ProcessorDescriptor.create(SumProcessor.class.getName())
                    .setUserPayload(createPayload(topK)), Integer.valueOf(numPartitions));

    Vertex writerVertex = Vertex.create(WRITER,
            ProcessorDescriptor.create(Writer.class.getName())
                    .setUserPayload(createPayload(topK)), 1)
            .addDataSink(OUTPUT, dataSink);

    OrderedPartitionedKVEdgeConfig tokenSumEdge = OrderedPartitionedKVEdgeConfig
            .newBuilder(Text.class.getName(), IntWritable.class.getName(),
                    HashPartitioner.class.getName()).build();

    UnorderedKVEdgeConfig sumWriterEdge = UnorderedKVEdgeConfig
            .newBuilder(IntWritable.class.getName(), Text.class.getName()).build();
</code></pre>

<p><code>
First of all we define a `DataSourceDescriptor` which represents our dataset and a `DataSinkDescriptor` where we'll
write the results to. As you can see there are plenty of utility classes to help you define your DAGs. Now that the input and output is
ready let's define our `Vertices`. You'll see the actual data transformation is really easy as Hadoop will take care of the heavy
lifting. The first Vertex is a
[tokenizer](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L198)
which does nothing more than splitting the rows of the CSV and emit a record with the selected column as the key and `1` as the value.
</code>java</p>

<pre><code>@Override
public void initialize() throws Exception {
  byte[] payload = getContext().getUserPayload().deepCopyAsArray();
  ByteArrayInputStream bis = new ByteArrayInputStream(payload);
  DataInputStream dis = new DataInputStream(bis);
  columnIndex = dis.readInt();
  dis.close();
  bis.close();
}
@Override
public void run() throws Exception {
  KeyValueWriter kvWriter = (KeyValueWriter) getOutputs().get(WRITER).getWriter();
  KeyValuesReader kvReader = (KeyValuesReader) getInputs().get(TOKENIZER).getReader();
  while (kvReader.next()) {
    Text word = (Text) kvReader.getCurrentKey();
    int sum = 0;
    for (Object value : kvReader.getCurrentValues()) {
      sum += ((IntWritable) value).get();
    }
    kvWriter.write(new IntWritable(sum), word);
  }
}
</code></pre>

<p><code>
The interesting part here is the `initialize` method which reads the `UserPayload` to find out in which column we're looking for
the top K elements. What happens after the first Vertex is that Hadoop will `group` the records by key, so we'll have all the keys
with a bunch of 1s. In the next Vertex we
[sum](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L243)
these values so we'll have all the words in the given column counted. We could emit all the values to make Hadoop sort them for us,
but we can optimize this process by reducing the data we need to send over the network. And how we are going to do that? If we
[maintain](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L340)
a `local` top K of the data which the sum tasks process and emit only these values all we have left in the last Vertex is to
select the top K results from a much smaller data set. Another important improvement here is that we can use the `UnorderedKVEdgeConfig`
to avoid sorting the data at the task output and the merge sort at the next tasks input. We used the `OrderedPartitionedKVEdgeConfig`
between the first 2 Vertices to demonstrate the different edges and as partition can be a huge asset.
</code>java</p>

<pre><code>    @Override
    public void run() throws Exception {
        Preconditions.checkArgument(getInputs().size() == 1);
        Preconditions.checkArgument(getOutputs().size() == 1);
        KeyValueWriter kvWriter = (KeyValueWriter) getOutputs().get(WRITER).getWriter();
        KeyValuesReader kvReader = (KeyValuesReader) getInputs().get(TOKENIZER).getReader();
        while (kvReader.next()) {
            Text currentWord = (Text) kvReader.getCurrentKey();
            int sum = 0;
            for (Object value : kvReader.getCurrentValues()) {
                sum += ((IntWritable) value).get();
            }
            localTop.store(sum, currentWord.toString());
        }

        Map&lt;Integer, List&lt;String&gt;&gt; result = localTop.getTopK();
        for (int top : result.keySet()) {
            IntWritable topWritable = new IntWritable(top);
            for (String string : result.get(top)) {
                word.set(string);
                kvWriter.write(topWritable, word);
            }
        }
    }
</code></pre>

<p><code>
All we have left is to [take](https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L318)
the first K elements of the reduced data set (from the local top Ks) and write it to HDFS and we're done. Except that we have to
define the data movements with edges.
</code>java</p>

<pre><code>DAG dag = DAG.create("topk");
dag
    .addVertex(tokenizerVertex)
    .addVertex(sumVertex)
    .addVertex(writerVertex)
    .addEdge(Edge.create(tokenizerVertex, sumVertex, tokenSumEdge.createDefaultEdgeProperty()))
    .addEdge(Edge.create(sumVertex, writerVertex, sumWriterEdge.createDefaultBroadcastEdgeProperty()));
</code></pre>

<p>```
The execution of this DAG looks something like this:</p>

<p><img src="http://yuml.me/b6bf74a3" alt="" /></p>

<p>In the last Vertex we start collecting the grouped sorted data so we can take the first K elements. This part kills the parallelism as
we need to see the global picture here, that&rsquo;s why you can see that the parallelism is
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L129">set</a> to <code>1</code>.
Leaving the parallelism undefined or -1 results that the number of launched tasks will be determined by the <code>ApplicationMaster</code>.</p>

<h3>TopK DataGen</h3>

<p>You also can generate an arbitrary size of dataset with the
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDataGen.java">TopKDataGen</a>
job. This is a special DAG which has only 1 Vertex and no Edges.</p>

<h3>How to run the examples</h3>

<p>First of all you will need a Tez cluster &ndash; we have put together a real one, you can get it from <a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a>. Pull the container, and follow the instructions below.</p>

<p>Build the project <code>mvn clean install</code> which will generate a jar. Copy this jar to HDFS and you are good to go. In order to make this jar
runnable we also created a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDriver.java">driver</a>
class.
<code>
hadoop jar tez-topk-1.0.jar topkgen /data 1000000
hadoop jar tez-topk-1.0.jar topk /data /result 0 10
</code></p>

<h2>What&rsquo;s next</h2>

<p>In the next post we&rsquo;ll see how we can achieve the same with Spark and we&rsquo;ll do a performance comparison on a large dataset.
Cascading also works on the Tez integration, so we&rsquo;ll definitely report on that too.
If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Tez cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/"/>
    <updated>2014-09-19T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster</id>
    <content type="html"><![CDATA[<p>This week the <a href="http://tez.apache.org/">Apache Tez</a> community announced the release of the 0.5 version of the project. At <a href="http://sequenceiq.com/">SequenceIQ</a> first time we came across Tez was in 2013 &ndash; after <a href="http://hortonworks.com/">Hortonworks</a> launched the <code>Stinger Initiative</code>. Though we were not using Hive (that might change soon) we have quickly realized the <code>other</code> capabilities of Tez &ndash; the expressive data flow API, data movement patterns, dynamic graph reconfiguration, etc &ndash; to name a few.</p>

<p>We quickly became <code>fans</code> of Tez &ndash; and have started to run internal PoC projects, rewrite ML algorithms and legacy MR2 code to run/leverage Tez. The new release comes with a stable developer API and a proven stability track, and this has triggered a <code>major</code> re-architecture/refactoring project at SequenceIQ. While I don’t want to enter into deep details, we are building a Platform as a Service API &ndash; with the first stages of the project already released, open sourced and in public beta:</p>

<p><a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; our Docker based cloud agnostic Hadoop as a Service API (AWS, Azure, Google Cloud, DigitalOcean);
<a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; an SLA policy based autoscaling API for Hadoop YARN</p>

<p>One of the unreleased component is a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; a big data pipeline API (with 50+ pre-built data and job pipes), running on <strong>MR2, Tez and Spark</strong>.</p>

<p>With all these said, we have put together a <code>Tez Ready</code> Docker based Hadoop cluster to share our excitement and allow you to quickly start and get familiar with the nice features of the Tez API. The cluster is built on our widely used Apache Ambari Docker <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">container</a>, with some additional features. The containers are <code>service discovery</code> aware. You don’t need to setup anything beforehand, configure IP addresses or DNS names &ndash; the only thing you will need to do is just specify the number of nodes desired in your cluster, and you are ready to go. If you are interested on the underlying architecture (using Docker, Serf and dnsmasq) you can check my slides/presentation from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit</a>.</p>

<p>I&rsquo;d like to highlight one important feature of Tez &ndash; us being crazy about automation/DevOps &ndash; the simplicity and the capability of running multiple versions of Tez on the same YARN cluster. We are contributors to many Apache projects (Hadoop, YARN, Ambari, etc) and since we have started to use Tez we consider to contribute there as well (at the end of the day will be a core part of our platform). Adding new features, changing code or fixing bugs always introduce undesired <code>features</code> &ndash; nevertheless, the Tez binaries built by different colleagues can be tested at scale, using the same cluster without affecting each others work. Check Gopal V&rsquo;s good <a href="http://bit.ly/tez-devops">introduction</a> about Tez and DevOps.</p>

<h2>Apache Tez cluster on Docker</h2>

<p>The container’s code is available on our <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea-tez">GitHub</a> repository.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/ambari:1.7.0-ea-tez
</code></p>

<!-- more -->


<h3>Building the image</h3>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<p><code>
docker build --rm -t sequenceiq/ambari:1.7.0-ea-tez ambari-server/
</code></p>

<h2>Running the cluster</h2>

<p>We have put together a few shell functions to simplify your work, so before you start make sure you get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea-tez/ambari-functions">file</a>.</p>

<p><code>
curl -Lo .amb j.mp/docker-ambari-tez &amp;&amp; . .amb
</code></p>

<h3>Create your Apache Tez cluster</h3>

<p>You are almost there. The only thing you will need to do is to specify the number of nodes you need in your cluster. We will launch the containers, they will dynamically join the cluster and apply the Tez specific configurations.</p>

<p><code>
amb-deploy-cluster 4
</code></p>

<p>Once the cluster is started you can <a href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/">enter</a> in the container and submit your custom Tez application or use one of the stock Tez examples.</p>

<p>Check back next week, as we are releasing <code>real world</code> examples running on three different big data fabrics: Tez, MR2 and Spark.</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
