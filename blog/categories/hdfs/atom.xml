<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HDFS | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hdfs/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-01-07T14:28:40+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data cleaning with MapReduce and Morphlines]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/"/>
    <updated>2014-03-11T09:21:07+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines</id>
    <content type="html"><![CDATA[<p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<p><code>
userID  movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
75      3       1       29       10           2006      23          17          16
75      32      4.5     29       10           2006      23          23          44
75      110     4       29       10           2006      23          30          8
75      160     2       29       10           2006      23          16          52
75      163     4       29       10           2006      23          29          30
75      165     4.5     29       10           2006      23          25          15
75      173     3.5     29       10           2006      23          17          37
</code></p>

<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<p>```
{</p>

<pre><code>readCSV {
    separator : "\t"
    columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
    ignoreFirstLine : false
    trim : true
    charset : UTF-8
}
</code></pre>

<p>}</p>

<p>{</p>

<pre><code>java {
  imports : "import java.util.*;"
  code: """
      record.removeAll("date_hour");
      record.removeAll("date_minute");
      record.removeAll("date_second");
      return child.process(record);
}
</code></pre>

<p>}
```</p>

<!-- more -->


<p>Now lets create our mapper only job to process the data. What we need to do is build the Morphlines command chain by parsing the configuration file as shown</p>

<p><code>java protected void setup(Context context)
File morphLineFile = new File(context.getConfiguration().get(MORPHLINE_FILE));
String morphLineId = context.getConfiguration().get(MORPHLINE_ID);
RecordEmitter recordEmitter = new RecordEmitter(context);
MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
Command morphline = new org.kitesdk.morphline.base.Compiler().compile(morphLineFile, morphLineId, morphlineContext, recordEmitter);
</code>
and pass the lines through it.
```java protected void map(Object key, Text value, Context context)
Record record = new Record()
record.put(Fields.ATTACHMENT_BODY, new ByteArrayInputStream(value.toString().getBytes()));
if (!morphline.process(record)) {</p>

<pre><code>LOGGER.info("Morphline failed to process record: {}", record);
</code></pre>

<p>}
record.removeAll(Fields.ATTACHMENT_BODY);
<code>
Notice that the compile method takes an important parameter called finalChild which is in our example the `RecordEmitter`.
The returned command will feed records into finalChild which means if this parameter is not provided a DropRecord command will
be assigned automatically. In Apache Flume there is a Collector command to avoid loosing any transformed record.
The only thing left is to outbox the processed record and write the results to HDFS. The RecordEmitter will serve this purpose:
</code>java
@Override
public boolean process(Record record) {</p>

<pre><code>line.set(record.toString());
</code></pre>

<p>  try {</p>

<pre><code>context.write(line, null);
</code></pre>

<p>  } catch (Exception e) {</p>

<pre><code>  LOGGER.warn("Cannot write record to context", e);
</code></pre>

<p>  }
  return true;
}
<code>
By default the readCSV command extracts the ATTACHMENT_BODY into headers with id provided in the columns field so the
transformed data will look like this (3 columns were dropped):
</code>
{date_day=[10], date_month=[10], date_year=[2008], movieID=[62049], rating=[4.5], userID=[71534]}
```
The source code is available in our samples repository on <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a>.
It is just a simple example but you can go further and download a much bigger dataset with 10 millions of lines and process it with multiple nodes to see how it scales.</p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS and java.nio.channels]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs/"/>
    <updated>2014-03-07T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs</id>
    <content type="html"><![CDATA[<p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<p>```
java BufferedInputStream bufferedInputStream</p>

<p>/<em>*
 * For the sake of readability, try/cacth/finally blocks are removed
 * Don&rsquo;t Say We Didn&rsquo;t Warn You
 </em>/</p>

<p>FileSystem fs = FileSystem.get(configuration);</p>

<pre><code>        Path filePath = getFilePath(dataPath);
</code></pre>

<p>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));</p>

<pre><code>listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
                    CsvPreference.STANDARD_PREFERENCE);
</code></pre>

<p>```
The exception looks like this:</p>

<p>```
ERROR SimpleFeatureSelector:67 &ndash; Exception {}
java.lang.IllegalStateException: Must not use direct buffers with InputStream API</p>

<pre><code>at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)
</code></pre>

<p>```</p>

<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<!-- more -->


<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<p>``` java</p>

<p>@SuppressWarnings(&ldquo;deprecation&rdquo;)
public static BlockReader newBlockReader(</p>

<pre><code>                                 Conf conf,
                             Socket sock, String file,
                                 ExtendedBlock block, 
                                 Token&lt;BlockTokenIdentifier&gt; blockToken,
                                 long startOffset, long len,
                                 int bufferSize, boolean verifyChecksum,
                                 String clientName)
                                 throws IOException {
if (conf.useLegacyBlockReader) {
  return RemoteBlockReader.newBlockReader(
      sock, file, block, blockToken, startOffset, len, bufferSize, verifyChecksum, clientName);
} else {
  return RemoteBlockReader2.newBlockReader(
      sock, file, block, blockToken, startOffset, len, bufferSize, verifyChecksum, clientName);      
}
</code></pre>

<p>  }</p>

<p>```</p>

<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
