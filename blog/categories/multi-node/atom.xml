<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Multi-node | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/multi-node/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-16T14:59:00+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multi-node Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/"/>
    <updated>2014-06-19T20:29:10+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In the <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">previous post</a>
you saw how easy is to create a single-node Hadoop <em>cluster</em> on your devbox.</p>

<p>Now lets raise the bar and create a multinode Hadoop cluster on Docker. Before we
start, make sure you have the latest Ambari image:</p>

<p><code>
docker pull sequenceiq/ambari:1.6.0
</code></p>

<h2>One-liner</h2>

<p>Once you have the latest image, you can start running Docker containers.
But instead of typing long commands like <code>docker run [options] image [command]</code>,
we have created a couple of <a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-functions">shell functions</a> to help you with Docker commands.</p>

<p>Using these functions the impatient can provision a 3 node Hadoop cluster with this one-liner:
<code>
curl -Lo .amb j.mp/docker-ambari &amp;&amp; . .amb &amp;&amp; amb-deploy-cluster
</code></p>

<!-- more -->


<p>Note that you can always alter the default parameters as the blueprint, cluster size, etc &hellip; check the shell <code>j.mp/docker-ambari</code> function&rsquo;s head for the parameters list.</p>

<p>It does the following steps:</p>

<ul>
<li>runs <code>ambari-server start</code> in a daemon Docker (background) container (and also an <code>ambari-agent start</code>)</li>
<li>runs <code>n-1</code> daemon containers with <code>ambari-agent start</code> connecting to the server</li>
<li>runs AmbariShell with attached terminal (to see provision progress)

<ul>
<li>AmbariShell will post the built-in multi-node blueprint to <code>/api/v1/blueprints</code> REST API</li>
<li>AmbariShell auto-assign hosts to host_groups defined in the blueprint</li>
<li>creates a cluster, by posting to the <code>/api/v1/clusters</code> REST API</li>
</ul>
</li>
</ul>


<h2>Custom blueprint</h2>

<p>If you have your own blueprint, put it into a <a href="https://gist.github.com/">gist</a>
and you can use it from AmbariShell. First start AmbariShell:
<code>
amb-start-cluster 2
amb-shell
</code></p>

<p>AmbariShell will wait for:</p>

<ul>
<li>Ambari REST API
Below you will see a happy path to create a multi node Hadoop cluster using the AmbariShell.</li>
</ul>


<p><code>
host list
blueprint add --url https://gist.githubusercontent.com/lalyos/xxx/raw/custum-blueprint.json
cluster build --blueprint custom-blueprint
cluster assign --hostGroup host_group_1 --host amb0.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster create
</code></p>

<p>In AmbariShell the <code>hint</code> command will always guide you on the happy path,
and remember that devops are lazy, so instead of typing press <code>&lt;TAB&gt;</code> for autocomplete or suggestions.</p>

<p>Autocomplete will help you to:</p>

<ul>
<li>complete the command in the given context (e.g. without any blueprint, cluster commands are not available)</li>
<li>add required parameters</li>
<li>add optional parameters: press tab after double dash <code>--&lt;TAB&gt;</code></li>
<li>complete parameter arguments, such as blueprint names, hostnames &hellip;</li>
</ul>


<h2>Summary</h2>

<p>Ever since we started to use Docker we are always developing against a multi-node
Hadoop cluster &ndash; as running a 3-4 node cluster in a laptop actually has less overhead
than working on a Sandbox VM.</p>

<p>We are <em>Dockerizing</em> the Hadoop ecosystem and simplifying the provisioning
process &ndash; watch this space or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>
for the latest news about <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; the
open source cloud agnostic <em>Hadoop as a Service</em> API built on Docker.</p>

<p>Hope this helps and simplifies your development process &ndash; let us know how it goes
for you or if you need any help with Hadoop on Docker.</p>
]]></content>
  </entry>
  
</feed>
