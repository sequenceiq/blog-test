<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-01T03:12:34+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Managing database upgrades with Liquibase and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process/"/>
    <updated>2014-09-26T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are <em>crazy</em> about automating everything &ndash; let it be the provisioning of a thousand nodes Hadoop
cluster using <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> or a simple database change.
We apply the same automated CI/CD process to all our projects, including plain old RDBMS schema changes &hellip; yes, though we are a <em>big data</em> technology
company sometimes we do use JPA as well.</p>

<p>As applications evolve their underlying data model change. New functionalities often need data model changes and the initial design
needs to be adapted to the ever changing demands. These changes usually are of two types : structural changes
(e.g.: addition/removal of tables, columns, constraints etc &hellip;)
and migration of the existing data to the new version of the datamodel.
As the data model gets more and more complex  &ndash; this will happen in-spite of trying to keep it as simple as possible &ndash;
the complexity of these tasks grow proportionally. This happens here at SequenceIQ too; the post is about how we address some of these problems.</p>

<!-- more -->


<h2>Directives</h2>

<ul>
<li>We need a process to follow each time such changes arise</li>
<li>Use appropriate tools that do the job (instead of reinventing the wheel)</li>
<li>Make the process <strong>automated</strong> as much as possible</li>
</ul>


<h3>The process</h3>

<p>The process &ndash; as the common sense suggests &ndash; could be split in the following steps:</p>

<ul>
<li>start from the initial version of the database (the version in production)</li>
<li>perform changes required by the new version of the application</li>
<li>capture and store differences between the two versions of the database</li>
<li>(automatically) apply changes to the initial database version</li>
<li>perform tests</li>
<li>apply changes to production</li>
</ul>


<h3>Tools</h3>

<ul>
<li>Dockerized (PostgreSQL) database</li>
<li>Dockerized Liquibase</li>
<li>Jenkins</li>
</ul>


<h3>Implementation</h3>

<h4>Start from the initial version of the database</h4>

<p>To start with, you need a database that&rsquo;s (structurally) identical with the production one. There are several ways to achieve this;we use Postgres and try to
keep it simple, so here&rsquo;s what we do:</p>

<ul>
<li>we always have a QA database which is identical with the production (obliviously the data is not the same)</li>
<li>we make a copy of the <em>data</em> folder of the postgres installation into an arbitrary location on the host</li>
<li>we pass it as a volume to a Docker container running Postgres</li>
</ul>


<p>We run the following command each time we need a fresh database:</p>

<p><code>
docker run -d \
  --name $CONTAINER_NAME \
  -v /$WORKING_DIR/data:/data \
  -p 5432:5432 \
  -e "USER=$DB_USER" \
  -e "PASS=$DB_PASS" \
  -e "DB=$DB_NAME" \
paintedfox/postgresql
</code></p>

<p>where the passed in variables are the following:</p>

<ul>
<li>CONTAINER_NAME &ndash; the name of the database Docker container</li>
<li>DB_USER &ndash; the database user name</li>
<li>DB_PASS &ndash; the database password</li>
<li>DB_NAME &ndash; the database schema</li>
</ul>


<p>We have a running database now (in less than a minute) &ndash; same as the prod; we can connect to it with a client on your localhost, port 5432 with the given username/password.</p>

<h4>Perform changes required by the new version of the application</h4>

<p>As expected, this is the most challenging part in the process: changes need to be implemented and also
captured so that they can be applied any time (preferably in an <strong>automated</strong> way)
As we&rsquo;re using JPA (with Hibernate as JPA provider) incremental structural changes are executed with the
SchemaUpdate tool. This can be done during the application startup or using <em>ant</em> or <em>maven</em>.
As we continuously test the application we choose to start the application configured to update the database based on the
changed data model (annotations). Alternatively we could regenerate the whole schema. (See the SchemaUpdate tool documentation:
<a href="http://docs.jboss.org/hibernate/core/3.6/reference/en-US/html/toolsetguide.html">here</a>)</p>

<p>At this point we have a database that aligns with the new version of the application. Please note here, that only <code>incremental</code> changes have been applied to
the database till now, meaning that for example new fields have been added,
 but old/deprecated fields haven&rsquo;t been deleted.</p>

<p>Other type of scripts need to be implemented manually:</p>

<ul>
<li><p>changes that couldn&rsquo;t be performed by the SchemaUpdate tool, such as cleanup (SQL) scripts. This being done, differences till this phase
can be automatically generated by running the Liquibase Docker container &ndash; see the next section. Differences are stored under version control,
in form of <em>Liquibase changelogs</em></p></li>
<li><p>data migration scripts, that adapt the existing data to the new structure. Think of cases
when for example a field becomes a new entity and instead of a value you need to store a reference to the new entity. We store these kind of scripts along
with the generated diff files under version control in form of <em>Liquibase changelogs</em></p></li>
</ul>


<h4>Dockerized Liquibase</h4>

<p>Speaking of tools, we found that <a href="http://www.liquibase.org/index.html">Liquibase</a> addresses many of our requirements, such as</p>

<ul>
<li>track database changes (changes being stored in VCS)</li>
<li>automatically generate diffs between two versions of the database</li>
<li>automatically update a database based on changelogs</li>
</ul>


<p>We have created a docker image with a <em>liquibase</em> installation. You can find the project <a href="https://github.com/sequenceiq/docker-liquibase">here</a></p>

<p>The image can be built locally with the command:</p>

<p><code>
docker build -t sequenceiq/docker-liquibase .
</code></p>

<p>or from the project root, or pulled from the Docker repository:</p>

<p><code>
docker pull sequenceiq/docker-liquibase
</code></p>

<p>Containers built from this image can be used to perform <em>liquibase</em> operations on any host.
This saves us a lot of time by having the installation and configuration shipped and helps us to automate most of the tasks.
You can use the container for performing liquibase tasks manually in a terminal, or you can start the container to
automatically perform specific tasks (and quit eventually). To start the container linked to the previously started database
container and perform manual operations, run:</p>

<p><code>
docker run -it \
--name $LIQUIBASE_CONTAINER \
--link $DB_CONTAINER:db \
--entrypoint /bin/bash \
-v /$LIQUIBASE_CHANGELOGS:/changelogs \
$LIQUIBASE_DOCKER_IMAGE \
/bin/bash
</code></p>

<p>See the description of the variables:</p>

<ul>
<li>LIQUIBASE_CONTAINER the name of the Liquibase Docker container</li>
<li>DB_CONTAINER the name of the database container the Liquibase container is to be linked to</li>
<li>LIQUIBASE_CHANGELOGS the folder holding the liquibase changelogs (Liquibase will read and write here)</li>
<li>LIQUIBASE_DOCKER_IMAGE the name of the dockerized Liquibase Docker image</li>
</ul>


<p>Some of the Liquibase tasks can be scripted. We scripted the diff generation and changelog application. Liquibase offers more advanced features too.</p>

<h4>Testing</h4>

<p>We write tests that can be run automatically to check the process. Each <code>changeset</code>, especially those related to data migration / transformation is covered.</p>

<h4>Apply liquibase changelogs to the production database</h4>

<p>After the application is tested upon applying the database changes &ndash; that ensures that changelogs are correct, it&rsquo;s easy to set up a <code>jenkins</code> job that:</p>

<ul>
<li>checks out the proper version of changelogs</li>
<li>starts a docker container linked to the (production) database and applies changelogs</li>
</ul>


<p>Obviously this step needs to be designed carefully and adapted to the custom application deployment needs.</p>

<h4>Notes</h4>

<ul>
<li>Thanks to Docker, all the work described here can be done offline (setting up the infrastructure can be done fast, on a dev&rsquo;s machine for example)</li>
<li>Liquibase changelogs can be executed individually or in group (by including subsets of changelogs) thus during the whole process we can adopt a
step-by step approach</li>
</ul>


<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Edit files in Docker containers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker/"/>
    <updated>2014-09-24T13:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker</id>
    <content type="html"><![CDATA[<p>I wish I get 1 dollar each time I install vi in a docker container &hellip; I wanted
an easier way to edit files in a running docker container. First of all try to
<strong>avoid</strong> editing files at all, as it is against the container philosophy
(see the last paragraph).</p>

<p>But if you have a valid reason, here comes the how-to.</p>

<h2>Why Zedapp</h2>

<p>Most of the time I use either vi or <a href="https://atom.io/">Atom</a>, but a few months
ago I stumbled upon <a href="http://zedapp.org/">Zedapp</a> an opinionated editor. It aims to
reduce cognitive load while editing, by simplifying things, like deliberately
not using tabs.</p>

<p>It stands out with its <strong>first-class support of remote editing</strong> let it be a
remote server, or even directly editing github repositories.</p>

<p>Zedapp just reached version 1.0 and if you like it, consider help Zef Hemmel
at <a href="https://gratipay.com/zefhemel/">gratipay</a>, who was brave enough to quit his
regular job, and work full time on an open-source project!</p>

<h2>Install Zedapp</h2>

<p>You can use zedapp as a <em>chrome plugin</em> or a <em>standalone</em> app. Downloads are
available at: <a href="http://zedapp.org/download/">zedapp.org</a>. I recommend to
go for the <strong>standalone</strong> version.</p>

<!-- more -->


<h2>Install zedrem</h2>

<p>For <a href="http://zedapp.org/features/edit-remote-files/">remote editing</a>,
you need zedrem, a small process serving files to be edited in Zedapp.
Zedrem is packaged into a docker image:
<a href="https://github.com/sequenceiq/docker-zedapp">sequenceiq/zedapp</a></p>

<p>To start a local zed-server, and zed-client in the target container, there
is a helper script: <strong>zed</strong></p>

<p>To install the docker image and the shell script run this:
<code>
docker run --rm \
  -v /usr/local/bin:/target \
  -v /usr/local/bin/docker:/usr/local/bin/docker \
  -v /var/run/docker.sock:/var/run/docker.sock \
  sequenceiq/zedapp
</code></p>

<p>Actually there is only a single binary called <strong>zedrem</strong>, i just use the
terminology: zed-server and zed-client to
distinguish when you use it with or without the <code>--server</code> option.</p>

<p>Now you are ready to start a zedrem session, to edit files in Zedapp which are
inside of a Docker container&rsquo;s directory.</p>

<h2>Start a zedrem session</h2>

<p>To start a zedrem client in a container
<code>
zed &lt;container&gt; &lt;directory&gt;
</code></p>

<p>This will:
&ndash; start a <code>zedrem-server</code> if not already running.
&ndash; copy and start <code>zedrem-client</code> into the selected container and print out
  the zedrem session&rsquo;s <strong>remote-url</strong>.</p>

<p>Navigate to the project list window by: <code>Command-Shift-O</code>/<code>Ctrl-Shift-O</code>. Select
 <code>Remote Folder</code>, enter the remote-url into <code>Zedrem URL</code> input field and press
 <code>Open</code>.</p>

<p>Thats all enyoj! All the following paragraphs are for the curious only.</p>

<h2>Boot2docker helper function</h2>

<p>The <code>Install zedrem</code> step should have detected that you are using Boot2docker,
and instructed you to create a helper function, but in case you missed it, or
for reference:</p>

<p><code>
zed() { boot2docker ssh "sudo zed $@" ; }
</code>
This is needed as the helper script called <code>zed</code> is installed inside of
Boot2docker, so you need the ususal <code>boot2docker ssh</code> workaround.</p>

<p>after that you can issue directly on OSX:
<code>
zed &lt;container&gt; &lt;directory&gt;
</code></p>

<h2>Local zedrem server.</h2>

<p>By default when you want to use Zedapp for remote editing, you need two
other components then Zedapp:</p>

<ul>
<li><strong>zedrem-server</strong> Zedapp gets file content, and sends edit commands
on webservices protocol. It maintains sessions with zedrem-clients.</li>
<li><strong>zedrem-client</strong> a small process serving files from a specified directory.</li>
</ul>


<p>When you use zedrem-client via the official server, all the editing commands/content
travel around the blobe:</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-remote.png" alt="zedrem remote" /></p>

<p>Compare it with the dockerized local server setup, which is more quick and
secure:
<img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-docker.png" alt="zedrem docker" /></p>

<h2>nsenter</h2>

<p>You might wonder about the step: <strong>copy zedrem into the container</strong>. How is it
possible? Docker&rsquo;s <code>cp</code> command only supportts the other direction: copy from a
container into a local dir.</p>

<p>There is an <a href="https://github.com/docker/docker/issues/5846">open issue</a>, so it
will be fixed soon, but meanwhile you can use nsenter to the rescue. Jérôme
Petazzoni prepared us a canned <a href="https://github.com/jpetazzo/nsenter">nsenter</a>
with the helper script: <code>docker-enter</code>. We can missuse docker-enter to copy
a file from local fs into the container by:</p>

<p><code>
cat local-file | docker-enter $container sh -c 'cat&gt;/zedrem'
</code></p>

<p>btw: <code>docker exec</code> is already merged into the master branch, so it will replace
nsenter completely.</p>

<h2>Don&rsquo;t do this at all</h2>

<p>Let&rsquo;s make it clear, that most of the time you don&rsquo;t need this.
First of all editing files in a container, other than development or debug
considered bad practice.</p>

<p>You find yourself editing nginx config files? Don&rsquo;t do it, use the great generic
<a href="https://github.com/progrium/nginx-appliance">nginx appliance</a> from Jeff Lindsay.</p>

<p>If you <strong>really</strong> need to edit files in a docker container, just use volumes.</p>

<p>This process comes handy if you&rsquo;ve already started a container, and the file in
question doesn&rsquo;t sits on a volume.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Tez cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/"/>
    <updated>2014-09-19T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster</id>
    <content type="html"><![CDATA[<p>This week the <a href="http://tez.apache.org/">Apache Tez</a> community announced the release of the 0.5 version of the project. At <a href="http://sequenceiq.com/">SequenceIQ</a> first time we came across Tez was in 2013 &ndash; after <a href="http://hortonworks.com/">Hortonworks</a> launched the <code>Stinger Initiative</code>. Though we were not using Hive (that might change soon) we have quickly realized the <code>other</code> capabilities of Tez &ndash; the expressive data flow API, data movement patterns, dynamic graph reconfiguration, etc &ndash; to name a few.</p>

<p>We quickly became <code>fans</code> of Tez &ndash; and have started to run internal PoC projects, rewrite ML algorithms and legacy MR2 code to run/leverage Tez. The new release comes with a stable developer API and a proven stability track, and this has triggered a <code>major</code> re-architecture/refactoring project at SequenceIQ. While I don’t want to enter into deep details, we are building a Platform as a Service API &ndash; with the first stages of the project already released, open sourced and in public beta:</p>

<p><a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; our Docker based cloud agnostic Hadoop as a Service API (AWS, Azure, Google Cloud, DigitalOcean);
<a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; an SLA policy based autoscaling API for Hadoop YARN</p>

<p>One of the unreleased component is a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; a big data pipeline API (with 50+ pre-built data and job pipes), running on <strong>MR2, Tez and Spark</strong>.</p>

<p>With all these said, we have put together a <code>Tez Ready</code> Docker based Hadoop cluster to share our excitement and allow you to quickly start and get familiar with the nice features of the Tez API. The cluster is built on our widely used Apache Ambari Docker <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">container</a>, with some additional features. The containers are <code>service discovery</code> aware. You don’t need to setup anything beforehand, configure IP addresses or DNS names &ndash; the only thing you will need to do is just specify the number of nodes desired in your cluster, and you are ready to go. If you are interested on the underlying architecture (using Docker, Serf and dnsmasq) you can check my slides/presentation from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit</a>.</p>

<p>I&rsquo;d like to highlight one important feature of Tez &ndash; us being crazy about automation/DevOps &ndash; the simplicity and the capability of running multiple versions of Tez on the same YARN cluster. We are contributors to many Apache projects (Hadoop, YARN, Ambari, etc) and since we have started to use Tez we consider to contribute there as well (at the end of the day will be a core part of our platform). Adding new features, changing code or fixing bugs always introduce undesired <code>features</code> &ndash; nevertheless, the Tez binaries built by different colleagues can be tested at scale, using the same cluster without affecting each others work. Check Gopal V&rsquo;s good <a href="http://bit.ly/tez-devops">introduction</a> about Tez and DevOps.</p>

<h2>Apache Tez cluster on Docker</h2>

<p>The container’s code is available on our <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea-tez">GitHub</a> repository.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/ambari:1.7.0-ea-tez
</code></p>

<!-- more -->


<h3>Building the image</h3>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<p><code>
docker build --rm -t sequenceiq/ambari:1.7.0-ea-tez ambari-server/
</code></p>

<h2>Running the cluster</h2>

<p>We have put together a few shell functions to simplify your work, so before you start make sure you get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea-tez/ambari-functions">file</a>.</p>

<p><code>
curl -Lo .amb j.mp/docker-ambari-tez &amp;&amp; . .amb
</code></p>

<h3>Create your Apache Tez cluster</h3>

<p>You are almost there. The only thing you will need to do is to specify the number of nodes you need in your cluster. We will launch the containers, they will dynamically join the cluster and apply the Tez specific configurations.</p>

<p><code>
amb-deploy-cluster 4
</code></p>

<p>Once the cluster is started you can <a href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/">enter</a> in the container and submit your custom Tez application or use one of the stock Tez examples.</p>

<p>Check back next week, as we are releasing <code>real world</code> examples running on three different big data fabrics: Tez, MR2 and Spark.</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak new provider implementation - Part I: Build your custom image]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc/"/>
    <updated>2014-09-18T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc</id>
    <content type="html"><![CDATA[<p>Not so long ago we have released <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the cloud agnostic, open source and Docker based Hadoop as a Service API (with support for <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">autoscaling</a> Hadoop clusters). As we have <code>dockerized</code> the whole Hadoop ecosystem, we are shipping the containers to different cloud providers, such as Amazon AWS, Microsoft Azure and Google Cloud Compute. Also Cloudbreak has an <a href="http://sequenceiq.com/cloudbreak/#add-new-cloud-providers">SDK</a> which allows you to quickly add your favorite cloud provider. In this post (series) we’d like to guide you trough the process, and show you how to create a custom image &ndash; on Google Cloud. We have chose Google Cloud as this is the least documented and has the smallest amount on default images (there are thousand for Amazon, and hundreds for Azure). Nevertheless on all cloud provider usually you’d like to have a custom image with your preferred OS, configuration and potentially installed applications.</p>

<!-- more -->


<h3>Why do we need custom images on every cloud?</h3>

<p>All the above are true for us as well &ndash; with some simplifications. We use Docker to run every process/application &ndash; for the benefits we have covered in other posts many times &ndash; and apart from Docker, our (or the customer’s) preferred OS and a few other helper/debugger things (such as <a href="https://registry.hub.docker.com/u/jpetazzo/nsenter/">nsenter</a>)
we are almost fine. We have made some PAM related fixes/contributions for Docker &ndash; and until they are not in the upstream we have built/derive from our base layer/containers &ndash; so with this and the actual containers included this is pretty much how a cloud base image looks like for us.</p>

<p>As usual we always automate everything &ndash; building custom cloud base images is part of the automation and our CI/CD process as well. For that we use <a href="http://www.ansible.com/home">Ansible</a> as the preferred IT automation tool. So the first step is to define your own <a href="http://docs.ansible.com/playbooks.html">playbook</a> to install everything on the virtual machine.</p>

<p>A simple playbook looks like this:</p>

<p>```
  &ndash; name: Install Docker</p>

<pre><code>shell: curl -sL https://get.docker.io/ | sh
when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
</code></pre>

<ul>
<li><p>name: Pull sequenceiq/ambari image
shell: docker pull sequenceiq/ambari:pam-fix</p></li>
<li><p>name: Pull jpetazzo/nsenter image
shell: docker pull jpetazzo/nsenter</p></li>
<li><p>name: Install bridge-utils
apt: name=bridge-utils state=latest
when: ansible_distribution == &lsquo;Debian&rsquo; or ansible_distribution == &lsquo;Ubuntu&rsquo;</p></li>
<li><p>name: install jq
shell: curl -o /usr/bin/jq <a href="http://stedolan.github.io/jq/download/linux64/jq">http://stedolan.github.io/jq/download/linux64/jq</a> &amp;&amp; chmod +x /usr/bin/jq</p></li>
</ul>


<p>```</p>

<p>Using Google cloud you have 2 choices:</p>

<ol>
<li> Create snapshots starting from a default image</li>
<li> Create a custom image</li>
</ol>


<h3>Image creation using snapshots</h3>

<p>We are using Debian as the host OS on Google Cloud, and have created a virtual machine using the default <a href="https://developers.google.com/compute/docs/operating-systems#backported_debian_7_wheezy">Debian</a> image. First thing first, you need to create a persistent disk:</p>

<p><code>
gcloud compute disks create temporary-disk --zone ZONE
</code></p>

<p>Then create a virtual machine with the temporary-disk:</p>

<p><code>
gcloud compute instances create example-instance \
  --scopes storage-rw --image IMAGE \
  --disk name=temporary-disk device-name=temporary-disk --zone ZONE
</code></p>

<p>And attach the disk to the google cloud instance:</p>

<p><code>
gcloud compute instances attach-disk example-instance
  --disk temporary-disk --device-name temporary-disk --zone ZONE
</code></p>

<p>When this is finished then you can <code>shh</code> to the <code>sample-instance</code>. You can now check your mounted volumes with this command:</p>

<p><code>
ls -l /dev/disk/by-id/google-*
</code></p>

<p>Now you need to create a folder which will contain your custom built image:</p>

<p><code>
sudo mkdir /mnt/tmp
</code></p>

<p>You have to format your partition before the image creation:</p>

<p><code>
sudo /usr/share/google/safe_format_and_mount -m "mkfs.ext4 -F" /dev/sdb /mnt/tmp
</code></p>

<p>Now you can start building the image which will last about 10 minutes:</p>

<p><code>
sudo gcimagebundle -d /dev/sda -o /mnt/tmp/ --log_file=/tmp/imagecreation.log
</code></p>

<p>You have now an image in /tmp with a special hex number like <code>/tmp/HEX-NUMBER.image.tar.gz</code></p>

<p>Once you uploaded it to a Google bucket you are done, and ready to use it.</p>

<p><code>
gsutil cp /mnt/tmp/IMAGE_NAME.image.tar.gz gs://BUCKET_NAME
</code></p>

<h3>Create a custom image &ndash; using your favorite OS</h3>

<p><a href="http://www.ubuntu.com/download/server">Ubuntu server 14.04</a> is many’s preferred Linux distribution &ndash; unluckily there is no default image using Ubuntu as the OS in the Google Cloud](<a href="https://developers.google.com/compute/docs/operating-systems">https://developers.google.com/compute/docs/operating-systems</a>). Luckily this is not that complicated &ndash; the process below works with any other OS as well. In order to start you should have <a href="https://www.virtualbox.org/">Virtualbox</a> installed. Download an Ubuntu server from <a href="http://www.ubuntu.com/server">Ubuntu’s</a> web page.
Install in into the <a href="https://www.virtualbox.org/">Virtualbox</a> box, start it and <code>ssh</code> into. Once you are inside you will have to install the <a href="https://developers.google.com/cloud/sdk/">Google Cloud SDK</a>. This is needed for the custom image, as contains some extra feature like <code>google-startup-scripts</code>. Remember that Ubuntu (and in general a few cloud providers) support <code>cloud-init</code> scripts, and this is why we need the Google Cloud SDK &ndash; as we ship these images to the <code>cloud</code>.</p>

<p>After the installation add the following kernel options into the <code>/etc/default/grub</code>:</p>

<p>```</p>

<h1>to enable paravirtualization</h1>

<p>CONFIG_KVM_GUEST=y</p>

<h1>to enable the paravirtualized clock.</h1>

<p>CONFIG_KVM_CLOCK=y</p>

<h1>to enable paravirtualized PCI devices.</h1>

<p>CONFIG_VIRTIO_PCI=y</p>

<h1>to enable access to paravirtualized disks.</h1>

<p>CONFIG_SCSI_VIRTIO=y</p>

<h1>to enable access to the networking.</h1>

<p>CONFIG_VIRTIO_NET=y
```</p>

<p>Now you are ready to prepare an <code>official</code> image into a tar file, by selecting the virtual box image file on your disk and convert it.
You can convert your <code>vmdk</code> file into the supported raw type by using:</p>

<p><code>
qemu-img convert -f vmdk -O raw VMDK_FILE_NAME.vmdk disk.img
</code></p>

<p>The .img file name has to be <code>disk.img</code>. After you have converted the image, you have to make a tar file:</p>

<p><code>
tar -Szcf &lt;image-tar-name&gt;.tar.gz disk.raw
</code></p>

<p>Same as before, you have to upload in to a Google Cloud Bucket:</p>

<p><code>
gsutil cp &lt;image-tar-name&gt;.tar.gz gs://&lt;bucket-name&gt;
</code></p>

<p>Now you have an <code>official</code> image template but you have to create the image in Google Cloud:</p>

<p><code>
gcutil addimage my-ubuntu gs://&lt;bucket-name&gt;/ubuntu_image.tar.gz
</code></p>

<p>Once this is done you have created your custom built Google Cloud image, and you are ready to start cloud instances using it. Let us know how it works for you, and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> for updates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark 1.1.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker/"/>
    <updated>2014-09-17T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker</id>
    <content type="html"><![CDATA[<p>As you might be already familiar, we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well. For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>In this current post we’d like to help you to start with the <code>latest - 1.1.0</code> Spark release in minutes &ndash; using Docker. Docker and Spark are two technologies which are very <code>hyped</code> these days. At <a href="http://sequenceiq.com/">SequenceIQ</a> we use both quite a lot, thus we put together a Docker container and sharing it with the community.</p>

<p>The container’s code is available in our <a href="https://github.com/sequenceiq/docker-spark/blob/v1.1onHadoop-2.5.1/README.md">GitHub</a> repository.</p>

<h3>Pull the image from Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<p><code>
docker pull sequenceiq/spark:1.1.0
</code></p>

<!-- more -->


<h2>Building the image</h2>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<p><code>
docker build --rm -t sequenceiq/spark:1.1.0 .
</code></p>

<h2>Running the image</h2>

<p>Once you have pulled or built the container, you are ready to start with Spark.</p>

<p><code>
docker run -i -t -h sandbox sequenceiq/spark /etc/bootstrap.sh -bash
</code></p>

<h3>Testing</h3>

<p>In order to check whether everything is OK, you can run one of the stock examples, coming with Spark. Check our previous blog posts and examples about Spark <a href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/">here</a> and <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">here</a>.</p>

<p>```
cd /usr/local/spark</p>

<h1>run the spark shell</h1>

<p>./bin/spark-shell &mdash;master yarn-client &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1</p>

<h1>execute the the following command which should return 1000</h1>

<p>scala> sc.parallelize(1 to 1000).count()
```</p>

<p>There are two deploy modes that can be used to launch Spark applications on YARN. In yarn-cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>

<p>Estimating Pi (yarn-cluster mode):</p>

<p>```
cd /usr/local/spark</p>

<h1>execute the the following command which should write the &ldquo;Pi is roughly 3.1418&rdquo; into the logs</h1>

<p>./bin/spark-submit &mdash;class org.apache.spark.examples.SparkPi &mdash;master yarn-cluster &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar
```</p>

<p>Estimating Pi (yarn-client mode):</p>

<p>```
cd /usr/local/spark</p>

<h1>execute the the following command which should print the &ldquo;Pi is roughly 3.1418&rdquo; to the screen</h1>

<p>./bin/spark-submit &mdash;class org.apache.spark.examples.SparkPi &mdash;master yarn-client &mdash;driver-memory 1g &mdash;executor-memory 1g &mdash;executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar
```</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
