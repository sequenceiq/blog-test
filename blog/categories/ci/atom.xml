<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CI | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/ci/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-02-02T18:07:58+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Managing database upgrades with Liquibase and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process/"/>
    <updated>2014-09-26T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are <em>crazy</em> about automating everything &ndash; let it be the provisioning of a thousand nodes Hadoop
cluster using <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> or a simple database change.
We apply the same automated CI/CD process to all our projects, including plain old RDBMS schema changes &hellip; yes, though we are a <em>big data</em> technology
company sometimes we do use JPA as well.</p>

<p>As applications evolve their underlying data model change. New functionalities often need data model changes and the initial design
needs to be adapted to the ever changing demands. These changes usually are of two types : structural changes
(e.g.: addition/removal of tables, columns, constraints etc &hellip;)
and migration of the existing data to the new version of the datamodel.
As the data model gets more and more complex  &ndash; this will happen in-spite of trying to keep it as simple as possible &ndash;
the complexity of these tasks grow proportionally. This happens here at SequenceIQ too; the post is about how we address some of these problems.</p>

<!-- more -->


<h2>Directives</h2>

<ul>
<li>We need a process to follow each time such changes arise</li>
<li>Use appropriate tools that do the job (instead of reinventing the wheel)</li>
<li>Make the process <strong>automated</strong> as much as possible</li>
</ul>


<h3>The process</h3>

<p>The process &ndash; as the common sense suggests &ndash; could be split in the following steps:</p>

<ul>
<li>start from the initial version of the database (the version in production)</li>
<li>perform changes required by the new version of the application</li>
<li>capture and store differences between the two versions of the database</li>
<li>(automatically) apply changes to the initial database version</li>
<li>perform tests</li>
<li>apply changes to production</li>
</ul>


<h3>Tools</h3>

<ul>
<li>Dockerized (PostgreSQL) database</li>
<li>Dockerized Liquibase</li>
<li>Jenkins</li>
</ul>


<h3>Implementation</h3>

<h4>Start from the initial version of the database</h4>

<p>To start with, you need a database that&rsquo;s (structurally) identical with the production one. There are several ways to achieve this;we use Postgres and try to
keep it simple, so here&rsquo;s what we do:</p>

<ul>
<li>we always have a QA database which is identical with the production (obliviously the data is not the same)</li>
<li>we make a copy of the <em>data</em> folder of the postgres installation into an arbitrary location on the host</li>
<li>we pass it as a volume to a Docker container running Postgres</li>
</ul>


<p>We run the following command each time we need a fresh database:</p>

<p><code>
docker run -d \
  --name $CONTAINER_NAME \
  -v /$WORKING_DIR/data:/data \
  -p 5432:5432 \
  -e "USER=$DB_USER" \
  -e "PASS=$DB_PASS" \
  -e "DB=$DB_NAME" \
paintedfox/postgresql
</code></p>

<p>where the passed in variables are the following:</p>

<ul>
<li>CONTAINER_NAME &ndash; the name of the database Docker container</li>
<li>DB_USER &ndash; the database user name</li>
<li>DB_PASS &ndash; the database password</li>
<li>DB_NAME &ndash; the database schema</li>
</ul>


<p>We have a running database now (in less than a minute) &ndash; same as the prod; we can connect to it with a client on your localhost, port 5432 with the given username/password.</p>

<h4>Perform changes required by the new version of the application</h4>

<p>As expected, this is the most challenging part in the process: changes need to be implemented and also
captured so that they can be applied any time (preferably in an <strong>automated</strong> way)
As we&rsquo;re using JPA (with Hibernate as JPA provider) incremental structural changes are executed with the
SchemaUpdate tool. This can be done during the application startup or using <em>ant</em> or <em>maven</em>.
As we continuously test the application we choose to start the application configured to update the database based on the
changed data model (annotations). Alternatively we could regenerate the whole schema. (See the SchemaUpdate tool documentation:
<a href="http://docs.jboss.org/hibernate/core/3.6/reference/en-US/html/toolsetguide.html">here</a>)</p>

<p>At this point we have a database that aligns with the new version of the application. Please note here, that only <code>incremental</code> changes have been applied to
the database till now, meaning that for example new fields have been added,
 but old/deprecated fields haven&rsquo;t been deleted.</p>

<p>Other type of scripts need to be implemented manually:</p>

<ul>
<li><p>changes that couldn&rsquo;t be performed by the SchemaUpdate tool, such as cleanup (SQL) scripts. This being done, differences till this phase
can be automatically generated by running the Liquibase Docker container &ndash; see the next section. Differences are stored under version control,
in form of <em>Liquibase changelogs</em></p></li>
<li><p>data migration scripts, that adapt the existing data to the new structure. Think of cases
when for example a field becomes a new entity and instead of a value you need to store a reference to the new entity. We store these kind of scripts along
with the generated diff files under version control in form of <em>Liquibase changelogs</em></p></li>
</ul>


<h4>Dockerized Liquibase</h4>

<p>Speaking of tools, we found that <a href="http://www.liquibase.org/index.html">Liquibase</a> addresses many of our requirements, such as</p>

<ul>
<li>track database changes (changes being stored in VCS)</li>
<li>automatically generate diffs between two versions of the database</li>
<li>automatically update a database based on changelogs</li>
</ul>


<p>We have created a docker image with a <em>liquibase</em> installation. You can find the project <a href="https://github.com/sequenceiq/docker-liquibase">here</a></p>

<p>The image can be built locally with the command:</p>

<p><code>
docker build -t sequenceiq/docker-liquibase .
</code></p>

<p>or from the project root, or pulled from the Docker repository:</p>

<p><code>
docker pull sequenceiq/docker-liquibase
</code></p>

<p>Containers built from this image can be used to perform <em>liquibase</em> operations on any host.
This saves us a lot of time by having the installation and configuration shipped and helps us to automate most of the tasks.
You can use the container for performing liquibase tasks manually in a terminal, or you can start the container to
automatically perform specific tasks (and quit eventually). To start the container linked to the previously started database
container and perform manual operations, run:</p>

<p><code>
docker run -it \
--name $LIQUIBASE_CONTAINER \
--link $DB_CONTAINER:db \
--entrypoint /bin/bash \
-v /$LIQUIBASE_CHANGELOGS:/changelogs \
$LIQUIBASE_DOCKER_IMAGE \
/bin/bash
</code></p>

<p>See the description of the variables:</p>

<ul>
<li>LIQUIBASE_CONTAINER the name of the Liquibase Docker container</li>
<li>DB_CONTAINER the name of the database container the Liquibase container is to be linked to</li>
<li>LIQUIBASE_CHANGELOGS the folder holding the liquibase changelogs (Liquibase will read and write here)</li>
<li>LIQUIBASE_DOCKER_IMAGE the name of the dockerized Liquibase Docker image</li>
</ul>


<p>Some of the Liquibase tasks can be scripted. We scripted the diff generation and changelog application. Liquibase offers more advanced features too.</p>

<h4>Testing</h4>

<p>We write tests that can be run automatically to check the process. Each <code>changeset</code>, especially those related to data migration / transformation is covered.</p>

<h4>Apply liquibase changelogs to the production database</h4>

<p>After the application is tested upon applying the database changes &ndash; that ensures that changelogs are correct, it&rsquo;s easy to set up a <code>jenkins</code> job that:</p>

<ul>
<li>checks out the proper version of changelogs</li>
<li>starts a docker container linked to the (production) database and applies changelogs</li>
</ul>


<p>Obviously this step needs to be designed carefully and adapted to the custom application deployment needs.</p>

<h4>Notes</h4>

<ul>
<li>Thanks to Docker, all the work described here can be done offline (setting up the infrastructure can be done fast, on a dev&rsquo;s machine for example)</li>
<li>Liquibase changelogs can be executed individually or in group (by including subsets of changelogs) thus during the whole process we can adopt a
step-by step approach</li>
</ul>


<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the build environment with Ansible and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/"/>
    <updated>2014-05-09T11:51:57+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we put a strong emphasis on automating everything we can and this automation starts with our continuous integration &amp; delivery process.</p>

<h2>Introduction</h2>

<p>Lately there is a lot of buzz around continuous integration, development and deployment. More and more companies are moving away from long release cycles towards the &ldquo;release early, release often&rdquo; approach. The advantages of this approach are well known: lower overhead, earlier bug discovery and bug fixing, fewer context switches for the developers to name just a few. There are very good resources to learn about these concepts &ndash; blog posts by different companies (e.g.: by <a href="http://techblog.netflix.com/2013/08/deploying-netflix-api.html">Netflix</a>) and of course the <a href="http://www.amazon.com/dp/0321601912">book</a> &lsquo;Continuous Delivery&rsquo; by Jez Humble and David Farley &ndash; we&rsquo;ll now try to add our own experiences as well.</p>

<p>We&rsquo;ll share two blog posts about our continuous delivery at SequenceIQ: the first one being an introductory post about some tools we use to make the whole process easier and more robust, the second one explains the <a href="http://scottchacon.com/2011/08/31/github-flow.html">flow</a> we use from committing changes to being the changes available in our different environments.</p>

<h2>Tools</h2>

<p>Our CI and CD process at SequenceIQ is based on Ansible, Jenkins and of course Docker.
When we started to build our own process, we decided that we don&rsquo;t want to commit the same mistake that a lot of companies make about their build environment. At these companies the build servers where Jenkins and/or the other build tools are installed are often prepared once in the far past by someone who probably doesn&rsquo;t work there anymore. It quickly becomes something that everyone is afraid to touch and just hope that it will work forever. As the projects improve there will be a lot of different tools with a lot of different versions on the build machine and soon it leads to a small chaos, where the maintenance will involve a lot of hard manual work. To get rid of these problems, we use <a href="http://www.ansible.com/">Ansible</a> to &ldquo;build the build infrastructure&rdquo;, and Docker to run the builds in separated self-sufficient containers.</p>

<!-- more -->


<h2>Ansible</h2>

<p>We have an Ansible script which starts an EC2 instance in the cloud and provisions everything on this server automatically. This script can be easily executed with a single command from a developer laptop:</p>

<p><code>
ansible-playbook -i hosts ci.yml
</code></p>

<p>To run this command Ansible, python and some python modules must be installed on the local machine. To avoid having different version of these tools on the development machines we automated the installation of our development environment too &ndash; maybe the topic of another post in the future.
So let&rsquo;s see how the Ansible script works exactly.</p>

<h3>Creating an instance in the cloud</h3>

<p>First it needs to start an instance in the AWS cloud, so it invokes our <strong>ec2 role</strong> on localhost:
```yaml
&ndash; name: Request and init EC2 instance
  hosts: localhost
  roles:</p>

<pre><code> - ec2
</code></pre>

<p>```</p>

<p>The ec2 role requests an EC2 spot priced instance and associates it with an elastic IP. We can easily use a spot priced instance because if it gets shut down by AWS we can recreate it in a few minutes! Ansible has a few <a href="http://docs.ansible.com/list_of_cloud_modules.html">cloud modules</a> which makes it quite easy to manage EC2 instances. Requesting a spot priced instance looks like this (the placeholders come from Ansible group variables):

<code>yaml
- name: Create an EC2 spot priced instance
  local_action:
  module: ec2
  key_name: "{{ ec2.keypair }}"
  group: "{{ ec2.security_group }}"
  instance_type: "{{ ec2.instance_type }}"
  spot_price: "{{ ec2.spot_price }}"
  image: "{{ ec2.image }}"
  wait: yes
  region: "{{ ec2.region }}"
  id: "{{ ec2.idempotent_id }}"
  register: ec2result
</code>
</p>

<h3>Provisioning the build server</h3>

<p>After the EC2 instance is running and accepting SSH connections, the script can go on and start to install the tools needed. Because almost everything is running in separated Docker containers, we only need 3 things: Docker, Nginx and Jenkins. Installing Docker is pretty easy as the new Amazon Linux AMIs are <a href="http://aws.amazon.com/amazon-linux-ami/2014.03-release-notes/">prepared</a> to run Docker. We only need to install it from Amazon&rsquo;s provided Software Repository, and start the service. It looks like this in the Ansible script:</p>

<p>```yaml
&ndash; name: Install Docker on Amazon Linux AMI
  when: ansible_os_family == &ldquo;RedHat&rdquo;
  yum: name=docker state=present</p>

<ul>
<li>name: Start Docker service
service: name=docker state=started
```</li>
</ul>


<p>After Docker is installed we can start some containers that are used by some Jenkins builds later (e.g.: a SonarQube server and a MySQL database that holds the results &ndash; we&rsquo;ve created publicly available <a href="https://index.docker.io/u/sequenceiq/sonar-server/">containers</a> on our Github page.</p>

<p>To install and configure Nginx we use an <a href="https://galaxy.ansible.com/list#/roles/466">existing role</a> from Ansible Galaxy that is well prepared and easily configurable. We are configuring Nginx to forward requests from port 80 to either Jenkins or Sonar.</p>

<p>For example the configuration for Jenkins is the following in the Ansible group variables:</p>

<p>```yaml
nginx_sites:
  default:</p>

<pre><code>- listen 80
- server_name jenkins.sequenceiq.com
- location / {
   proxy_pass http://jenkins;
   proxy_redirect off;
   proxy_set_header Host $host;
   proxy_set_header X-Forwarded-Host $server_name;
  }
</code></pre>

<p>nginx_configs:
  proxy:</p>

<pre><code>- proxy_set_header X-Real-IP $remote_addr
- proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for
</code></pre>

<p>  upstream:</p>

<pre><code>- upstream jenkins { server 127.0.0.1:8080 weight=10; }
</code></pre>

<p>```</p>

<p>The most difficult thing to install is Jenkins: we want our configurations and jobs to be instantly available as well. Jenkins has a <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+CLI">command-line interface</a> that allows access from a script. It has a lot of built-in commands to manage jobs and other configurations and it even has a command to execute a Groovy script on the server. We use these features extensively to prepare the whole Jenkins environment from sketch. Our Jenkins role (that will be available soon on Ansible Galaxy) is able to do the following:
&ndash; Install Jenkins and its dependencies, and get the Jenkins CLI jar from the specified URL.
&ndash; Configure the global Jenkins properties like the mail server, or the properties needed for the Github pull request builder plugin &ndash; it is simply achieved by copying a global config.xml to the Jenkins home directory using Ansible&rsquo;s copy module.
&ndash; Install and update plugins through the Jenkins CLI. Installing plugins looks like this:</p>

<p>
<code>yaml
- name: Install plugins
  sudo: yes
  shell: java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ install-plugin {{item.item}}
  when: item.stdout.find('false') != -1
  with_items: check_plugins.results
  notify:
  - 'Restart Jenkins'
</code>
</p>

<ul>
<li>Configure security (we use Github OAuth). The Jenkins CLI doesn&rsquo;t have any dedicated commands for setting security, but it can be configured with a Groovy script that can be invoked from the CLI:</li>
</ul>


<p>```groovy
def githubSecurityRealm = new org.jenkinsci.plugins.GithubSecurityRealm(&ldquo;<a href="https://github.com">https://github.com</a>&rdquo;, &ldquo;<a href="https://api.github.com">https://api.github.com</a>&rdquo;, clientId, clientSecret)
def authorizationStrategy = new org.jenkinsci.plugins.GithubAuthorizationStrategy(&ldquo;admin1,admin2&rdquo;,true,&ldquo;organization name&rdquo;,true,false,false)
jenkins.model.Jenkins.instance.setSecurityRealm(githubSecurityRealm)
jenkins.model.Jenkins.instance.setAuthorizationStrategy(authorizationStrategy)
jenkins.model.Jenkins.instance.save()</p>

<p>```</p>

<ul>
<li><p>Copy private keys for Github builds &ndash; it simply copies the predefined private SSH keys from a local directory to the <code>~/.ssh</code> directory of the Jenkins user. We use a dedicated Github user to communicate with Github from Jenkins.</p></li>
<li><p>Creating jobs from XML configuration. The Jenkins CLI supports the creation of Jenkins jobs through the create-job command that accepts an XML file as input that defines the Jenkins job. Currently our Jenkins role works by invoking this command for every job that is defined in the variables and has a corresponding XML file in a predefined directory.  We are planning to later modify this role to have a template that holds the structure of a Jenkins job XML so it won&rsquo;t be needed to create the whole XML file manually, only the required parameters among the Ansible variables.</p></li>
</ul>


<p>
<code>yaml
- name: Create jenkins jobs
  shell: java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ create-job {{ item }} &lt; {{ jenkins.dest }}/{{item}}.xml
  with_items: jenkins_jobs
  when: existing_jobs.changed and existing_jobs.stdout.find('{{ item }}') == -1
</code>
</p>

<h2>Docker</h2>

<p>The other tool besides Ansible that we use extensively in our build environment is Docker. Docker is a quickly expanding technology that enables the creation of lightweight application containers. If you don&rsquo;t know about Docker yet, check out the official <a href="https://www.docker.io/gettingstarted/">Getting Started guide</a> or our own <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/">blog post</a> about it.
With the help of Docker we don&rsquo;t need to worry about the tools needed for the builds or its dependencies on the continuous integration server as they are packaged in separate containers. Every one of our builds on Jenkins are only a few lines that runs a container, maybe copies something out of it and removes the container after it finished. We provide a few environment variables, some shared directories or some links between containers where needed. One of our jobs in Jenkins that builds the master branch looks like this:
```bash</p>

<h1>!/bin/bash</h1>

<p>docker run -i &mdash;name $BUILD_TAG \
-v &ldquo;/var/lib/jenkins/.gradle-api:/root/.gradle:rw&rdquo; \
-e &ldquo;SONAR_USERNAME=$SONAR_USERNAME&rdquo; \
-e &ldquo;SONAR_PW=$SONAR_PW&rdquo; \
-e &ldquo;BUILD_NUMBER=$BUILD_NUMBER&rdquo; \
-e &ldquo;KEY=$(cat /var/lib/jenkins/.ssh/id_rsa| base64 -w 0)&rdquo; \
-e &ldquo;REPO=$REPO_ADDRESS&rdquo; \
-e &ldquo;BRANCH=master&rdquo; \
-e &ldquo;BUILD_TASKS=clean build sonarRunner uploadArchives&rdquo; \
-e &ldquo;BUILD_ENV=jenkins&rdquo; \
-e &ldquo;GRADLE_OPTS=-XX:MaxPermSize=512m&rdquo; \
&mdash;link sonar_server:sonar \
&mdash;link sonar_mysql:sonar_db \
sequenceiq/build /etc/build-project.sh
sleep 5
docker cp $BUILD_TAG:/tmp/prj/build/build.info $WORKSPACE
docker rm $BUILD_TAG</p>

<p>```</p>

<p>And not only our builds run in Docker, some other tools we use on the build environment also run in containers. For instance our code quality management tool, SonarQube and the MySQL database it uses also runs in separate containers. This way we don&rsquo;t need to install them on the EC2 instance directly, we only need to link them where needed &ndash; see the example above!</p>

<p>In our next blog post about continuous integration we&rsquo;ll explain the process we use at SequenceIQ to continuously deliver the new features to production using the Github flow with Jenkins and Docker.</p>
]]></content>
  </entry>
  
</feed>
