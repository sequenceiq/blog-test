<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: metrics | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/metrics/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-11-13T12:27:55+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time adjustments with Hadoop metrics]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/15/hadoop-metrics/"/>
    <updated>2014-10-15T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/15/hadoop-metrics</id>
    <content type="html"><![CDATA[<p>To properly understand and to be fully aware of the state of our Hadoop clusters at any time we needed a scalable and flexible solution
to monitor our Hadoop nodes. After investigating the possible solutions we realized that there is no available solution which satisfies
all our needs thus we&rsquo;ve created one and recently just open sourced it, called <a href="http://sequenceiq.com/periscope/#monitoring">Baywatch</a>. Baywatch is capable to capture and visualize real-time changes on Hadoop clusters to understand and make adjustments based on the submitted jobs resource
allocation needs. To plan ahead, viewing and comparing old and new metrics is just as
important as analyzing real-time ones, not to mention that we can find possible weaknesses and defects in our clusters.</p>

<p>To be able to do all of the above mentioned, Baywatch processes the metrics information produced by the Hadoop daemons. This might already sound familiar as we have another project called <a href="http://sequenceiq.com/periscope/">Periscope</a> where you can create alarms and cluster scaling activities making use of the same metrics, but just consuming it in a different way. Combine these 2
components and you&rsquo;ll have a powerful tool and you&rsquo;ll be able to view your cluster&rsquo;s state and based on that <code>make smart decisions</code>
to scale up or down, or simply just set alarms. If you&rsquo;re thrilled to see it in action we are at <a href="http://strataconf.com/stratany2014">Strata</a> and happy to show you a quick demo.</p>

<h2>Hadoop metrics</h2>

<p>So what are these metrics? As I mentioned it earlier metrics are collections of information about Hadoop daemons, e.g:
the <code>ResourceManager</code> produces information about the queue statuses which we use in Periscope when we <code>re-prioritise applications</code>.
To distinguish these metrics they are grouped into named contexts, e.g <code>jvm</code> for java virtual machine metrics, <code>rpc</code> for debugging
rcp calls, but there are many more:</p>

<ul>
<li>yarn</li>
<li>rpcdetailed</li>
<li>metricssystem</li>
<li>mapred</li>
<li>dfs</li>
<li>ugi</li>
</ul>


<p>This <code>Metrics2</code> framework is designed to collect and dispatch per-process metrics to monitor the overall status of the Hadoop system.
In Hadoop related technologies it is a common design to use sources and sinks, just like in this case. Metrics sources are where the
metrics are generated and metrics sinks consume the records generated by the metrics sources. A metrics system would poll the metrics
sources periodically and pass the metrics records to metrics sinks.</p>

<p><img class="<a" src="href="http://yuml.me/0faf3738">http://yuml.me/0faf3738</a>"></p>

<!-- more -->


<p>It is really easy to implement new sinks and sources, just for reference here&rsquo;s the <code>FileSink</code>:
```java
  @Override
  public void putMetrics(MetricsRecord record) {</p>

<pre><code>writer.print(record.timestamp());
writer.print(" ");
writer.print(record.context());
writer.print(".");
writer.print(record.name());
String separator = ": ";
for (MetricsTag tag : record.tags()) {
  writer.print(separator);
  separator = ", ";
  writer.print(tag.name());
  writer.print("=");
  writer.print(tag.value());
}
for (AbstractMetric metric : record.metrics()) {
  writer.print(separator);
  separator = ", ";
  writer.print(metric.name());
  writer.print("=");
  writer.print(metric.value());
}
writer.println();
</code></pre>

<p>  }
<code>
and the `FairSchedulerQueueMetrics`:
</code>java
  @Metric(&ldquo;Fair share of memory in MB&rdquo;) MutableGaugeInt fairShareMB;
  @Metric(&ldquo;Fair share of CPU in vcores&rdquo;) MutableGaugeInt fairShareVCores;
  @Metric(&ldquo;Steady fair share of memory in MB&rdquo;) MutableGaugeInt steadyFairShareMB;
  @Metric(&ldquo;Steady fair share of CPU in vcores&rdquo;) MutableGaugeInt steadyFairShareVCores;
  @Metric(&ldquo;Minimum share of memory in MB&rdquo;) MutableGaugeInt minShareMB;
  @Metric(&ldquo;Minimum share of CPU in vcores&rdquo;) MutableGaugeInt minShareVCores;
  @Metric(&ldquo;Maximum share of memory in MB&rdquo;) MutableGaugeInt maxShareMB;
  @Metric(&ldquo;Maximum share of CPU in vcores&rdquo;) MutableGaugeInt maxShareVCores;
```
Hadoop comes by default with 3 sinks:</p>

<ul>
<li>FileSink</li>
<li>GraphiteSink</li>
<li>GangliaSink30</li>
</ul>


<h2>Configuration</h2>

<p>The Metrics2 framework uses the <code>PropertiesConfiguration</code> thus the metrics sinks needs to be defined in a configuration-file:
<code>hadoop-metrics2.properties</code>. The declaration should be familiar for those who used <code>Apache Flume</code> before. Here is an example
taken from our <a href="link">Ambari docker image</a>:
<code>
*.sink.logstash.class=org.apache.hadoop.metrics2.sink.FileSink
namenode.sink.logstash.filename=/var/log/hadoop-metrics/namenode-metrics.out
secondarynamenode.sink.logstash.filename=/var/log/hadoop-metrics/secondarynamenode-metrics.out
datanode.sink.logstash.filename=/var/log/hadoop-metrics/datanode-metrics.out
resourcemanager.sink.logstash.filename=/var/log/hadoop-metrics/resourcemanager-metrics.out
nodemanager.sink.logstash.filename=/var/log/hadoop-metrics/nodemanager-metrics.out
maptask.sink.logstash.filename=/var/log/hadoop-metrics/maptask-metrics.out
reducetask.sink.logstash.filename=/var/log/hadoop-metrics/reducetask-metrics.out
mrappmaster.sink.logstash.filename=/var/log/hadoop-metrics/mrappmaster-metrics.out
</code></p>

<h3>Hadoop WebServices</h3>

<p>There is another way to obtain these metrics without any configuration which the Periscope leverages. It&rsquo;s the <code>WebServices</code> provided
by Hadoop. <code>Jax-RS</code> is used to define the mappings, e.g collect the <code>ResourceManager</code> queue related metrics on mapping <code>/ws/v1/cluster</code>:
```java
  @GET
  @Path(&ldquo;/scheduler&rdquo;)
  @Produces({ MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML })
  public SchedulerTypeInfo getSchedulerInfo() {</p>

<pre><code>init();
ResourceScheduler rs = rm.getResourceScheduler();
SchedulerInfo sinfo;
if (rs instanceof CapacityScheduler) {
  CapacityScheduler cs = (CapacityScheduler) rs;
  CSQueue root = cs.getRootQueue();
  sinfo = new CapacitySchedulerInfo(root);
} else if (rs instanceof FairScheduler) {
  FairScheduler fs = (FairScheduler) rs;
  sinfo = new FairSchedulerInfo(fs);
} else if (rs instanceof FifoScheduler) {
  sinfo = new FifoSchedulerInfo(this.rm);
} else {
  throw new NotFoundException("Unknown scheduler configured");
}
return new SchedulerTypeInfo(sinfo);
</code></pre>

<p>  }
```
The only difference is that you&rsquo;re application have to poll now, while the other way you can create forwarders to create push events
just like we did with Baywatch. To which to use depends on you&rsquo;re needs.</p>

<h2>Summary and resources</h2>

<p>As you see using <strong>Baywatch</strong> and <strong>Periscope</strong> you can monitor and scale your cluster based on the configured policies &ndash; all available open sources in our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<ul>
<li><a href="http://sequenceiq.com/periscope/">Periscope</a></li>
<li><a href="https://github.com/sequenceiq/docker-baywatch-client">Baywatch client</a></li>
<li><a href="https://github.com/sequenceiq/docker-baywatch">Baywatch</a></li>
</ul>


<p>For updates follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or
<a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real-time monitoring of Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/"/>
    <updated>2014-10-07T18:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are running Hadoop clusters on different environments using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and apply <a href="http://sequenceiq.com/periscope/">SLA autoscaling</a> policies on the fly, thus monitoring the cluster is a key operation.</p>

<p>Although various solutions have been created in the software industry for monitoring of activities taking place in a cluster, but it turned out that only a very few of them satisfies most of our needs. When we made the decision about which monitoring libraries and components to integrate in our stack we kept in mind that it needs to be:</p>

<ul>
<li><p><strong>scalable</strong> to be able to efficiently monitor small Hadoop clusters which are consisting of only a few nodes and also clusters which containing thousands of nodes</p></li>
<li><p><strong>flexible</strong> to be able to provide overview about the health of the whole cluster or about the health of individual nodes or even dive deeper into the internals of Hadoop, e.g. shall be able to visualize how our autoscaling solution for Hadoop YARN called  <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> moves running applications between <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a></p></li>
<li><p><strong>extensible</strong> to be able to use the gathered and stored data by extensions written by 3rd parties, e.g. a module which processes the stored (metrics) data and does real-time anomaly detection</p></li>
<li><p><strong>zero-configuration</strong> to be able to plug into any existing Hadoop cluster without additional configuration, component installation</p></li>
</ul>


<p>Based on the requirements above our choice were the followings:</p>

<ul>
<li><a href="http://logstash.net">Logstash</a> for log/metrics enrichment, parsing and transformation</li>
<li><a href="http://www.elasticsearch.org">Elasticsearch</a> for data storage, indexing</li>
<li><a href="http://www.elasticsearch.org/overview/kibana">Kibana</a> for data visualization</li>
</ul>


<h2>High Level Architecture</h2>

<p>In our monitoring solution one of the design goal was to provide a <strong>generic, pluggable and isolated monitoring component</strong> to existing Hadoop deployments. We also wanted to make it non-invasive and avoid adding any monitoring related dependency to our Ambari, Hadoop or other Docker images. For that reason we have packaged the monitoring client component into its own Docker image which can be launched alongside with a Hadoop running in another container or even alongside a Hadoop which is not even containerized.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png</a>"></p>

<p>In a nutshell the monitoring solution consist of client and server containers. The <code>server</code> contains the Elasticsearch and the Kibana module. The server container is horizontally scalable and it can be clustered trough the clustering capabilities of Elasticsearch.</p>

<p>The <code>client</code> container &ndash; which is deployed on the machine what is needed to be monitored &ndash; contains the Logstash and the collectd module. The Logstash connects to Elasticsearch cluster as client and stores the processed and transformed metrics data there.</p>

<!-- more -->


<h2>Hadoop metrics</h2>

<p>The metrics data what we are collecting and visualizing are provided by <a href="http://blog.cloudera.com/blog/2012/10/what-is-hadoop-metrics2">Hadoop metrics</a>, which is a collection of runtime information that are exposed by all Hadoop daemons. We have configured the Metrics subsystem in that way that it writes the valuable metrics information into the filesystem.</p>

<p>In order to be able to access the metrics data from the monitoring client component &ndash; which is running inside a different Docker container &ndash; we used the capability of <a href="https://docs.docker.com/userguide/dockervolumes">Docker Volumes</a> which basically let&rsquo;s you access a directory within one container form other container or even access directories from host systems.</p>

<p>For example if you would like mount the <code>/var/log</code> from the container named <code>ambari-singlenode</code> under the <code>/amb/log</code> in the monitoring client container then the following sequence of commands needs to be executed:</p>

<p>
<code>bash
EXPOSED_LOG_DIR=$(docker inspect --format='{{index .Volumes "/var/log"}}' ambari-singlenode)
docker run -i -t -v $EXPOSED_LOG_DIR:/amb/log  sequenceiq/baywatch-client /etc/bootstrap.sh -bash
</code>
</p>

<p>Hundreds of different metrics are gathered form Hadoop metrics subsystem and all data is transformed by Logstash to JSON and stored in ElasticSearch to make it ready for querying or displaying it with Kibana.</p>

<p>The screenshot below has been created from one of our sample dashboard which is displaying Hadoop metrics for a small cluster which was started on my notebook. In this cluster the Yarn&rsquo;s Capacity Scheduler is used and for demonstration purposes I have created a queue called <code>highprio</code> alongside the <code>default</code> queue. I have reduced the capacity of the <code>default</code> queue to 30 and defined the <code>highprio</code> queue with a capacity of 70.
The red line in the screenshot belongs to the <code>highprio</code> queue, the yellow line belongs to the <code>default</code> queue and the green line is the <code>root</code> queue which is the common ancestor both of them.
In the benchmark, the jobs were submitted to the <code>default</code> queue and a bit later (somewhere around 17:48) the same jobs were submitted to the <code>highprio</code> queue. As it is clearly observable for <code>highprio</code> queue the allocated Containers, Memory and VCores were higher and jobs were finished much more faster than those that were submitted to the default queue.</p>

<p>Such kind of dashboard is extremely useful when we are visualizing decisions made by <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> and check e.g. how the applications are moved across <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a>, or additional nodes are added or removed dynamically from the cluster.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png</a>"></p>

<p>To see it in large, please <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">click here</a>.</p>

<p>Since all of the Hadoop metrics are stored in the Elasticsearch, therefore there are a lot of possibilities to create different dashboards using that particular parameter of the cluster which is interesting for the operator. The dashboards can be configured on the fly and the metrics are displayed in real-time.</p>

<h2>System resources</h2>

<p>Beside Hadoop metrics, &ldquo;traditional&rdquo; system resource data (cpu, memory, io, network) are gathered with the aid of <a href="https://collectd.org">collectd</a>. This can also run inside the monitoring client container since due to the <a href="https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_example_managing_the_cpu_shares_of_a_container">resource management</a> in Docker the containers can access and gather information about the whole system and a container can even &ldquo;steal&rdquo; the network of other container if you start with: <code>--net=container:id-of-other-container</code> which is very useful if cases when network traffic is monitored.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png</a>"></p>

<h2>Summary</h2>

<p>So far the Hadoop metrics and system resource metrics have been processed, but it is planned to use the information written into the history file (or fetch from History server) and make it also <code>queryable</code> trough Elasticsearch to be able to provide information about what is happening inside the jobs.</p>

<p>The development preview of the monitoring server and client is already available on our GitHub <a href="https://github.com/sequenceiq/docker-elk">here</a> and <a href="https://github.com/sequenceiq/docker-elk-client">here</a>. In the next release this will be part of <strong>Periscope</strong> and <strong>Cloudbreak</strong>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
