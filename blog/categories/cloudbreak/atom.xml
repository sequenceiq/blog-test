<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudbreak | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/cloudbreak/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-04-13T15:18:25+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenStack integration with Cloudbreak]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/02/17/openstack-cloudbreak/"/>
    <updated>2015-02-17T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/02/17/openstack-cloudbreak</id>
    <content type="html"><![CDATA[<p>Cloudbreak can provision HDP clusters on different public cloud providers like Amazon Web Services (AWS), Google Cloud Platform (GCP) and Microsoft Azure. Since the last release Cloudbreak supports provision Hadoop on <a href="https://www.openstack.org/">OpenStack</a> as well. OpenStack is probably the most popular open-source cloud computing platform for private clouds. This blogpost explains in a nutshell how the OpenStack integration was done with Cloudbreak &ndash; in order to provision Hadoop &ndash; but if you are just interested in playing with OpenStack then it is also worth to read as the <strong>Set Up Your Own Private Cloud</strong> section explains how to install DevStack (Openstack suitable for development purposes) with just a few lines of commands.</p>

<h2>Public and Private Clouds</h2>

<p>The overly simplified definition of the two deployment models:</p>

<ul>
<li><strong>public cloud</strong> consists of services that are usually purchased on-demand and provided off-site over the Internet by cloud provider</li>
<li><strong>private cloud</strong> is one in which the services and infrastructure are purchased, maintained and managed within the company</li>
</ul>


<p> <!-- more --></p>

<p>From Cloudbreak&rsquo;s point of view the most important difference is that the services and API of a public cloud is consistent within a provider and it does not really depends on tenants, in other words the AWS provides the same API and same services independently whether Company A or Company B is using it. In case of private cloud, the situation is not so simple, since even if the cloud platform is the same the provided services could be very different. If we take the OpenStack as example then one company can use <a href="http://www.xenproject.org/">XEN</a> as hypervisor, <a href="http://ceph.com/ceph-storage/block-storage/">Ceph</a> as block storage and the Nova network for networking, but another company might use <a href="http://www.linux-kvm.org/">KVM</a>, <a href="https://wiki.openstack.org/wiki/Cinder">Cinder</a> and <a href="https://wiki.openstack.org/wiki/Neutron">Neutron</a> to provide the same functionality. This divergence makes the integration of cloud platforms like OpenStack much more challenging than integrating a public cloud provider.</p>

<h2>Orchestration</h2>

<p>Because of the diversity of OpenStack deployments we decided to use the <a href="https://wiki.openstack.org/wiki/Heat">Heat</a> orchestration service. With the template mechanism of Heat we can describe the infrastructure resources like servers, floating IPs, volumes and security groups for a cloud application in a text file (JSON, YAML or HOT synax) and we can easily adapt this template description in order to support different deployments without changing the code of Cloudbreak.</p>

<p>To make it easier to understand you can take a look at the following template snippet:
```yaml
heat_template_version: 2014-10-16</p>

<p>resources:
  app_network:</p>

<pre><code>type: OS::Neutron::Net
properties:
  admin_state_up: true
  name: app_network
</code></pre>

<p>  app_subnet:</p>

<pre><code>type: OS::Neutron::Subnet
properties:
  network_id: { get_resource: app_network }
  cidr: 10.10.1.0/24
  gateway_ip: 10.10.1.1
  allocation_pools:
    - start: 10.10.1.4
      end: 10.10.1.254
</code></pre>

<p>  &hellip; other resources defined here &hellip;</p>

<p>```</p>

<p>In the above Heat template fragment Neutron (OS::Neutron::Subnet) is used to provide networking services, but in deployments where the Neutron is not supported we would use another type of Network resource definition like Rackspace::Cloud::Network in case of <a href="http://www.rackspace.com/cloud/private">Rackspace</a> based deployments:</p>

<p>```yaml
heat_template_version: 2014-10-16</p>

<p>resources:
  app_network:</p>

<pre><code>type: Rackspace::Cloud::Network
properties:
  cidr: 10.10.1.0/24
  label: app_network
</code></pre>

<p>  &hellip; other resources defined here &hellip;</p>

<p>```</p>

<p>The Heat stack is a model that holds the Heat template, parameters and other meta data related content, when the infrastructure is changed (e.g. new VM is added or removed, or the CIDR of network is changed) then template needs to be modified and can be used to update your existing stack and the Heat knows how to make the necessary changes in the infrastructure in order to satisfy the resource definition described in the updated template.</p>

<p>Although the Heat can manage the whole lifecycle of the application, Cloudbreak uses it only for infrastructure management and the Hadoop provisioning is done by using Ambari and Docker containers exactly in the same way as we do it in other public cloud providers.</p>

<h2>Set Up Your Own Private Cloud</h2>

<p>To try out the OpenStack integration you need to have an OpenStack cluster. If you don&rsquo;t already have one then for development purposes the  <a href="https://wiki.openstack.org/wiki/DevStack">DevStack</a> can be used to quickly deploy an OpenStack cloud. When we started with the integration we have realized that finding a proper configuration and install guide for DevStack is not an easy task. Of course there are plenty of documentations, but rest of them are not very accurate or extremely long or a bit outdated, therefore we have gathered all the necessary pieces of information to set up OpenStack development environment with Neutron, Cinder, Glance, Nova, Horizon and created an <code>Ansible Playbook</code> which we added to a Vagrant config, therefore staring and installing a VM with DevStack is as simple as:</p>

<p><code>bash
$ git clone https://github.com/sequenceiq/sequenceiq-samples
$ cd sequenceiq-samples/devstack/vagrant/devstack-neutron
$ vagrant up
</code>
<em>Note: you need to have Vagrant, VirtualBox and Ansible installed before using the <code>vagrant up</code> command</em></p>

<p>The installation takes at least 30mins, but after that the Horizon UI will be available at <a href="http://192.168.60.10/">http://192.168.60.10/</a> and you can login with user <em>admin</em>  and <em>openstack</em> password. The most important configuration values used for setting up the DevStack can be found <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/devstack/ansible/local-vagrant-vm.yml">here</a>.</p>

<p>So we have launched a whole OpenStack cloud in one single VM running in VirtualBox; as you might guessed already that is just for demonstration purpose of the Ansible install scripts, because as you can see from the configuration <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/devstack/ansible/local-vagrant-vm.yml">file</a> that in case of OpenStack being launched in VirtualBox the <a href="http://wiki.qemu.org/Main_Page">QEMU</a> emulation is used. These instances of the OpenStack launched VMs will be to slow and not suitable for Hadoop installation. But based on this you can always install and configure configure the  <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/devstack/ansible/devstack.yml">devstack.yml</a> and <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/devstack/ansible/hosts">hosts</a> and set up DevStack on a physical machine with KVM virtualization support. After the OpenStack is running you can launch a VM and install Cloudbreak on it and use that Cloudbreak instance to set up VMs and provision HDP on them.</p>

<h2>Future plans</h2>

<p>With the current implementation the Hadoop is provisioned into VMs running on OpenStack, but we are also experimenting with  <a href="http://docs.openstack.org/developer/heat/template_guide/contrib.html#DockerInc::Docker::Container">DockerInc::Docker::Container</a> in order to provision Hadoop cluster directly on the physical machines and avoid the overhead caused by VMs.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cluster extensions with Cloudbreak recipes]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/02/02/cluster-extensions-with-cloudbreak-recipes/"/>
    <updated>2015-02-02T14:59:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/02/02/cluster-extensions-with-cloudbreak-recipes</id>
    <content type="html"><![CDATA[<p>With the help of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari <a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">blueprint</a>. That&rsquo;s cool but it is often needed to make some additional changes on the nodes, like putting a JAR file on the Hadoop classpath or run some custom scripts. To help with these kind of situations we are introducing the concept of Cloudbreak <code>recipes</code>. Recipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation.</p>

<h2>How does it work?</h2>

<p>Since the latest release, Cloudbreak uses <a href="https://consul.io">Consul</a> for cluster membership instead of Serf so we can make use of Consul’s other features, namely <code>events</code> and the <code>key-value</code> store. It won’t be detailed here how these features of Consul work, a whole post about Consul based clusters is coming soon. Recipes are using one more additional thing: the small <a href="https://github.com/progrium/plugn">plugn</a> project.</p>

<p>The main concept behind this is the following: before the cluster install is started, a <code>recipe-pre-install</code> Consul event is sent to the cluster that triggers the <code>recipe-pre-install</code> hook of the enabled plugins, therefore executing the plugins' <code>recipe-pre-install</code> script. After the cluster installation is finished the same happens but with the <code>recipe-post-install</code> event and hook. The key-value store is used to signal plugin success or failure &ndash; after the plugins finished execution on a node, a new Consul key is added in the format <code>/events/&lt;event-id&gt;/&lt;node-hostname&gt;</code> that contains the exit status. Cloudbreak is able to check the key-value store if the recipe finished successfully or not.</p>

<!-- more -->


<h2>Register plugins for a cluster install</h2>

<p>We cannot predict all the custom use cases that can come up when installing a Hadoop cluster in the cloud, so we were focusing on making this feature easily extendable. We had to find a solution that enables someone to write their own script that will be run by Cloudbreak later. That&rsquo;s where the <em>plugn</em> project comes handy. With a simple <code>plugn install</code> command, a new plugin can be installed from <em>Github</em> so we only need to make one plugin available by default on every node &ndash; the <a href="https://github.com/sequenceiq/consul-plugins-install">one</a> that can install other plugins from a Github source. The other plugins are installed as the first step of a Cloudbreak cluster setup. This uses the same mechanism to trigger this plugin &ndash; it sends <code>plugin-install</code> events to Consul’s HTTP interface with the plugin source and name passed as parameters in the Consul event’s payload.</p>

<h3>Creating a plugin</h3>

<p>We’ve created an <a href="https://github.com/sequenceiq/consul-plugins-gcs-connector">example plugin</a> that downloads the <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">Google Cloud Storage connector for Hadoop</a> and puts it on the Hadoop classpath. As you can see a plugin is quite simple &ndash; it consists of a <code>.toml</code> descriptor, and the hook scripts. In the example only the <code>recipe-pre-install</code> hook is implemented, there is nothing to do after the cluster installation is done. Make sure that the hook scripts are executable when pushing them to Github.</p>

<h3>Adding properties to plugins</h3>

<p>Properties can be passed to plugins by using environment variables, but we use Consul&rsquo;s key-value store for that purpose instead. The GCS connector mentioned above needs a few more things to work besides the JAR on the classpath. To be able to authenticate to the Google Cloud Platform the connector also needs a private key in <code>p12</code> format. We have a <a href="https://github.com/sequenceiq/consul-plugins-gcp-p12">plugin</a> that does exactly this &ndash; it reads a <em>base64</em> encoded private key file located under the <code>recipes.gcp-p12.p12-encoded</code> key in the key-value store (using <code>curl</code> and some environment variables containing Consul’s HTTP address) and saves it in a local folder on the node.</p>

<h2>Putting things together</h2>

<p>We already know how to write plugins, how to get properties from the key-value store inside a plugin and how these things are triggered from Cloudbreak, but the key piece is missing: how do we tell <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> which plugins to install on our cluster and which properties to use. With the latest release a new resource is available on the API, the <em>recipe</em>. To create a new recipe make a <code>POST</code> to the <code>account/recipes</code> endpoint like this one:</p>

<p>```
{
  &ldquo;name&rdquo;: &ldquo;gcp-extension&rdquo;,
  &ldquo;description&rdquo;: &ldquo;sets up Google Cloud Storage connector on an Ambari cluster&rdquo;,
  &ldquo;properties&rdquo;: {</p>

<pre><code>"recipes.gcp-p12.p12-encoded": "&lt;base64-encoded-p12-file&gt;"
</code></pre>

<p>  },
  &ldquo;plugins&rdquo;: [</p>

<pre><code>"https://github.com/sequenceiq/consul-plugins-gcs-connector.git",
"https://github.com/sequenceiq/consul-plugins-gcp-p12.git"
</code></pre>

<p>  ]
}
```</p>

<p>To make sure that only trusted plugins are used in Cloudbreak, there is a validation on the source URL &ndash; plugins must come from configurable trusted Github accounts. It can be useful if you deploy your own <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> and want to make sure that only verified scripts can be executed in a cluster. Our hosted solution is configured to accept all Github accounts as trusted, so you can experiment with this feature more easily. After the recipe is created, the API answers with the ID of the created resource, so it can be used to create a cluster. The <code>recipeId</code> field is optional, and no scripts are executed if it is missing from the cluster <code>POST</code> request.</p>

<p><code>
{
  "name": "recipe-cluster",
  "blueprintId": 1400,
  "recipeId": 3744,
  "description": "Demonstrates the recipe feature"
}
</code></p>

<p>The recipes are not yet available on the Cloudbreak UI, if you’d like to try it out without hacking <code>curl</code> requests with proper authentication then I suggest to try the <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak Shell</a>. The requests above correspond to the following shell commands (assuming that the above recipe description is saved in <code>/tmp/test-recipe.json</code> and the cluster infrastructure &ndash; the stack &ndash; is already created):</p>

<p><code>
recipe add --file /tmp/test-recipe.json
recipe select --id &lt;recipe-id&gt;
blueprint select --id &lt;blueprint-id&gt;
</code></p>

<p>```</p>

<p>cluster create &mdash;description &ldquo;Demonstrates the recipe feature&rdquo;
```</p>

<h2>Future improvements</h2>

<p>This feature is <em>just a preview</em> in its current state, there are a few important parts that are missing. The most important one is that the plugins are currently installed and executed in all of the <code>ambari-agent</code> containers, but there are scenarios where it is not needed or not good at all. Consider the case where you’d like to add a JAR to HDFS &ndash; it should be run on only one of the nodes. It is also possible that a script should be executed only on a set of nodes, typically the nodes in a <code>hostgroup</code>. This means that the API will probably change in the next few weeks, but then we’ll update our blog too.</p>

<p>There are a few more things that you can expect to be implemented in the long run:</p>

<ul>
<li>install plugins from private sources too along public Github repositories</li>
<li>validate required properties when creating a new recipe</li>
</ul>


<p>If you have any comments, questions or feature requests about this topic, feel free to create an issue on Cloudbreak’s <a href="https://github.com/sequenceiq/cloudbreak/issues">Github page</a> or use the comments section below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Apache Spark with Cloudbreak]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak/"/>
    <updated>2015-01-30T12:04:22+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak</id>
    <content type="html"><![CDATA[<p>In the previous weeks many of you often asked us how to run our Apache Spark Docker <a href="https://github.com/sequenceiq/docker-spark">container</a> on a multi node cluster or how to install Spark and use it with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.
Cloudbreak uses Ambari (1.7) blueprints to provision multi node HDP clusters (on different cloud providers: AWS, Google Cloud, Azure, Openstack &ndash; with Rackspace and HP Helion coming soon).</p>

<p>In this post we&rsquo;d like to help you with installing Spark on <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> in a quick and easy way.</p>

<p>First of all you will have to create a cluster using Cloudbreak on your favorite cloud provider &ndash; Google Cloud, AWS, Azure or Openstack (check this <a href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/">post</a>) using a simple <code>multi-node-hdfs-yarn</code> blueprint. After your cluster is ready, you can install Apache Spark with the following steps:</p>

<h3>Install from the cloud instance</h3>

<p>First, you need to enter to one of your cloud instances. Then use the one-liner below:</p>

<p><code>
curl -Lo .docker-spark-install j.mp/spark-hdp-docker-install &amp;&amp; . .docker-spark-install
</code></p>

<p>After the file is downloaded it will be sourced, then you can use the following command:</p>

<p><code>
install-spark ambari-agent install
</code></p>

<!--more-->


<p>Alternatively you can install it without uploading the Spark assembly <code>uberjar</code> using :</p>

<p><code>
install-spark ambari-agent install-local
</code></p>

<p>After it is done, enter into the ambari-agent container:</p>

<p><code>
docker exec -it ambari-agent bash
</code></p>

<p>Apache Spark will be installed at /usr/local/spark in the container. If you want to try it you need to configure a few environment variables such as YARN_CONF_DIR or SPARK_JAR (see the Install from container option)</p>

<h3>Install from container</h3>

<p>If you entered in one of your cloud instances, enter into the ambari-agent container: (same as you seen above):</p>

<p><code>
docker exec -it ambari-agent bash
</code></p>

<p>Inside the container use the following command:</p>

<p><code>
curl -Lo .spark-install j.mp/spark-hdp-install &amp;&amp; . .spark-install
</code></p>

<p>Then you can install spark with &ldquo;install-spark &lt;install/install-local>&rdquo; command:</p>

<p><code>
install-spark install
</code></p>

<p>With this approach you do not need to set up your environment variables. The script will do it for you.</p>

<h2>Run examples</h2>

<p><code>
spark-submit --class org.apache.spark.examples.SparkPi    --master yarn-cluster  --num-executors 3 --driver-memory 512m  --executor-memory 512m  --executor-cores 1  /usr/local/spark/lib/spark-examples*.jar 10
</code></p>

<h2>Issues</h2>

<ul>
<li>You need to install Spark into every node (you can use it only on 1, but it is not the best approach).</li>
<li>You do not want to enter docker container or even cloud instances to do things like this.</li>
</ul>


<p>As you see until Ambari will not fully support Spark installation with Blueprints this is not an ideal situation.</p>

<p>Nevertheless we understood this and with the introduction of <strong>recipes</strong> in the latest Cloudbreak release we are going to publish a new Cloudbreal Spark recipe next week. In the meanwhile stay tuned as we are publishing a post early next week about the concept and architecture of <code>recipes</code>, how to use it and will publish a few custom ones (by request).</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - new release available]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta/"/>
    <updated>2015-01-28T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta</id>
    <content type="html"><![CDATA[<p>We are happy to announce that the <code>Release Candidate</code> version of Cloudbreak is almost around the corner. This major release is the last one in the <code>public beta program</code> and contains quite a few new features and architectural changes.</p>

<p>All theses new major features will be covered in the coming weeks on our blog, but in the meantime let us do a quick skim through.</p>

<h3>Accounts</h3>

<p>We have introduced the concept of <code>accounts</code> &ndash; after a user registers/signs in the first time will have the option to invite other people in the <code>account</code>. Being the administrator of the account, will have the option to activate, deactive and give admin rights for all the invited users.</p>

<p>Users can share <code>resources</code> (such as: cloud credentials, templates, blueprints, clusters) within the account by making it <code>public in account</code> but at the same time can create his own private resources as well. As you might be already aware, we use OAuth2 to make all these possible.</p>

<h3>Usage explorer</h3>

<p>We have built a unified (accross all cloud providers) usage explorer tool, where you can drill down into details to learn your (or in your account if you have admin rights) usage history. You can filter by date, users, cloud providers, region, etc &ndash; and generate a consolidated table/chart overview.</p>

<h3>Heterogenous clusters</h3>

<p>This was a feature many have asked &ndash; and we are happy to deliver it. Up till now all the nodes in your YARN clusters were built on the same cloud <code>instance types</code>. While this was an easy an convenient way to build a cluster (as far as we are aware all the Hadoop as a Service providers are doing it this way) back in the MR1 era, times changed now and with the emergence of <code>YARN</code> different workloads are running within a cluster.</p>

<p>While for example Spark jobs require a high memory instance a legacy MR2 code might require a high CPU instance, whereas a HBase RegionServer likes better a high I/O throughput one.</p>

<p>At SequenceIQ we have quickly realized this and the new release allows you to apply different <code>stack templates</code> to all these YARN services/components. We do the heavy lifting for you in the background &ndash; the only thing you will have to do is to associate stack templates to Ambari <code>hostgroups</code>.</p>

<p>This is a major step forward when you are using and running different workloads on your YARN cluster &ndash; and not just saving on costs but at the same time increasing your cluster throughput.</p>

<!--more-->


<h3>Hostgroup based autoscaling</h3>

<p>Cloudbreak now integrates with <a href="http://sequenceiq.com/periscope">Periscope</a> &ndash; and allows you to set up alarms and autoscaling SLA policies based on YARN metrics. Having done the heterogenous cluster integration, now it&rsquo;s time to apply <code>autoscaling</code> for those nodes based on Ambari Blueprints.</p>

<h3>Recipes</h3>

<p>While Cloudbreak and Ambari combined are a pretty powerful way to configure your Hadoop cluster, sometimes there are manuall steps required to reconfigure services, build dependent cluster architectures (e.g.: permanent and ephemeral clusters), etc &ndash; the list can be long.
Even a simple configuration on a large (thousands nodes) cluster is a tedious job &ndash; and usually people use Ansible, Chef, Puppet or Saltstack to do so &ndash; however these all have some drawback and are not integrated with Cloudbreak. As Cloudbreak under the hood uses Consul, we came up with a simple solution which facilitates creating, applying and running <code>recipes</code> on your already provisioned cluster &ndash; pre/post Ambari installation. A follow up blog post will be released in the coming days whch will explain the concept, architecture and gives you a few sample recipes.</p>

<h3>OpenStack</h3>

<p>This was one of the other highly desired features &ndash; and a perfect use case for Docker. You might be aware that we run the full Hadoop stack inside Docker container &ndash; and Cloudbreak&rsquo;s integration with the cloud provider is pretty thin. This gives us the option to add quick integration with a new cloud provider &ndash; the full OpenStack integration with Cloudbreak took few weeks only.</p>

<p>Long story short &ndash; Cloudbreak now support and automates provisioning of Hadoop clusters with custom blueprints on OpenStack. Give it a try and let us know how it works for you.</p>

<h3>Ambari 1.7 integration</h3>

<p>Shortly after Ambari 1.7 came out we have upgraded Cloudbreak to use this new version. Ambari 1.7 supports HDP and Bigtop stacks and an increased number of Hadoop services/components.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Cloudbreak release - support for HDP 2.2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/"/>
    <updated>2014-12-23T12:59:42+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2</id>
    <content type="html"><![CDATA[<p>The last two weeks were pretty busy for us &ndash; we have <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">Dockerized</a> the new release of Ambari (1.7.0), <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">integrated</a> Periscope with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Cloudbreak</a> and just now we are announcing a new Cloudbreak <a href="https://cloudbreak.sequenceiq.com">release</a> which uses Ambari 1.7.0 and has full support for Hortonworks HDP 2.2 and Apache Bigtop stacks. But first &ndash; since this has been asked many times &ndash; see a <code>short</code> movie about Cloudbreak and Periscope in action.</p>

<h2>On-demand Hadoop cluster with autoscaling</h2>

<iframe width="640" height="480" src="//www.youtube.com/embed/E6bnEW76H_E" frameborder="0" allowfullscreen></iframe>




<!--more-->


<h2>Ambari 1.7.0</h2>

<p>The Ambari community recently released the 1.7.0 version which comes with lots of new features and bug fixes. We&rsquo;ve been testing the new version
internally for a while now and finally made it to Cloudbreak. Just to highlight the important ones:</p>

<ul>
<li>Ambari Views framework</li>
<li>Ambari Administration

<ul>
<li>Management of users/groups</li>
<li>Management of view instances</li>
<li>Management of cluster permissions</li>
</ul>
</li>
<li>Cancel/Abort background operation requests</li>
<li>Expose Ambari UI for config versioning, history and rollback</li>
<li>Ability to manage -env.sh configuration files</li>
<li>Recommendations and validations (via a &ldquo;Stack Advisor&rdquo;)</li>
<li>Export service configurations via Blueprint</li>
<li>Install + Manage Flume</li>
<li>HDFS Rebalance</li>
<li>ResourceManager HA</li>
</ul>


<p>These are nice features but for us one of the most important thing is that it allows you to install the latest versions of the Hadoop ecosystem.
As usual the Docker image is available for <em>local</em> deployments as well, described <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">here</a>.</p>

<p><code>Note: There were small changes around the API so if you built an application on top of it check your REST calls. The Ambari Shell and the
underlying Groovy rest client have been updated and will go into the Apache repository once it's passed the reviews.</code></p>

<h2>Hadoop 2.6</h2>

<p>Since with Ambari 1.7.0 we&rsquo;re able to install Hadoop 2.6 let&rsquo;s see what happened in <code>YARN</code> in the last couple of months (it&rsquo;s stunning):</p>

<ul>
<li>Support for long running services &ndash; install <em>Slider</em> with Ambari and scale your Hadoop services!

<ul>
<li>Service Registry for applications</li>
</ul>
</li>
<li>Support for rolling upgrades &ndash; wow!

<ul>
<li>Work-preserving restarts of ResourceManager</li>
<li>Container-preserving restart of NodeManager</li>
</ul>
</li>
<li>Supports node labels during scheduling &ndash; label based scaling is on the way with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Periscope</a></li>
<li>Support for time-based resource reservations in Capacity Scheduler (beta) &ndash; more on this awesome feature soon</li>
<li>Support running of applications natively in Docker containers (alpha) &ndash; <a href="http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker/">Docker in Docker</a></li>
</ul>


<p>I&rsquo;m excited about these great innovations (not, because we&rsquo;re involved in a few of them), but because people can leverage them by using Cloudbreak.</p>

<h2>HDP 2.2 blueprint</h2>

<p>I have created a blueprint which is not an <code>official</code> one, but it contains a few from the new services like: <code>SLIDER</code>, <code>KAFKA</code>, <code>FLUME</code>.
```
{
  &ldquo;configurations&rdquo;: [
  {</p>

<pre><code>"nagios-env": {
  "nagios_contact": "admin@localhost"
}
},
{
  "hive-site": {
    "javax.jdo.option.ConnectionUserName": "hive",
    "javax.jdo.option.ConnectionPassword": "hive"
  }
}
</code></pre>

<p>  ],
  &ldquo;host_groups&rdquo;: [</p>

<pre><code>{
  "name": "master_1",
  "components": [
    {
      "name": "NAMENODE"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "HBASE_MASTER"
    },
    {
      "name": "GANGLIA_SERVER"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_SERVER"
    },
    {
      "name": "FLUME_HANDLER"
    },
    {
      "name": "KAFKA_BROKER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_2",
  "components": [
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "HISTORYSERVER"
    },
    {
      "name": "HIVE_SERVER"
    },
    {
      "name": "SECONDARY_NAMENODE"
    },
    {
      "name": "HIVE_METASTORE"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MYSQL_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "WEBHCAT_SERVER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_3",
  "components": [
    {
      "name": "RESOURCEMANAGER"
    },
    {
      "name": "APP_TIMELINE_SERVER"
    },
    {
      "name": "SLIDER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_4",
  "components": [
    {
      "name": "OOZIE_SERVER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "slave_1",
  "components": [
    {
      "name": "HBASE_REGIONSERVER"
    },
    {
      "name": "NODEMANAGER"
    },
    {
      "name": "DATANODE"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_CLIENT"
    },
    {
      "name": "OOZIE_CLIENT"
    }
  ],
  "cardinality": "${slavesCount}"
},
{
  "name": "gateway",
  "components": [
    {
      "name": "AMBARI_SERVER"
    },
    {
      "name": "NAGIOS_SERVER"
    },
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "PIG"
    },
    {
      "name": "OOZIE_CLIENT"
    },
    {
      "name": "HBASE_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "SQOOP"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MAPREDUCE2_CLIENT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "KNOX_GATEWAY"
    }
  ],
  "cardinality": "1"
}
],
"Blueprints": {
  "blueprint_name": "hdp-multinode-sequenceiq",
  "stack_name": "HDP",
  "stack_version": "2.2"
}
</code></pre>

<p>}
```</p>

<h2>What&rsquo;s next?</h2>

<p><blockquote><p>Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.</p></blockquote></p>

<p>We&rsquo;ve walked a long journey since we started the company almost a year ago to reach where we are now, but our products are not complete yet. We have big plans
with our product stacks. A couple of things from our roadmap:</p>

<h3>Cloudbreak</h3>

<ul>
<li>Cloudbreak currently supports homogeneous cluster deployments which we&rsquo;re going to change. The heterogeneous stack structure is more convenient
from Hadoop&rsquo;s perspective. The ability to define different type of cloud instances is a must, giving the users the option to use much more
powerful instances for the <code>ResourceManager</code> and <code>NameNodes</code>.</li>
<li>Service discovery and decentralization is always a key aspect. At the moment we&rsquo;re using Serf and dnsmasq, but we&rsquo;re already started the
integration with <a href="https://consul.io">Consul</a> which generally is a better fit. It provides service registration via DNS, key-value store and
decentralization across datacenters.</li>
<li>The deployment of Cloudbreak itself is going to change and use Consul with other side projects like <code>Consul templates</code> or <code>Registrator</code>. The
deployment is already based on Docker, but will be much more simplified.</li>
<li>Custom stack deployments with Ambari will be supported as <code>"recipes"</code>.</li>
<li>Generating reports of cloud instance usages and cost calculation.</li>
<li>Web hooks to subscribe to different cluster events.</li>
<li>Shared/company accounts.</li>
</ul>


<h3>Periscope</h3>

<ul>
<li>Add more <code>YARN</code> and <code>NameNode</code> related metrics.</li>
<li>Node label based scaling.</li>
<li>Pluggable metric system for custom metrics.</li>
<li>Application movement in Capacity Scheduler queues enforcing SLAs.</li>
<li>Time-based resource reservations in Capacity Scheduler for Applications.</li>
<li>Integration with <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">ELK</a>.</li>
</ul>


<h2>Happy Holidays</h2>

<p>We&rsquo;re taking a short break of writing new blog posts until next year. You can still reach us on the usual social sites, but you can expect
small delays for answering questions. <code>Happy Holidays everyone.</code></p>

<p><a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> <a href="https://twitter.com/sequenceiq">Twitter</a> <a href="https://www.facebook">Facebook</a></p>
]]></content>
  </entry>
  
</feed>
