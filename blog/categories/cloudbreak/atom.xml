<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudbreak | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/cloudbreak/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-01-29T11:56:19+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - new release available]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta/"/>
    <updated>2015-01-28T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta</id>
    <content type="html"><![CDATA[<p>We are happy to announce that the <code>Release Candidate</code> version of Cloudbreak is almost around the corner. This major release is the last one in the <code>public beta program</code> and contains quite a few new features and architectural changes.</p>

<p>All theses new major features will be covered in the coming weeks on our blog, but in the meantime let us do a quick skim through.</p>

<h3>Accounts</h3>

<p>We have introduced the concept of <code>accounts</code> &ndash; after a user registers/signs in the first time will have the option to invite other people in the <code>account</code>. Being the administrator of the account, will have the option to activate, deactive and give admin rights for all the invited users.</p>

<p>Users can share <code>resources</code> (such as: cloud credentials, templates, blueprints, clusters) within the account by making it <code>public in account</code> but at the same time can create his own private resources as well. As you might be already aware, we use OAuth2 to make all these possible.</p>

<h3>Usage explorer</h3>

<p>We have built a unified (accross all cloud providers) usage explorer tool, where you can drill down into details to learn your (or in your account if you have admin rights) usage history. You can filter by date, users, cloud providers, region, etc &ndash; and generate a consolidated table/chart overview.</p>

<h3>Heterogenous clusters</h3>

<p>This was a feature many have asked &ndash; and we are happy to deliver it. Up till now all the nodes in your YARN clusters were built on the same cloud <code>instance types</code>. While this was an easy an convenient way to build a cluster (as far as we are aware all the Hadoop as a Service providers are doing it this way) back in the MR1 era, times changed now and with the emergence of <code>YARN</code> different workloads are running within a cluster.</p>

<p>While for example Spark jobs require a high memory instance a legacy MR2 code might require a high CPU instance, whereas a HBase RegionServer likes better a high I/O throughput one.</p>

<p>At SequenceIQ we have quickly realized this and the new release allows you to apply different <code>stack templates</code> to all these YARN services/components. We do the heavy lifting for you in the background &ndash; the only thing you will have to do is to associate stack templates to Ambari <code>hostgroups</code>.</p>

<p>This is a major step forward when you are using and running different workloads on your YARN cluster &ndash; and not just saving on costs but at the same time increasing your cluster throughput.</p>

<!--more-->


<h3>Hostgroup based autoscaling</h3>

<p>Cloudbreak now integrates with <a href="http://sequenceiq.com/periscope">Periscope</a> &ndash; and allows you to set up alarms and autoscaling SLA policies based on YARN metrics. Having done the heterogenous cluster integration, now it&rsquo;s time to apply <code>autoscaleing</code> for those nodes based on Ambari Blueprints.</p>

<h3>Recipes</h3>

<p>While Cloudbreak and Ambari combined are a pretty powerful way to configure your Hadoop cluster, sometimes there are manuall steps required to reconfigure services, build dependent cluster architectures (e.g.: permanent and ephemeral clusters), etc &ndash; the list can be long.
Even a simple configuration on a large (thousands nodes) cluster is a tedious job &ndash; and usually people use Ansible, Chef, Puppet or Saltstack to do so &ndash; however these all have some drawback and are not integrated with Cloudbreak. As Cloudbreak under the hood uses Consul, we came up with a simple solution which facilitates creating, applying and running <code>recipes</code> on your already provisioned cluster &ndash; pre/post Ambari installation. A follow up blog post will be released in the coming days whch will explain the concept, architecture and gives you a few sample recipes.</p>

<h3>OpenStack</h3>

<p>This was one of the other highly desired features &ndash; and a perfect use case for Docker. You might be aware that we run the full Hadoop stack inside Docker container &ndash; and Cloudbreak&rsquo;s integration with the cloud provider is pretty thin. This gives us the option to add quick integration with a new cloud provider &ndash; the full OpenStack integration with Cloudbreak took few weeks only.</p>

<p>Long story short &ndash; Cloudbreak now support and automates provisioning of Hadoop clusters with custom blueprints on OpenStack. Give it a try and let us know how it works for you.</p>

<h3>Ambari 1.7 integration</h3>

<p>Shortly after Ambari 1.7 came out we have upgraded Cloudbreak to use this new version. Ambari 1.7 supports HDP and Bigtop stacks and an increased number of Hadoop services/components.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Cloudbreak release - support for HDP 2.2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/"/>
    <updated>2014-12-23T12:59:42+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2</id>
    <content type="html"><![CDATA[<p>The last two weeks were pretty busy for us &ndash; we have <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">Dockerized</a> the new release of Ambari (1.7.0), <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">integrated</a> Periscope with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Cloudbreak</a> and just now we are announcing a new Cloudbreak <a href="https://cloudbreak.sequenceiq.com">release</a> which uses Ambari 1.7.0 and has full support for Hortonworks HDP 2.2 and Apache Bigtop stacks. But first &ndash; since this has been asked many times &ndash; see a <code>short</code> movie about Cloudbreak and Periscope in action.</p>

<h2>On-demand Hadoop cluster with autoscaling</h2>

<iframe width="640" height="480" src="//www.youtube.com/embed/E6bnEW76H_E" frameborder="0" allowfullscreen></iframe>




<!--more-->


<h2>Ambari 1.7.0</h2>

<p>The Ambari community recently released the 1.7.0 version which comes with lots of new features and bug fixes. We&rsquo;ve been testing the new version
internally for a while now and finally made it to Cloudbreak. Just to highlight the important ones:</p>

<ul>
<li>Ambari Views framework</li>
<li>Ambari Administration

<ul>
<li>Management of users/groups</li>
<li>Management of view instances</li>
<li>Management of cluster permissions</li>
</ul>
</li>
<li>Cancel/Abort background operation requests</li>
<li>Expose Ambari UI for config versioning, history and rollback</li>
<li>Ability to manage -env.sh configuration files</li>
<li>Recommendations and validations (via a &ldquo;Stack Advisor&rdquo;)</li>
<li>Export service configurations via Blueprint</li>
<li>Install + Manage Flume</li>
<li>HDFS Rebalance</li>
<li>ResourceManager HA</li>
</ul>


<p>These are nice features but for us one of the most important thing is that it allows you to install the latest versions of the Hadoop ecosystem.
As usual the Docker image is available for <em>local</em> deployments as well, described <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">here</a>.</p>

<p><code>Note: There were small changes around the API so if you built an application on top of it check your REST calls. The Ambari Shell and the
underlying Groovy rest client have been updated and will go into the Apache repository once it's passed the reviews.</code></p>

<h2>Hadoop 2.6</h2>

<p>Since with Ambari 1.7.0 we&rsquo;re able to install Hadoop 2.6 let&rsquo;s see what happened in <code>YARN</code> in the last couple of months (it&rsquo;s stunning):</p>

<ul>
<li>Support for long running services &ndash; install <em>Slider</em> with Ambari and scale your Hadoop services!

<ul>
<li>Service Registry for applications</li>
</ul>
</li>
<li>Support for rolling upgrades &ndash; wow!

<ul>
<li>Work-preserving restarts of ResourceManager</li>
<li>Container-preserving restart of NodeManager</li>
</ul>
</li>
<li>Supports node labels during scheduling &ndash; label based scaling is on the way with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Periscope</a></li>
<li>Support for time-based resource reservations in Capacity Scheduler (beta) &ndash; more on this awesome feature soon</li>
<li>Support running of applications natively in Docker containers (alpha) &ndash; <a href="http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker/">Docker in Docker</a></li>
</ul>


<p>I&rsquo;m excited about these great innovations (not, because we&rsquo;re involved in a few of them), but because people can leverage them by using Cloudbreak.</p>

<h2>HDP 2.2 blueprint</h2>

<p>I have created a blueprint which is not an <code>official</code> one, but it contains a few from the new services like: <code>SLIDER</code>, <code>KAFKA</code>, <code>FLUME</code>.
```
{
  &ldquo;configurations&rdquo;: [
  {</p>

<pre><code>"nagios-env": {
  "nagios_contact": "admin@localhost"
}
},
{
  "hive-site": {
    "javax.jdo.option.ConnectionUserName": "hive",
    "javax.jdo.option.ConnectionPassword": "hive"
  }
}
</code></pre>

<p>  ],
  &ldquo;host_groups&rdquo;: [</p>

<pre><code>{
  "name": "master_1",
  "components": [
    {
      "name": "NAMENODE"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "HBASE_MASTER"
    },
    {
      "name": "GANGLIA_SERVER"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_SERVER"
    },
    {
      "name": "FLUME_HANDLER"
    },
    {
      "name": "KAFKA_BROKER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_2",
  "components": [
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "HISTORYSERVER"
    },
    {
      "name": "HIVE_SERVER"
    },
    {
      "name": "SECONDARY_NAMENODE"
    },
    {
      "name": "HIVE_METASTORE"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MYSQL_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "WEBHCAT_SERVER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_3",
  "components": [
    {
      "name": "RESOURCEMANAGER"
    },
    {
      "name": "APP_TIMELINE_SERVER"
    },
    {
      "name": "SLIDER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_4",
  "components": [
    {
      "name": "OOZIE_SERVER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "slave_1",
  "components": [
    {
      "name": "HBASE_REGIONSERVER"
    },
    {
      "name": "NODEMANAGER"
    },
    {
      "name": "DATANODE"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_CLIENT"
    },
    {
      "name": "OOZIE_CLIENT"
    }
  ],
  "cardinality": "${slavesCount}"
},
{
  "name": "gateway",
  "components": [
    {
      "name": "AMBARI_SERVER"
    },
    {
      "name": "NAGIOS_SERVER"
    },
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "PIG"
    },
    {
      "name": "OOZIE_CLIENT"
    },
    {
      "name": "HBASE_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "SQOOP"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MAPREDUCE2_CLIENT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "KNOX_GATEWAY"
    }
  ],
  "cardinality": "1"
}
],
"Blueprints": {
  "blueprint_name": "hdp-multinode-sequenceiq",
  "stack_name": "HDP",
  "stack_version": "2.2"
}
</code></pre>

<p>}
```</p>

<h2>What&rsquo;s next?</h2>

<p><blockquote><p>Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.</p></blockquote></p>

<p>We&rsquo;ve walked a long journey since we started the company almost a year ago to reach where we are now, but our products are not complete yet. We have big plans
with our product stacks. A couple of things from our roadmap:</p>

<h3>Cloudbreak</h3>

<ul>
<li>Cloudbreak currently supports homogeneous cluster deployments which we&rsquo;re going to change. The heterogeneous stack structure is more convenient
from Hadoop&rsquo;s perspective. The ability to define different type of cloud instances is a must, giving the users the option to use much more
powerful instances for the <code>ResourceManager</code> and <code>NameNodes</code>.</li>
<li>Service discovery and decentralization is always a key aspect. At the moment we&rsquo;re using Serf and dnsmasq, but we&rsquo;re already started the
integration with <a href="https://consul.io">Consul</a> which generally is a better fit. It provides service registration via DNS, key-value store and
decentralization across datacenters.</li>
<li>The deployment of Cloudbreak itself is going to change and use Consul with other side projects like <code>Consul templates</code> or <code>Registrator</code>. The
deployment is already based on Docker, but will be much more simplified.</li>
<li>Custom stack deployments with Ambari will be supported as <code>"recipes"</code>.</li>
<li>Generating reports of cloud instance usages and cost calculation.</li>
<li>Web hooks to subscribe to different cluster events.</li>
<li>Shared/company accounts.</li>
</ul>


<h3>Periscope</h3>

<ul>
<li>Add more <code>YARN</code> and <code>NameNode</code> related metrics.</li>
<li>Node label based scaling.</li>
<li>Pluggable metric system for custom metrics.</li>
<li>Application movement in Capacity Scheduler queues enforcing SLAs.</li>
<li>Time-based resource reservations in Capacity Scheduler for Applications.</li>
<li>Integration with <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">ELK</a>.</li>
</ul>


<h2>Happy Holidays</h2>

<p>We&rsquo;re taking a short break of writing new blog posts until next year. You can still reach us on the usual social sites, but you can expect
small delays for answering questions. <code>Happy Holidays everyone.</code></p>

<p><a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> <a href="https://twitter.com/sequenceiq">Twitter</a> <a href="https://www.facebook">Facebook</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak welcomes Periscope]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/"/>
    <updated>2014-12-12T14:13:33+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope</id>
    <content type="html"><![CDATA[<p>Today we have pushed out a new release of <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> &ndash; our Docker container based and cloud agnostic Hadoop as a Service solution &ndash; containing a few major changes. While there are many significant changes (both functional and architectural) in this blog post we&rsquo;d like to describe one of the most expected one &ndash; the <code>autoscaling</code> of Hadoop clusters.</p>

<p>Just to quickly recap, Cloudbreak allows you to provision clusters &ndash; <code>full stacks</code> &ndash; in all major cloud providers using a unified API, UI or CLI/shell. Currently we support provisioning of clusters in <code>AWS</code>, <code>Google Cloud</code>, <code>Azure</code> and <code>OpenStack</code> (in private beta) &ndash; new cloud providers can be added quite easily (as everything runs in Docker) using our SDK.</p>

<p><a href="http://sequenceiq.com/periscope/">Periscope</a> allows you to configure SLA policies for your Hadoop cluster and scale up or down on demand. You are able to set alarms and notifications for different metrics like <code>pending containers</code>, <code>lost nodes</code> or <code>memory usage</code>, etc and set SLA scaling policies based on these alarms.</p>

<p>Today&rsquo;s <a href="http://cloudbreak.sequenceiq.com/">release</a> made available the integration between the two projects (they work independently as well) and allows subscribers to enable autoscaling for their already deployed or newly created Hadoop cluster.</p>

<p>We would like to guide you through the UI and help you to set up an autoscaling Hadoop cluster.</p>

<!--more-->


<h2>Using Periscope</h2>

<p>Once you have created your Hadoop clusters with Cloudbreak you will now how the option to configure autoscaling policies.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/select.png" alt="" /></p>

<p>In order to configure autoscaling for your cluster you should go to <code>autoscaling SLA policies</code> tab and hit the <code>enable</code> button.</p>

<h3>Alarms</h3>

<p>Periscope allows you to configure two types of <code>alarms</code>.</p>

<p><strong>Metric based</strong> alarms are alarms based on different <code>YARN</code> metrics. A plugin mechanism will be available in case you&rsquo;d like to plug your own metrics. As a quick note, we have another project called <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">Baywatch</a> where we collect around 400 Hadoop metrics &ndash; and those will be all pluggable in Periscope.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-metric.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>metrics &ndash; currently the default YARN metrics we support are: <code>pending containers</code>, <code>pending applications</code>, <code>lost nodes</code>, <code>unhealthy nodes</code> and <code>global resources</code></li>
<li>period &ndash;  the time that the metric has to be sustained in order for an alarm to be triggered</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<p><strong>Time based</strong> alarms allow autoscaling of clusters based on the configured time. We have <a href="http://blog.sequenceiq.com/blog/2014/11/25/periscope-scale-your-cluster-on-time/">blogged</a> about this new feature recently &ndash; with this new release of <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> this feature is available through UI as well.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-time.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>time zone &ndash; the timezone for the <code>cron</code> expression</li>
<li>cron expression &ndash; the cron expression</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<h2>Scaling policies</h2>

<p>Once you have an alarm you can configure scaling policies based on it. Scaling policies defines the actions you&rsquo;d like Periscope to take in case of a triggered alarm.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/scaling.png" alt="" /></p>

<ul>
<li>policy name &ndash; the name of the SLA scaling policy</li>
<li>scaling adjustment &ndash; the adjustment counted in <code>nodes</code>, <code>percentage</code> or <code>exact</code> numbers of cluster nodes</li>
<li>host group &ndash; the <code>autoscaled</code> Ambari hostgroup</li>
<li>alarm &ndash; the configured alarm</li>
</ul>


<h2>Cluster scaling configurations</h2>

<p>A cluster has a default configuration which Periscope scaling policies can&rsquo;t override. This is due to avoid over or under scaling a Hadoop cluster with policies and also to definde a cooldown time period between two scaling actions.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/cluster-config.png" alt="" /></p>

<ul>
<li>cooldown time &ndash; the time spent between two scaling actions</li>
<li>cluster size min. &ndash; the minimum size (in nodes) of a cluster</li>
<li>cluster size max. &ndash; the maximum size (in nodes) of a cluster</li>
</ul>


<p>It&rsquo;s that simple. Happy autoscaling.</p>

<p>In case you&rsquo;d like to test autoscaling and generate some load on your cluster you can use these <code>stock</code> Hadoop examples and the scripts below:</p>

<p>```</p>

<h1>!/bin/bash</h1>

<p>export HADOOP_LIBS=/usr/lib/hadoop-mapreduce
export JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient-2.4.0.2.1.2.0-402-tests.jar</p>

<p>smalljobs(){
  echo &ldquo;############################################&rdquo;
  echo Running smalljobs tests..
  echo &ldquo;############################################&rdquo;</p>

<p>  CMD=&ldquo;hadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/hrt_qa/smallJobsBenchmark -numRuns 2 -maps 10 -reduces 5 -inputLines 10 -inputType ascending&rdquo;
  echo TEST 1: $CMD
  su hdfs -c &ldquo;$CMD&rdquo; 1> smalljobs-time.log 2> smalljobs.log
}</p>

<p>smalljobs
```</p>

<p>To test it you can run it with the following script:</p>

<p>```</p>

<h1>!/bin/bash</h1>

<p>for i in {1..10}
do
nohup /test.sh &amp;
done
```</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the data lake in the cloud - Part2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/17/datalake-cloudbreak-2/"/>
    <updated>2014-11-17T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/17/datalake-cloudbreak-2</id>
    <content type="html"><![CDATA[<p>Few weeks ago we had a <a href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/">post</a> about building a <code>data lake</code> in the cloud using a cloud based <code>object storage</code> as the primary file system.
In this post we&rsquo;d like to move forward and show you how to create an <code>always on</code> persistent datalake with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and create <code>ephemeral</code> clusters which can be scaled up and down based on configured SLA policies using <a href="http://sequenceiq.com/periscope/">Periscope</a>.</p>

<p>Just as a quick reminder &ndash; both are open source projects under Apache2 license and the documentation and code is available following these links below.</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<h2>Sample architecture</h2>

<p>For the sample use case we will create a <code>datalake</code> on <strong>AWS</strong> and <strong>Google Cloud</strong> as well &ndash; and use the most popular data warehouse software with an SQL interface &ndash; <a href="https://hive.apache.org/">Apache Hive</a>.</p>

<!--more-->


<p>From Hive perspective (simplified) while building the <code>datalake</code> there are tree main components:</p>

<ul>
<li>Hive warehouse &ndash; the location where the raw data is stored. Usually it&rsquo;s HDFS, in our case it&rsquo;s the <code>object store</code> &ndash; <strong>Amazon S3</strong> or <strong>Google Cloud Storage</strong></li>
<li>Hive metastore service &ndash; the Hive metastore service stores the metadata for Hive tables and partitions in a relational database &ndash; aka: <strong>metastore DB</strong>, and provides clients (including Hive) access to this information</li>
<li>Metastore database &ndash; a database implementation where the metastore information is stored and the local/remote metastore services talk to, over a JDBC interface</li>
</ul>


<p>The proposed sample architecture is shown on the diagram below &ndash; we have a <strong>permanent</strong> cluster which contains the <code>metastore database</code> and a local <code>metastore service</code>, an <strong>ephemeral</strong> cluster where the <code>metastore service</code> talks to a remote <code>metastore database</code> and the Hive <code>warehouse</code> with the data being stored in the cloud provider&rsquo;s <code>object store</code>.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/blog-test/source/source/images/hive-metastore/hive-permanent-ephemeral.jpg" alt="" /></p>

<p>Setting up a an architecture as such can be pretty complicated and involves a few steps &ndash; where many things could go wrong.</p>

<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we try to automate all these steps and build into our product stack &ndash; and we did exactly the same with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. While a default Hive metastore cluster can be created in a fully automated manner using Cloudbreak <code>blueprints</code> in case of different cloud providers (remember we support AWS, Google Cloud and Azure, Open Stack in the pipeline) there are settings which you will need to apply on each nodes, reconfigure services, etc &ndash; and on a large cluster this is pretty awkward.
Because of these in the next release of Cloudbreak we introduce a new concept called <strong>recipes</strong>. A recipe will embed a full architectural representation of the Hadoop stack &ndash; incorporating all the necessary settings, service configurations &ndash; and allows the end user to bring up clusters as the one(s) discussed in this blog &ndash; with a push of a button, API call or CLI interface.</p>

<h2>Permanent cluster &ndash; on AWS and Google Cloud</h2>

<p>Both Amazon EC2 and Google Cloud allows you to set up a permanent cluster and use their <code>object store</code> for the Hive warehouse. You can set up these clusters with <a href="http://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; overriding the default configurations in the blueprints.</p>

<h4>Using AWS S3 as the Hive warehouse</h4>

<p>This setup will use the S3 Block FileSystem &ndash; as a quick note you need to remember that this is not interoperable with other S3 tools.</p>

<p>```</p>

<pre><code>{
  "core-site": {
    "fs.s3.awsAccessKeyId": "YOUR ACCESS KEY",
    "fs.s3.awsSecretAccessKey": "YOUR SECRET KEY"
  }
},
{
  "hive-site": {
    "hive.metastore.warehouse.dir": "s3://siq-hadoop/apps/hive/warehouse"
  }
}
</code></pre>

<p>```</p>

<p>You will need to create an S3 <code>bucket</code> first &ndash; <code>siq-hadoop</code> in our example &ndash; that will contain the Hive warehouse. After the cluster is up you can start using Hive as usual. When you create a table its metadata will be stored in the MySQL database configured in the blueprint and if you load data in it, it will be moved to the warehouse location on S3. Note that in order to use the <code>LOAD DATA INPATH</code> hive command the source and target directories must be located on the same filesystem, so a file in local HDFS cannot be used.</p>

<h4>Using Google Storage as the Hive warehouse</h4>

<p>This setup will use the Google Storage &ndash; and the GS to HDFS connector.</p>

<p>```</p>

<pre><code>  "global": {
    "fs.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem",
    "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
    "fs.gs.project.id": "siq-haas",
    "google.cloud.auth.service.account.enable": true,
    "google.cloud.auth.service.account.email": "YOUR_ACCOUNT_ID@developer.gserviceaccount.com",
    "google.cloud.auth.service.account.keyfile": "/mnt/fs1/&lt;PRIVATE_KEY_FILE&gt;.p12"
  }
</code></pre>

<p>```</p>

<p>Note that in case of Google being used as an object store you will need to add your account details and the path towards your P12 file. You&rsquo;ll also have to copy the connector JAR to the classpath and the p12 file to every node as mentioned in our previous <a href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/">post</a>.</p>

<h2>Ephemeral cluster &ndash; on AWS and Google Cloud</h2>

<p>Ephemeral Hive clusters are using a very similar configuration: they also have to reach the object store as HDFS so the corresponding configurations must be there in the blueprint. The only <a href="http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.2.2/bk_installing_manually_book/content/rpm-chap6-3.html">additional parameters</a> needed are the ones that configure how the metastore service of the ephemeral cluster will reach the Hive <code>metastore DB</code> in the permanent cluster. Note: on the permanent cluster you will have to configure the <code>metastore DB</code> to allow connections from remote clusters.</p>

<p>```</p>

<pre><code>{
  "hive-site": {
    "hive.metastore.warehouse.dir": "s3://siq-hadoop/apps/hive/warehouse",
    "javax.jdo.option.ConnectionURL": "jdbc:mysql://$mysql.full.hostname:3306/$database.name?createDatabaseIfNotExist=true",
    "javax.jdo.option.ConnectionDriverName": "com.mysql.jdbc.Driver",
    "javax.jdo.option.ConnectionUserName": "dbusername",
    "javax.jdo.option.ConnectionPassword": "dbpassword"
  }
}
</code></pre>

<p>```</p>

<h2>Conclusion</h2>

<p>As highlighted in this example, building a data lake or data warehouse is pretty simple and can be automated with <a href="http://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; also with the new <code>recipe</code> feature we are standardizing the provisioning of different Hadoop clusters. One of the coming posts will highlight the new architectural changes &ndash; and the components we use for service discovery/registry, failure detection, key/value store for dynamic configuration, feature flagging, coordination, leader election and more.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying a Hadoop Cluster - DevOps way]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops/"/>
    <updated>2014-10-30T12:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops</id>
    <content type="html"><![CDATA[<p>A while ago we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider &ndash; record the process and automate it.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Are we <code>obsessed with automation</code>? Definitely yes &ndash; all the step which are candidates of doing it twice we script or automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<p><code>
git clone https://github.com/sequenceiq/cloudbreak-shell.git
cd cloudbreak-shell
mvn clean package
</code></p>

<!--more-->


<p><em>Note: In case you use the hosted version of Cloudbreak you should use the <code>latest-release.sh</code> to get the right version of the CLI.
In case you build your own Cloudbreak from the <code>master</code> branch you should use the <code>latest-snap.sh</code> to get the right version of the CLI.</em></p>

<!--more-->


<h2>Sign in and connect to Cloudbreak</h2>

<p>There are several different ways to use the shell. First of all you&rsquo;ll need a Cloudbreak instance you can connect to. The easiest way is to use our hosted solution &ndash; you can access it with your SequenceIQ credentials. If you don&rsquo;t have an account, you can subscribe <a href="https://accounts.sequenceiq.com/register">here</a>.</p>

<p>Alternatively you can host your own Cloudbreak instance &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. If you&rsquo;re hosting your own Cloudbreak server you can still use your SequenceIQ credentials and our identity server, but then you&rsquo;ll have to configure your Cloudbreak installation with proper client credentials that will be accepted by our identity server. It is currently not supported to register your Cloudbreak application through an API (but it is planned), so contact us if you&rsquo;d like to use this solution.</p>

<p>The third alternative is to deploy our whole stack locally in your organization along with <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">Cloudbreak</a>, our OAuth2 based <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">Identity Server</a>, and our user management application, <a href="https://github.com/sequenceiq/sultans">Sultans</a>.</p>

<p>We suggest to try our hosted solution as in case you have any issues we can always help you. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).</p>

<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<p>```
Usage:
  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar &mdash;cmdfile=<FILE> : Cloudbreak executes commands read from the file.</p>

<p>Options:
  &mdash;cloudbreak.address=&lt;http[s]://HOSTNAME:PORT>  Address of the Cloudbreak Server [default: <a href="https://cloudbreak-api.sequenceiq.com">https://cloudbreak-api.sequenceiq.com</a>].
  &mdash;identity.address=&lt;http[s]://HOSTNAME:PORT>    Address of the SequenceIQ identity server [default: <a href="https://identity.sequenceiq.com">https://identity.sequenceiq.com</a>].
  &mdash;sequenceiq.user=<USER>                        Username of the SequenceIQ user [default: <a href="&#109;&#x61;&#105;&#x6c;&#x74;&#111;&#x3a;&#117;&#x73;&#101;&#x72;&#x40;&#x73;&#101;&#x71;&#x75;&#101;&#110;&#99;&#101;&#105;&#113;&#x2e;&#99;&#111;&#109;">&#x75;&#x73;&#101;&#114;&#64;&#115;&#x65;&#113;&#117;&#x65;&#110;&#x63;&#x65;&#105;&#113;&#46;&#x63;&#x6f;&#109;</a>].
  &mdash;sequenceiq.password=<PASSWORD>                Password of the SequenceIQ user [default: password].</p>

<p>Note:
  You should specify at least your username and password.
<code>``
Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use</code>hint<code>. You can always use</code>TAB<code>for completion. Note that all commands are</code>context aware` &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<p><code>
credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"
</code></p>

<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<p><code>
credential select --id #ID of the credential
</code></p>

<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<p><code>
template createEC2 --name awstemplate --description aws-template  --region EU_WEST_1 --instanceType M3Xlarge --volumeSize 100 --volumeCount 2
</code>
You can check whether the template was created successfully by using the <code>template list</code> command. Check the template and select it if you are happy with it:</p>

<p>```
template show &mdash;id #ID of the template</p>

<p>template select &mdash;id #ID of the template
```</p>

<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<p><code>
stack create --name “myStackName" --nodeCount 10
</code></p>

<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<p>```
blueprint list</p>

<p>blueprint select &mdash;id #ID of the blueprint
```</p>

<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<p><code>
cluster create --description “my cluster desc"
</code>
You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
