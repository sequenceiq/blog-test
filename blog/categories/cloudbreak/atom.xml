<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudbreak | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/cloudbreak/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-11-02T19:10:01+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploying a Hadoop Cluster - DevOps way]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops/"/>
    <updated>2014-10-30T12:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops</id>
    <content type="html"><![CDATA[<p>A while ago we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider &ndash; record the process and automate it.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Are we <code>obsessed with automation</code>? Definitely yes &ndash; all the step which are candidates of doing it twice we script or automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<p><code>
git clone https://github.com/sequenceiq/cloudbreak-shell.git
cd cloudbreak-shell
mvn clean package
</code></p>

<!--more-->


<p><em>Note: In case you use the hosted version of Cloudbreak you should use the <code>latest-release.sh</code> to get the right version of the CLI.
In case you build your own Cloudbreak from the <code>master</code> branch you should use the <code>latest-snap.sh</code> to get the right version of the CLI.</em></p>

<!--more-->


<h2>Sign in and connect to Cloudbreak</h2>

<p>There are several different ways to use the shell. First of all you&rsquo;ll need a Cloudbreak instance you can connect to. The easiest way is to use our hosted solution &ndash; you can access it with your SequenceIQ credentials. If you don&rsquo;t have an account, you can subscribe <a href="https://accounts.sequenceiq.com/register">here</a>.</p>

<p>Alternatively you can host your own Cloudbreak instance &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. If you&rsquo;re hosting your own Cloudbreak server you can still use your SequenceIQ credentials and our identity server, but then you&rsquo;ll have to configure your Cloudbreak installation with proper client credentials that will be accepted by our identity server. It is currently not supported to register your Cloudbreak application through an API (but it is planned), so contact us if you&rsquo;d like to use this solution.</p>

<p>The third alternative is to deploy our whole stack locally in your organization along with <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">Cloudbreak</a>, our OAuth2 based <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">Identity Server</a>, and our user management application, <a href="https://github.com/sequenceiq/sultans">Sultans</a>.</p>

<p>We suggest to try our hosted solution as in case you have any issues we can always help you. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).</p>

<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<p>```
Usage:
  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar &mdash;cmdfile=<FILE> : Cloudbreak executes commands read from the file.</p>

<p>Options:
  &mdash;cloudbreak.address=&lt;http[s]://HOSTNAME:PORT>  Address of the Cloudbreak Server [default: <a href="https://cloudbreak-api.sequenceiq.com">https://cloudbreak-api.sequenceiq.com</a>].
  &mdash;identity.address=&lt;http[s]://HOSTNAME:PORT>    Address of the SequenceIQ identity server [default: <a href="https://identity.sequenceiq.com">https://identity.sequenceiq.com</a>].
  &mdash;sequenceiq.user=<USER>                        Username of the SequenceIQ user [default: <a href="&#x6d;&#97;&#x69;&#108;&#x74;&#111;&#x3a;&#x75;&#x73;&#x65;&#114;&#64;&#115;&#101;&#113;&#117;&#101;&#x6e;&#99;&#101;&#105;&#113;&#x2e;&#x63;&#111;&#109;">&#117;&#x73;&#101;&#114;&#x40;&#x73;&#x65;&#x71;&#x75;&#101;&#x6e;&#x63;&#x65;&#105;&#113;&#46;&#99;&#x6f;&#109;</a>].
  &mdash;sequenceiq.password=<PASSWORD>                Password of the SequenceIQ user [default: password].</p>

<p>Note:
  You should specify at least your username and password.
<code>``
Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use</code>hint<code>. You can always use</code>TAB<code>for completion. Note that all commands are</code>context aware` &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<p><code>
credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"
</code></p>

<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<p><code>
credential select --id #ID of the credential
</code></p>

<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<p><code>
template createEC2 --name awstemplate --description aws-template  --region EU_WEST_1 --instanceType M3Xlarge --volumeSize 100 --volumeCount 2
</code>
You can check whether the template was created successfully by using the <code>template list</code> command. Check the template and select it if you are happy with it:</p>

<p>```
template show &mdash;id #ID of the template</p>

<p>template select &mdash;id #ID of the template
```</p>

<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<p><code>
stack create --name “myStackName" --nodeCount 10
</code></p>

<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<p>```
blueprint list</p>

<p>blueprint select &mdash;id #ID of the blueprint
```</p>

<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<p><code>
cluster create --description “my cluster desc"
</code>
You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the data lake in the cloud - Part1]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/"/>
    <updated>2014-10-28T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak</id>
    <content type="html"><![CDATA[<p>A while ago we have released our cloud agnostic and Docker container based Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Though the purpose of <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> is to quickly provision arbitrary sized Hadoop clusters in the cloud, the project emerged from bare metal Hadoop provisioning in Docker containers. We were (still doing it) <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">provisioning</a> Hadoop on bare metal using Docker &ndash; and because of this legacy the data was always stored in HDFS. Recently we have been asked to run a proof-of-concept project and build an <code>always on</code> data lake using a cloud <code>object storage</code>.</p>

<p>This post is the first in this series and will cover the connectivity, interoperability and access of data from an <code>object storage</code> and work with that in Hadoop. For this post we choose to create a <code>data lake</code> on Google Cloud Compute and guide you through the steps, run performance tests and understand the benefits/drawbacks of such a setup.</p>

<p><em>Next post will be about sharing the <code>data lake</code> among multiple clusters, using <a href="http://hortonworks.com/hadoop/hcatalog/">Apache HCatalog</a>.</em></p>

<h2>Object storage</h2>

<p>An object storage usually is an <code>internet service</code> to store data in the cloud and comes with a programming interface which allows to retrieve data in a secure, durable and highly-scalable way. The most well know object storage is <strong>Amazon S3</strong> &ndash; with a pretty well covered literature, thus in this example we will use the <strong>Google Cloud Storage</strong>. Google Cloud Storage enables application developers to store their data on Google’s infrastructure with very high reliability, performance and availability, and can be used to distribute large data objects &ndash; like HDFS. In many occasions companies stores their data in objects storages &ndash; but for analytics they would like to access it from their Hadoop cluster. There are several options available:</p>

<ul>
<li>replicate the full dataset in HDFS</li>
<li>read and write from <code>object storage</code> at start/stop of the flow and use HDFS for intermediary data</li>
<li>use a connector such as Google Cloud Storage Connector for Hadoop</li>
</ul>


<h2>Google Cloud Storage Connector for Hadoop</h2>

<p>Using <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">this</a> connector developed by Google allows you to choose <code>Google Cloud Storage</code> as the default file system for Hadoop, and run all your jobs on top (we will come up with MR2 and Spark examples). Using the connector can have several benefits, to name a few:</p>

<ul>
<li>Direct data access &ndash; data is stored in GCS, no need to transfer it into HDFS</li>
<li>HDFS compatibility &ndash; data stored in HDFS can be accessed through the connector</li>
<li>Data accessibility &ndash; data is always accessible, even when the Hadoop cluster is shut down</li>
<li>High data availability &ndash; data is highly available and globally replicated</li>
</ul>


<!-- more -->


<h2>DIY &ndash; build your data lake</h2>

<p>Follow these steps in order to create your own <code>data lake</code>.</p>

<ol>
<li>Create your <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak account</a></li>
<li>Configure your Google Cloud account following these <a href="http://sequenceiq.com/cloudbreak/#accounts">steps</a></li>
<li>Copy the appropriate version of the <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">connector jar</a> to the Hadoop classpath and the key file for auth on every node of the cluster &ndash; use this <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/copyscripts.sh">script</a> to automate the process</li>
<li>Use this Ambari <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/gcs-con-multi-node-hdfs-yarn.blueprint">blueprint</a> to configure the connector</li>
<li>Restart the following services: HDFS, YARN and MapReduce2</li>
</ol>


<p>That&rsquo;s it &ndash; you are done, you can work on your data stored in Google Storage. The next release of <a href="https://github.com/sequenceiq/cloudbreak">Cloudbreak</a> will incorporate and automate these steps for you &ndash; and will use HCatalog to allow you to configure an <code>always on</code> data store using object storages.</p>

<h2>Performance results</h2>

<p>We configured two identical clusters with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> on Google Cloud with the following parameters</p>

<ul>
<li>Number of nodes: 1 master node + 10 slave nodes</li>
<li>2 * 200 GB rotating HDD (where appropriate)</li>
<li>2 Virtual CPU</li>
<li>7.5 GB of memory</li>
</ul>


<p>First of all we run all the Hadoop and the certification tests in order to validate the correctness of the setups. For the tests we have provisioned an <strong>Hortonwork&rsquo;s HDP 2.1</strong> cluster.</p>

<p>After these steps we have switched to the <code>standard</code> performance test &ndash; <strong>TeraGen, TeraSort and TeraValidate</strong>. Please see the results below.</p>

<table>
<thead>
<tr>
<th></th>
<th> File System           </th>
<th> TeraGen </th>
<th> TeraSort </th>
<th> TeraValidate</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> HDFS                  </td>
<td>58mins, 58sec</td>
<td>4hrs, 59mins, 6sec</td>
<td>35mins, 58sec</td>
</tr>
<tr>
<td></td>
<td> Google Cloud Storage  </td>
<td>34mins, 36sec</td>
<td>4hrs, 34mins, 52sec</td>
<td> 29mins, 22sec</td>
</tr>
</tbody>
</table>


<h2>Summary</h2>

<p>There is a pretty good literature about HDFS and object storages and lots of debates around. At <a href="http://sequenceiq.com">SequenceIQ</a> we support both &ndash; and we also believe that each and every company or use case has his own rationale behind choosing one of them. When we came up with the mission statement of simplifying how people work with Hadoop and stated that we&rsquo;d like to give the broadest available options to developers we were pretty serious about.</p>

<p><a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> was designed around being cloud agnostic &ndash; running on Docker and being able to ship those containers to bare metal or any cloud provider with a very easy integration process: currently we support <strong>Amazon AWS, Microsoft Azure and Google Cloud</strong> in public beta and <strong>OpenStack, Digital Ocean</strong> integration in progress/private beta.
As for the supported Hadoop distribution we provision <strong>Apache Hadoop and Hortonworks HDP</strong> in public and <strong>Cloudera CDH</strong> in private beta.</p>

<p>All the private betas will emerge into public programs and will be in GA &ndash; and open sourced under an Apache2 license during Q4.</p>

<p><a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> will be released quite soon &ndash; stay tuned &ndash; will support one API/representation of your big data pipeline and running on multiple runtimes: <strong>MR2, Spark and Tez</strong>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or
<a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Securing Cloudbreak with OAuth2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/"/>
    <updated>2014-10-16T14:23:59+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server</id>
    <content type="html"><![CDATA[<p>When we first released <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; our Hadoop as a Service API &ndash; it contained its own authentication and user management layer.
We were using basic authentication for the API calls so every request had to contain a username and a password <em>Base64</em> encoded in the authorization header.
Cloudbreak also had its own user representation and we were binding the resources &ndash; like clusters &ndash; to these users.</p>

<p>This approach had multiple flaws. As we were starting to develop multiple <a href="http://sequenceiq.com/periscope/">projects</a> for our future Platform as a Service solution it became obvious that we will have to refactor our whole user management layer out from Cloudbreak and <strong>share it across our projects</strong>.
Base64 encoding of usernames and passwords is not the best solution either even if transport layer security is working.</p>

<p>What comes into play almost instantly when dealing with these kind of problems is <strong>OAuth2</strong> but it&rsquo;s not as trivial as it first sounds.</p>

<h2>OAuth2</h2>

<p>The main &ldquo;problem&rdquo; with OAuth2 is that its <a href="http://tools.ietf.org/html/rfc6749">specification</a> leaves a lot of decisions up to the implementations.
First of all it does not speak at all about authentication, only authorization. It also leaves out details such as how to manage users, how scopes and tokens look like or how these tokens should be checked by a resource server.</p>

<p>Because of all these reasons implementing a full OAuth2 solution from scratch means a <em>lot</em> of work and reinventing the wheel and of course we didn&rsquo;t want to do that.
Luckily there are a few specifications that complement the original standard and there are also some solutions that implement not only the basic specification but these complementary specifications too.</p>

<p><strong><a href="https://github.com/cloudfoundry/uaa">UAA</a> is CloudFoundry&rsquo;s fully open source identity management service.</strong>
According to the documentation its primary role is as an OAuth2 provider that can issue tokens for client applications, but it can also authenticate users and can manage user accounts and OAuth2 clients through an HTTP API.
To achieve these things it uses these specifications:</p>

<ul>
<li><p><a href="http://openid.net/connect/">OpenID Connect</a> for authentication</p></li>
<li><p><a href="http://www.simplecloud.info/">SCIM</a> for user management</p></li>
<li><p><a href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html">JWT</a> for token representation</p></li>
</ul>


<p>UAA adds a few more things on top of these like client management endpoints which makes it a complete solution as an identity server.
And the best thing is that it is <strong>fully configurable through environment variables and a YAML file</strong>.</p>

<!-- more -->


<h2>Deploying the UAA server</h2>

<p>UAA is a Spring-based Java web application that runs on Tomcat. The first thing we did was to create a <a href="https://registry.hub.docker.com/u/sequenceiq/uaa/">Docker image</a> that deploys a UAA server so it became this easy:
<code>
docker run -d --link uaa-db:db -e UAA_CONFIG_URL=https://raw.githubusercontent.com/sequenceiq/docker-uaa/master/uaa.yml sequenceiq/uaa:1.8.1
</code>
There are two ways to provide an UAA configuration file: you can specify an URL like above, or via volume sharing. You can simply put your configuration in the shared directory (<code>/tmp/uaa</code> in the example):
<code>
docker run -d --name uaa --link uaa-db:db -v /tmp/uaa:/uaa sequenceiq/uaa:1.8.1
</code>
Linking a database container is only necessary if you&rsquo;re using a configuration like we did <a href="https://github.com/sequenceiq/docker-uaa/blob/master/uaa.yml">in this example</a>.
If you&rsquo;d like to create a postgresql database to try out the sample configuration on your local environment run the following command first that creates a default postgresql database:
<code>
docker run -d --name uaa-db postgres
</code></p>

<h2>UAA Configuration</h2>

<p>The UAA <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/Sysadmin-Guide.rst#configuration">documentation</a> covers the configuration part pretty well, but I&rsquo;ll share my own experiences through some examples.</p>

<h3>Database</h3>

<p>The first part of the configuration file describes where the data will be stored. Environment variables can be used inside the YAML file, they will be expanded when UAA processes the file.
When linking Docker containers the address and the exposed ports of the linked container show up as environment variables in the other container so we can make use of it and provide the postgresql address like this:
<code>
database:
  driverClassName: org.postgresql.Driver
  url: jdbc:postgresql://${DB_PORT_5432_TCP_ADDR}:${DB_PORT_5432_TCP_PORT}/${DB_ENV_DB:postgres}
  username: ${DB_ENV_USER:postgres}
  password: ${DB_ENV_PASS:}
</code></p>

<h3>Default clients</h3>

<p>Default clients and users can also be described in the configuration, but they can be added or modified later through the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#user-account-management-apis">User Management API</a> and the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#client-registration-administration-apis">Client Administration API</a>.</p>

<p>```
oauth:
  clients:</p>

<pre><code>mywebapp:
  id: mywebapp
  secret: changeme
  authorized-grant-types: authorization_code
  scope: myresourceserver.scope1,myresourceserver.scope2,openid,password.write
  authorities: uaa.none
  redirect-uri: http://localhost:3000/authorize
</code></pre>

<p><code>``
Every client should have an</code>authorized-grant-types<code>attribute that tells which OAuth2 flow the client can use to obtain a token. The most common is the *authorization code flow* that is typically used by web applications. The other possible values are</code>implicit<code>,</code>password<code>and</code>client_credentials`.</p>

<p>A <code>secret</code> is not needed for a client with an implicit grant type (implicit flow is typically used from client-side web apps where a secret cannot be used), and of course a <code>redirect-uri</code> is not needed for a client with a <code>client_credentials</code> grant type.</p>

<p>The client can request the <code>scopes</code> described here from the user. These scopes are arbitrary strings that mean something only to the resource server, but UAA uses the base name (anything before the first dot) of the scopes as the <a href="http://tools.ietf.org/html/draft-ietf-oauth-json-web-token-25#section-4.1.3">audience field</a> in the JWT token, so it&rsquo;s recommended to use this kind of naming convention.</p>

<p><code>authorities</code> are basically scopes but only used when the token represents the client itself. It can be useful for example when a client wants to use the SCIM endpoints of the UAA server &ndash; there are built-in scopes for that: <code>scim.read</code> and <code>scim.write</code>.</p>

<p>There are some clients where the user should not be asked to approve a token grant explicitly (e.g.: a command line shell). To surpass the confirmation and accept the permission request automatically, add the following to the <code>oauth</code> section:
```
client:</p>

<pre><code>override: true
autoapprove:
  - mycommandlineshell
</code></pre>

<p>```</p>

<h3>Default users</h3>

<p>The users defined in this section are populated in the database after startup.</p>

<p>```
scim:
  username_pattern: &lsquo;[a-z0-9+-_.@]+&rsquo;
  users:</p>

<pre><code>- paul|wombat|paul@test.org|Paul|Smith|openid,myresourceserver.scope1,myresourceserver.scope2
</code></pre>

<p>```</p>

<p>This one is quite straightforward. The users are added in the specified format:
<code>
username|password|email|given name|last name|groups
</code>
The SCIM specification does not speak about roles, scopes or accounts, it only knows <em><a href="http://www.simplecloud.info/specs/draft-scim-core-schema-01.html#group-resource">groups</a></em> besides <em>users</em> where users can be <em>members</em> of a group.
UAA handles scopes as groups, but groups can also be used for other things like adding users to a company account.</p>

<h2>Resources</h2>

<p>If you&rsquo;d like to learn more about UAA, check out its <a href="https://github.com/cloudfoundry/uaa/tree/master/docs">documentation</a> or its <a href="https://github.com/cloudfoundry/uaa/tree/master/samples">sample applications</a>.
We&rsquo;ll also have another blog post soon where I&rsquo;ll show some code examples of the OAuth2 flows we&rsquo;re using with UAA as an identity server so check back in a few days if you&rsquo;re interested.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak new provider implementation - Part I: Build your custom image]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc/"/>
    <updated>2014-09-18T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc</id>
    <content type="html"><![CDATA[<p>Not so long ago we have released <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the cloud agnostic, open source and Docker based Hadoop as a Service API (with support for <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">autoscaling</a> Hadoop clusters). As we have <code>dockerized</code> the whole Hadoop ecosystem, we are shipping the containers to different cloud providers, such as Amazon AWS, Microsoft Azure and Google Cloud Compute. Also Cloudbreak has an <a href="http://sequenceiq.com/cloudbreak/#add-new-cloud-providers">SDK</a> which allows you to quickly add your favorite cloud provider. In this post (series) we’d like to guide you trough the process, and show you how to create a custom image &ndash; on Google Cloud. We have chose Google Cloud as this is the least documented and has the smallest amount on default images (there are thousand for Amazon, and hundreds for Azure). Nevertheless on all cloud provider usually you’d like to have a custom image with your preferred OS, configuration and potentially installed applications.</p>

<!-- more -->


<h3>Why do we need custom images on every cloud?</h3>

<p>All the above are true for us as well &ndash; with some simplifications. We use Docker to run every process/application &ndash; for the benefits we have covered in other posts many times &ndash; and apart from Docker, our (or the customer’s) preferred OS and a few other helper/debugger things (such as <a href="https://registry.hub.docker.com/u/jpetazzo/nsenter/">nsenter</a>)
we are almost fine. We have made some PAM related fixes/contributions for Docker &ndash; and until they are not in the upstream we have built/derive from our base layer/containers &ndash; so with this and the actual containers included this is pretty much how a cloud base image looks like for us.</p>

<p>As usual we always automate everything &ndash; building custom cloud base images is part of the automation and our CI/CD process as well. For that we use <a href="http://www.ansible.com/home">Ansible</a> as the preferred IT automation tool. So the first step is to define your own <a href="http://docs.ansible.com/playbooks.html">playbook</a> to install everything on the virtual machine.</p>

<p>A simple playbook looks like this:</p>

<p>```
  &ndash; name: Install Docker</p>

<pre><code>shell: curl -sL https://get.docker.io/ | sh
when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
</code></pre>

<ul>
<li><p>name: Pull sequenceiq/ambari image
shell: docker pull sequenceiq/ambari:pam-fix</p></li>
<li><p>name: Pull jpetazzo/nsenter image
shell: docker pull jpetazzo/nsenter</p></li>
<li><p>name: Install bridge-utils
apt: name=bridge-utils state=latest
when: ansible_distribution == &lsquo;Debian&rsquo; or ansible_distribution == &lsquo;Ubuntu&rsquo;</p></li>
<li><p>name: install jq
shell: curl -o /usr/bin/jq <a href="http://stedolan.github.io/jq/download/linux64/jq">http://stedolan.github.io/jq/download/linux64/jq</a> &amp;&amp; chmod +x /usr/bin/jq</p></li>
</ul>


<p>```</p>

<p>Using Google cloud you have 2 choices:</p>

<ol>
<li> Create snapshots starting from a default image</li>
<li> Create a custom image</li>
</ol>


<h3>Image creation using snapshots</h3>

<p>We are using Debian as the host OS on Google Cloud, and have created a virtual machine using the default <a href="https://developers.google.com/compute/docs/operating-systems#backported_debian_7_wheezy">Debian</a> image. First thing first, you need to create a persistent disk:</p>

<p><code>
gcloud compute disks create temporary-disk --zone ZONE
</code></p>

<p>Then create a virtual machine with the temporary-disk:</p>

<p><code>
gcloud compute instances create example-instance \
  --scopes storage-rw --image IMAGE \
  --disk name=temporary-disk device-name=temporary-disk --zone ZONE
</code></p>

<p>And attach the disk to the google cloud instance:</p>

<p><code>
gcloud compute instances attach-disk example-instance
  --disk temporary-disk --device-name temporary-disk --zone ZONE
</code></p>

<p>When this is finished then you can <code>shh</code> to the <code>sample-instance</code>. You can now check your mounted volumes with this command:</p>

<p><code>
ls -l /dev/disk/by-id/google-*
</code></p>

<p>Now you need to create a folder which will contain your custom built image:</p>

<p><code>
sudo mkdir /mnt/tmp
</code></p>

<p>You have to format your partition before the image creation:</p>

<p><code>
sudo /usr/share/google/safe_format_and_mount -m "mkfs.ext4 -F" /dev/sdb /mnt/tmp
</code></p>

<p>Now you can start building the image which will last about 10 minutes:</p>

<p><code>
sudo gcimagebundle -d /dev/sda -o /mnt/tmp/ --log_file=/tmp/imagecreation.log
</code></p>

<p>You have now an image in /tmp with a special hex number like <code>/tmp/HEX-NUMBER.image.tar.gz</code></p>

<p>Once you uploaded it to a Google bucket you are done, and ready to use it.</p>

<p><code>
gsutil cp /mnt/tmp/IMAGE_NAME.image.tar.gz gs://BUCKET_NAME
</code></p>

<h3>Create a custom image &ndash; using your favorite OS</h3>

<p><a href="http://www.ubuntu.com/download/server">Ubuntu server 14.04</a> is many’s preferred Linux distribution &ndash; unluckily there is no default image using Ubuntu as the OS in the Google Cloud](<a href="https://developers.google.com/compute/docs/operating-systems">https://developers.google.com/compute/docs/operating-systems</a>). Luckily this is not that complicated &ndash; the process below works with any other OS as well. In order to start you should have <a href="https://www.virtualbox.org/">Virtualbox</a> installed. Download an Ubuntu server from <a href="http://www.ubuntu.com/server">Ubuntu’s</a> web page.
Install in into the <a href="https://www.virtualbox.org/">Virtualbox</a> box, start it and <code>ssh</code> into. Once you are inside you will have to install the <a href="https://developers.google.com/cloud/sdk/">Google Cloud SDK</a>. This is needed for the custom image, as contains some extra feature like <code>google-startup-scripts</code>. Remember that Ubuntu (and in general a few cloud providers) support <code>cloud-init</code> scripts, and this is why we need the Google Cloud SDK &ndash; as we ship these images to the <code>cloud</code>.</p>

<p>After the installation add the following kernel options into the <code>/etc/default/grub</code>:</p>

<p>```</p>

<h1>to enable paravirtualization</h1>

<p>CONFIG_KVM_GUEST=y</p>

<h1>to enable the paravirtualized clock.</h1>

<p>CONFIG_KVM_CLOCK=y</p>

<h1>to enable paravirtualized PCI devices.</h1>

<p>CONFIG_VIRTIO_PCI=y</p>

<h1>to enable access to paravirtualized disks.</h1>

<p>CONFIG_SCSI_VIRTIO=y</p>

<h1>to enable access to the networking.</h1>

<p>CONFIG_VIRTIO_NET=y
```</p>

<p>Now you are ready to prepare an <code>official</code> image into a tar file, by selecting the virtual box image file on your disk and convert it.
You can convert your <code>vmdk</code> file into the supported raw type by using:</p>

<p><code>
qemu-img convert -f vmdk -O raw VMDK_FILE_NAME.vmdk disk.img
</code></p>

<p>The .img file name has to be <code>disk.img</code>. After you have converted the image, you have to make a tar file:</p>

<p><code>
tar -Szcf &lt;image-tar-name&gt;.tar.gz disk.raw
</code></p>

<p>Same as before, you have to upload in to a Google Cloud Bucket:</p>

<p><code>
gsutil cp &lt;image-tar-name&gt;.tar.gz gs://&lt;bucket-name&gt;
</code></p>

<p>Now you have an <code>official</code> image template but you have to create the image in Google Cloud:</p>

<p><code>
gcutil addimage my-ubuntu gs://&lt;bucket-name&gt;/ubuntu_image.tar.gz
</code></p>

<p>Once this is done you have created your custom built Google Cloud image, and you are ready to start cloud instances using it. Let us know how it works for you, and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> for updates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infrastructure management with CloudFormation]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier/"/>
    <updated>2014-08-29T14:13:06+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier</id>
    <content type="html"><![CDATA[<p>Our Hadoop as a Service solution, <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> integrates with multiple cloud providers to deploy Hadoop clusters in the cloud. It means that every time a cluster is requested, Cloudbreak goes to the selected cloud provider and creates a new, separated infrastructure through the provider’s API. Building this infrastructure can be a real pain and can cause a lot of problems &ndash; it involves a lot of API calls, the polling of created building blocks, the management of failures and the necessary rollbacks to name a few. With the help of <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation</a> we were able to avoid most of these problems when integrating AWS in Cloudbreak.</p>

<h3>Problems with the traditional approach</h3>

<p>When Cloudbreak creates a Hadoop cluster it should first create the underlying infrastructure on the cloud provider. The building blocks are a bit different on every provider, the following resources are created on AWS:</p>

<ul>
<li>a virtual private cloud (VPC)</li>
<li>a subnet</li>
<li>an internet gateway</li>
<li>a route table</li>
<li>an auto scaling group and its launch configuration</li>
<li>a security group</li>
</ul>


<p>Although AWS has a pretty good API and great SDKs to communicate with it, we needed to deal with the above described problems if we would like to create all of these elements one by one through the Java SDK. The code would start with something like this with the creation of the VPC:</p>

<p>```java
AmazonEC2Client amazonEC2Client = new AmazonEC2Client(basicSessionCredentials);
amazonEC2Client.setRegion(region);</p>

<p>CreateVpcRequest vpcRequest = new CreateVpcRequest().withCidrBlock(10.0.0.0/24);
CreateVpcResponse vpcResponse = amazonEC2Client.createVpc(createVpcRequest);</p>

<p>//poll vpc creation until it’s state is available
waitForVPC(amazonEC2Client, vpcResponse.getVpc());</p>

<p>ModifyVpcAttributeRequest modifyVpcRequest = new ModifyVpcAttributeRequest().withEnableDnsHostnames(true).withEnableDnsSupport(true);
amazonEC2Cient.modifyVpcAttribute(modifyVpcRequest);
```</p>

<!--more-->


<p>The above code is only a taste of the whole thing. The VPC is one of the most simple resources with very few attributes to set. Also the polling of the creation process isn’t detailed here as well as failure handling. In addition the different resources would be scattered around the code making it impossible to have an overview of the whole stack and making it much harder to find bugs or to modify some attributes. With CloudFormation all of the above problems can be solved very easily.</p>

<h3>Introduction to CloudFormation</h3>

<p>According to the <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation documentation</a> it was designed to create and manage a collection of related AWS resources easily and provisioning and updating them in an orderly and predictable fashion. What it really means is that the resources can be described declaratively in a JSON document (a <em>template</em>) and the whole <em>stack</em> can be created/updated/deleted with a simple API call. AWS also handles failures, and rollbacks the whole stack if something goes wrong. Furthermore it is able to send notifications to <em>SNS topics</em> when some event occurs (e.g.: a resource creation started or the resource is completed), making the polling of resource creations unnecessary.</p>

<h3>Template structure</h3>

<p>We don’t want to give a detailed introduction on how the structure of a CloudFormation template look like, the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">AWS documentation</a> covers it really well and there are also a lot of <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-services-us-west-2.html">samples</a>.
Instead we’re trying to focus on the advantages that CloudFormation gave us while using it, so let’s jump in the middle and start with a simple example. The declaration of a VPC in a template looks like this:</p>

<p>```json
&ldquo;Resources&rdquo; : {
  &ldquo;MY_VPC&rdquo; : {</p>

<pre><code>"Type" : "AWS::EC2::VPC",
"Properties" : {
  "CidrBlock" : { "10.0.0.0/16" },
  "EnableDnsSupport" : "true",
  "EnableDnsHostnames" : "true",
  "Tags" : [
    { "Key" : "Application", "Value" : { "Ref" : "AWS::StackId" } },
    { "Key" : "Network", "Value" : "Public" }
  ]
}
</code></pre>

<p>  }
}
```</p>

<p>The JSON syntax can be a bit complicated sometimes, especially when dealing with a lot of references to other properties with the <em>&ldquo;Ref&rdquo;</em> keyword or some other built-in CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html">functions</a>, but it is much clearer than the Java code above.
Other than resources, there are other parts of a CloudFormation template (<em>Conditions</em>, <em>Mappings</em>, <em>Outputs</em>, <em>Intrinsic Functions</em>) but here we will cover only one more: <em>Parameters</em>.</p>

<p>Declaring a parameter means that there is no hard-coded value for a given attribute, rather it is given to the template when creating the stack. If you’d like to have an EC2 Instance  in your template but you don’t want to hardcode its type, you can have a parameter like this:</p>

<p>```json
&ldquo;Parameters&rdquo; : {
  &ldquo;InstanceType&rdquo; : {</p>

<pre><code>"Description" : "EC2 instance type",
"Type" : "String",
"Default" : "m3.medium",
"AllowedValues" : [ "m3.medium","m3.large","m3.xlarge","m3.2xlarge"],
"ConstraintDescription" : "must be a valid EC2 instance type."
</code></pre>

<p>  }
}
```</p>

<p>After it’s declared, you can reference it from a resource with the <em>Ref</em> keyword:</p>

<p>```json
&ldquo;EC2Instance&rdquo; : {
  &ldquo;Type&rdquo; : &ldquo;AWS::EC2::Instance&rdquo;,
  &ldquo;Properties&rdquo; : {</p>

<pre><code>"SecurityGroups" : [ { "Ref" : "InstanceSecurityGroup" } ],
"InstanceType" : { "Ref" : "InstanceType" },
"KeyName" : { "Ref" : "KeyName" },
"ImageId" : "ami-123456",
"EbsOptimized" : "true"
</code></pre>

<p>  }
}
```</p>

<p>You can reference not only parameters, but other resources as well. In the above code example there is a reference to <em>InstanceSecurityGroup</em> that is an <em>AWS::EC2::SecurityGroup</em> type resource and that is declared in an other part of the template.</p>

<h3>Creating the stack</h3>

<p>So we’ve declared a few resources, how can we tell AWS to create the stack? Let’s see how it looks like with the Java SDK (two parameters are passed to the template):</p>

<p>```java
CreateStackRequest createStackRequest = new CreateStackRequest()</p>

<pre><code>.withStackName(“MyCFStack")
.withTemplateBody(templateAsString)
.withNotificationARNs(notificationTopicARN)
.withParameters(
    new Parameter().withParameterKey("InstanceCount").withParameterValue(“3"),
    new Parameter().withParameterKey("InstanceType").withParameterValue(“m3.large"));
</code></pre>

<p>client.createStack(createStackRequest);
```</p>

<p>And that’s it. It’s every code that should written in Java to create the complete stack. It is pretty straightforward, the only thing that needs to be explained is the <em>notification ARN</em> part. It is the identifier of an <em>SNS topic</em> and it is detailed below.</p>

<h3>Callbacks</h3>

<p>CloudFormation is able to send notifications to SNS <em>topics</em> when an event occurs. An event is when a resource creation is started, finished or failed (and the same with delete). SNS is Amazon’s Simple Notification Service that enables endpoints to subscribe to a topic, and when a message is sent to a topic every subscriber receives that message. AWS supports a lot of endpoint types. It can send notifications by email or text message, to Amazon Simple Queue Service (SQS) queues, or to any HTTP/HTTPS endpoint. In the Cloudbreak project we’re using HTTP endpoints as callback URLs. We’re also creating topics and subscriptions from code but that could fill up another full blog post.</p>

<p>If you just like to try SNS, you can create a topic and a subscription from the AWS Console. After you have a confirmed subscription of an HTTP endpoint (e.g.: <em>example.com/sns</em>), you can very easily create an HTTP endpoint in Java (with some help from <a href="http://spring.io/">Spring</a>):</p>

<p><code>java
@RequestMapping(value="sns", method = RequestMethod.POST)
@ResponseBody
public ResponseEntity&lt;String&gt; receiveSnsMessage(@RequestBody String request) {
  // parse and handle request
}
</code></p>

<p>For a more detailed example see the <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/controller/AmazonSnsController.java">controller class</a> in Cloudbreak.
So every time a CloudFormation stack event occurs, Cloudbreak receives a message that is parsed and handled correctly &ndash; there is no need to poll the creation of resources and dealing with timeouts.</p>

<h3>Failures and rollbacks</h3>

<p>It is always possible that something will go wrong when creating a stack with a lot of resources. With the traditional approach you must keep track of the resources that were created and you will have to implement some rollback logic that gets called when something unexpected happens and that rolls back the already created elements somehow. With CloudFormation these things are completely done by AWS.</p>

<p>The resources in the stack are tracked so the only thing you have to save is the identifier of the stack. If one of the resources fails to be created AWS rolls back every other resource and puts the stack in <em>ROLLBACK_COMPLETED</em> state. It also sends the failure message to the SNS topic with the exact cause of the failure.
The same is true if you’d like to delete the stack. The only call that you will have to send to the AWS API is the deletion of the stack (very similar to the creation in Java). CloudFormation will delete every resource one by one and will take care of failures.</p>

<h3>Notes</h3>

<p>The template we used in Cloudbreak is available <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/templates/aws-cf-stack.ftl">here</a>. It is not a pure CloudFormation template because of some limitations &ndash; the number of attached volumes cannot be specified dynamically and it is not possible to specify it as a parameter if spot priced instances are needed or not &ndash; we ended up generating the template with Freemarker.</p>

<h3>Terraform</h3>

<p>The <a href="http://www.hashicorp.com/products">company</a> that brought us <a href="http://www.vagrantup.com/">Vagrant</a>, <a href="http://www.packer.io/">Packer</a> and a few more useful things has recently announced a new project named <a href="http://www.terraform.io/intro/index.html">Terraform</a>. Terraform is inspired by tools like CloudFormation or <a href="https://wiki.openstack.org/wiki/Heat">OpenStack’s Heat</a>, but goes further as it supports multiple cloud platforms and their services can also be combined. If you’re interested in managing infrastructure from code and configuration you should check out that project too, we’ll keep an eye on it for sure.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
