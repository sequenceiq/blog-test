<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudbreak | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/cloudbreak/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-02-02T18:07:58+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cluster extensions with Cloudbreak recipes]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/02/02/cluster-extensions-with-cloudbreak-recipes/"/>
    <updated>2015-02-02T14:59:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/02/02/cluster-extensions-with-cloudbreak-recipes</id>
    <content type="html"><![CDATA[<p>With the help of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari <a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">blueprint</a>. That&rsquo;s cool but it is often needed to make some additional changes on the nodes, like putting a JAR file on the Hadoop classpath or run some custom scripts. To help with these kind of situations we are introducing the concept of Cloudbreak <code>recipes</code>. Recipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation.</p>

<h2>How does it work?</h2>

<p>Since the latest release, Cloudbreak uses <a href="https://consul.io">Consul</a> for cluster membership instead of Serf so we can make use of Consul’s other features, namely <code>events</code> and the <code>key-value</code> store. It won’t be detailed here how these features of Consul work, a whole post about Consul based clusters is coming soon. Recipes are using one more additional thing: the small <a href="https://github.com/progrium/plugn">plugn</a> project.</p>

<p>The main concept behind this is the following: before the cluster install is started, a <code>recipe-pre-install</code> Consul event is sent to the cluster that triggers the <code>recipe-pre-install</code> hook of the enabled plugins, therefore executing the plugins' <code>recipe-pre-install</code> script. After the cluster installation is finished the same happens but with the <code>recipe-post-install</code> event and hook. The key-value store is used to signal plugin success or failure &ndash; after the plugins finished execution on a node, a new Consul key is added in the format <code>/events/&lt;event-id&gt;/&lt;node-hostname&gt;</code> that contains the exit status. Cloudbreak is able to check the key-value store if the recipe finished successfully or not.</p>

<!-- more -->


<h2>Register plugins for a cluster install</h2>

<p>We cannot predict all the custom use cases that can come up when installing a Hadoop cluster in the cloud, so we were focusing on making this feature easily extendable. We had to find a solution that enables someone to write their own script that will be run by Cloudbreak later. That&rsquo;s where the <em>plugn</em> project comes handy. With a simple <code>plugn install</code> command, a new plugin can be installed from <em>Github</em> so we only need to make one plugin available by default on every node &ndash; the <a href="https://github.com/sequenceiq/consul-plugins-install">one</a> that can install other plugins from a Github source. The other plugins are installed as the first step of a Cloudbreak cluster setup. This uses the same mechanism to trigger this plugin &ndash; it sends <code>plugin-install</code> events to Consul’s HTTP interface with the plugin source and name passed as parameters in the Consul event’s payload.</p>

<h3>Creating a plugin</h3>

<p>We’ve created an <a href="https://github.com/sequenceiq/consul-plugins-gcs-connector">example plugin</a> that downloads the <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">Google Cloud Storage connector for Hadoop</a> and puts it on the Hadoop classpath. As you can see a plugin is quite simple &ndash; it consists of a <code>.toml</code> descriptor, and the hook scripts. In the example only the <code>recipe-pre-install</code> hook is implemented, there is nothing to do after the cluster installation is done. Make sure that the hook scripts are executable when pushing them to Github.</p>

<h3>Adding properties to plugins</h3>

<p>Properties can be passed to plugins by using environment variables, but we use Consul&rsquo;s key-value store for that purpose instead. The GCS connector mentioned above needs a few more things to work besides the JAR on the classpath. To be able to authenticate to the Google Cloud Platform the connector also needs a private key in <code>p12</code> format. We have a <a href="https://github.com/sequenceiq/consul-plugins-gcp-p12">plugin</a> that does exactly this &ndash; it reads a <em>base64</em> encoded private key file located under the <code>recipes.gcp-p12.p12-encoded</code> key in the key-value store (using <code>curl</code> and some environment variables containing Consul’s HTTP address) and saves it in a local folder on the node.</p>

<h2>Putting things together</h2>

<p>We already know how to write plugins, how to get properties from the key-value store inside a plugin and how these things are triggered from Cloudbreak, but the key piece is missing: how do we tell <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> which plugins to install on our cluster and which properties to use. With the latest release a new resource is available on the API, the <em>recipe</em>. To create a new recipe make a <code>POST</code> to the <code>account/recipes</code> endpoint like this one:</p>

<p>```
{
  &ldquo;name&rdquo;: &ldquo;gcp-extension&rdquo;,
  &ldquo;description&rdquo;: &ldquo;sets up Google Cloud Storage connector on an Ambari cluster&rdquo;,
  &ldquo;properties&rdquo;: {</p>

<pre><code>"recipes.gcp-p12.p12-encoded": "&lt;base64-encoded-p12-file&gt;"
</code></pre>

<p>  },
  &ldquo;plugins&rdquo;: [</p>

<pre><code>"https://github.com/sequenceiq/consul-plugins-gcs-connector.git",
"https://github.com/sequenceiq/consul-plugins-gcp-p12.git"
</code></pre>

<p>  ]
}
```</p>

<p>To make sure that only trusted plugins are used in Cloudbreak, there is a validation on the source URL &ndash; plugins must come from configurable trusted Github accounts. It can be useful if you deploy your own <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> and want to make sure that only verified scripts can be executed in a cluster. Our hosted solution is configured to accept all Github accounts as trusted, so you can experiment with this feature more easily. After the recipe is created, the API answers with the ID of the created resource, so it can be used to create a cluster. The <code>recipeId</code> field is optional, and no scripts are executed if it is missing from the cluster <code>POST</code> request.</p>

<p><code>
{
  "name": "recipe-cluster",
  "blueprintId": 1400,
  "recipeId": 3744,
  "description": "Demonstrates the recipe feature"
}
</code></p>

<p>The recipes are not yet available on the Cloudbreak UI, if you’d like to try it out without hacking <code>curl</code> requests with proper authentication then I suggest to try the <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak Shell</a>. The requests above correspond to the following shell commands (assuming that the above recipe description is saved in <code>/tmp/test-recipe.json</code> and the cluster infrastructure &ndash; the stack &ndash; is already created):</p>

<p><code>
recipe add --file /tmp/test-recipe.json
recipe select --id &lt;recipe-id&gt;
blueprint select --id &lt;blueprint-id&gt;
</code></p>

<p>```</p>

<p>cluster create &mdash;description &ldquo;Demonstrates the recipe feature&rdquo;
```</p>

<h2>Future improvements</h2>

<p>This feature is <em>just a preview</em> in its current state, there are a few important parts that are missing. The most important one is that the plugins are currently installed and executed in all of the <code>ambari-agent</code> containers, but there are scenarios where it is not needed or not good at all. Consider the case where you’d like to add a JAR to HDFS &ndash; it should be run on only one of the nodes. It is also possible that a script should be executed only on a set of nodes, typically the nodes in a <code>hostgroup</code>. This means that the API will probably change in the next few weeks, but then we’ll update our blog too.</p>

<p>There are a few more things that you can expect to be implemented in the long run:</p>

<ul>
<li>install plugins from private sources too along public Github repositories</li>
<li>validate required properties when creating a new recipe</li>
</ul>


<p>If you have any comments, questions or feature requests about this topic, feel free to create an issue on Cloudbreak’s <a href="https://github.com/sequenceiq/cloudbreak/issues">Github page</a> or use the comments section below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Apache Spark with Cloudbreak]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak/"/>
    <updated>2015-01-30T12:04:22+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak</id>
    <content type="html"><![CDATA[<p>In the previous weeks many of you often asked us how to run our Apache Spark Docker <a href="https://github.com/sequenceiq/docker-spark">container</a> on a multi node cluster or how to install Spark and use it with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.
Cloudbreak uses Ambari (1.7) blueprints to provision multi node HDP clusters (on different cloud providers: AWS, Google Cloud, Azure, Openstack &ndash; with Rackspace and HP Helion coming soon).</p>

<p>In this post we&rsquo;d like to help you with installing Spark on <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> in a quick and easy way.</p>

<p>First of all you will have to create a cluster using Cloudbreak on your favorite cloud provider &ndash; Google Cloud, AWS, Azure or Openstack (check this <a href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/">post</a>) using a simple <code>multi-node-hdfs-yarn</code> blueprint. After your cluster is ready, you can install Apache Spark with the following steps:</p>

<h3>Install from the cloud instance</h3>

<p>First, you need to enter to one of your cloud instances. Then use the one-liner below:</p>

<p><code>
curl -Lo .docker-spark-install j.mp/spark-hdp-docker-install &amp;&amp; . .docker-spark-install
</code></p>

<p>After the file is downloaded it will be sourced, then you can use the following command:</p>

<p><code>
install-spark ambari-agent install
</code></p>

<!--more-->


<p>Alternatively you can install it without uploading the Spark assembly <code>uberjar</code> using :</p>

<p><code>
install-spark ambari-agent install-local
</code></p>

<p>After it is done, enter into the ambari-agent container:</p>

<p><code>
docker exec -it ambari-agent bash
</code></p>

<p>Apache Spark will be installed at /usr/local/spark in the container. If you want to try it you need to configure a few environment variables such as YARN_CONF_DIR or SPARK_JAR (see the Install from container option)</p>

<h3>Install from container</h3>

<p>If you entered in one of your cloud instances, enter into the ambari-agent container: (same as you seen above):</p>

<p><code>
docker exec -it ambari-agent bash
</code></p>

<p>Inside the container use the following command:</p>

<p><code>
curl -Lo .spark-install j.mp/spark-hdp-install &amp;&amp; . .spark-install
</code></p>

<p>Then you can install spark with &ldquo;install-spark &lt;install/install-local>&rdquo; command:</p>

<p><code>
install-spark install
</code></p>

<p>With this approach you do not need to set up your environment variables. The script will do it for you.</p>

<h2>Run examples</h2>

<p><code>
spark-submit --class org.apache.spark.examples.SparkPi    --master yarn-cluster  --num-executors 3 --driver-memory 512m  --executor-memory 512m  --executor-cores 1  /usr/local/spark/lib/spark-examples*.jar 10
</code></p>

<h2>Issues</h2>

<ul>
<li>You need to install Spark into every node (you can use it only on 1, but it is not the best approach).</li>
<li>You do not want to enter docker container or even cloud instances to do things like this.</li>
</ul>


<p>As you see until Ambari will not fully support Spark installation with Blueprints this is not an ideal situation.</p>

<p>Nevertheless we understood this and with the introduction of <strong>recipes</strong> in the latest Cloudbreak release we are going to publish a new Cloudbreal Spark recipe next week. In the meanwhile stay tuned as we are publishing a post early next week about the concept and architecture of <code>recipes</code>, how to use it and will publish a few custom ones (by request).</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - new release available]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta/"/>
    <updated>2015-01-28T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta</id>
    <content type="html"><![CDATA[<p>We are happy to announce that the <code>Release Candidate</code> version of Cloudbreak is almost around the corner. This major release is the last one in the <code>public beta program</code> and contains quite a few new features and architectural changes.</p>

<p>All theses new major features will be covered in the coming weeks on our blog, but in the meantime let us do a quick skim through.</p>

<h3>Accounts</h3>

<p>We have introduced the concept of <code>accounts</code> &ndash; after a user registers/signs in the first time will have the option to invite other people in the <code>account</code>. Being the administrator of the account, will have the option to activate, deactive and give admin rights for all the invited users.</p>

<p>Users can share <code>resources</code> (such as: cloud credentials, templates, blueprints, clusters) within the account by making it <code>public in account</code> but at the same time can create his own private resources as well. As you might be already aware, we use OAuth2 to make all these possible.</p>

<h3>Usage explorer</h3>

<p>We have built a unified (accross all cloud providers) usage explorer tool, where you can drill down into details to learn your (or in your account if you have admin rights) usage history. You can filter by date, users, cloud providers, region, etc &ndash; and generate a consolidated table/chart overview.</p>

<h3>Heterogenous clusters</h3>

<p>This was a feature many have asked &ndash; and we are happy to deliver it. Up till now all the nodes in your YARN clusters were built on the same cloud <code>instance types</code>. While this was an easy an convenient way to build a cluster (as far as we are aware all the Hadoop as a Service providers are doing it this way) back in the MR1 era, times changed now and with the emergence of <code>YARN</code> different workloads are running within a cluster.</p>

<p>While for example Spark jobs require a high memory instance a legacy MR2 code might require a high CPU instance, whereas a HBase RegionServer likes better a high I/O throughput one.</p>

<p>At SequenceIQ we have quickly realized this and the new release allows you to apply different <code>stack templates</code> to all these YARN services/components. We do the heavy lifting for you in the background &ndash; the only thing you will have to do is to associate stack templates to Ambari <code>hostgroups</code>.</p>

<p>This is a major step forward when you are using and running different workloads on your YARN cluster &ndash; and not just saving on costs but at the same time increasing your cluster throughput.</p>

<!--more-->


<h3>Hostgroup based autoscaling</h3>

<p>Cloudbreak now integrates with <a href="http://sequenceiq.com/periscope">Periscope</a> &ndash; and allows you to set up alarms and autoscaling SLA policies based on YARN metrics. Having done the heterogenous cluster integration, now it&rsquo;s time to apply <code>autoscaling</code> for those nodes based on Ambari Blueprints.</p>

<h3>Recipes</h3>

<p>While Cloudbreak and Ambari combined are a pretty powerful way to configure your Hadoop cluster, sometimes there are manuall steps required to reconfigure services, build dependent cluster architectures (e.g.: permanent and ephemeral clusters), etc &ndash; the list can be long.
Even a simple configuration on a large (thousands nodes) cluster is a tedious job &ndash; and usually people use Ansible, Chef, Puppet or Saltstack to do so &ndash; however these all have some drawback and are not integrated with Cloudbreak. As Cloudbreak under the hood uses Consul, we came up with a simple solution which facilitates creating, applying and running <code>recipes</code> on your already provisioned cluster &ndash; pre/post Ambari installation. A follow up blog post will be released in the coming days whch will explain the concept, architecture and gives you a few sample recipes.</p>

<h3>OpenStack</h3>

<p>This was one of the other highly desired features &ndash; and a perfect use case for Docker. You might be aware that we run the full Hadoop stack inside Docker container &ndash; and Cloudbreak&rsquo;s integration with the cloud provider is pretty thin. This gives us the option to add quick integration with a new cloud provider &ndash; the full OpenStack integration with Cloudbreak took few weeks only.</p>

<p>Long story short &ndash; Cloudbreak now support and automates provisioning of Hadoop clusters with custom blueprints on OpenStack. Give it a try and let us know how it works for you.</p>

<h3>Ambari 1.7 integration</h3>

<p>Shortly after Ambari 1.7 came out we have upgraded Cloudbreak to use this new version. Ambari 1.7 supports HDP and Bigtop stacks and an increased number of Hadoop services/components.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Cloudbreak release - support for HDP 2.2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/"/>
    <updated>2014-12-23T12:59:42+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2</id>
    <content type="html"><![CDATA[<p>The last two weeks were pretty busy for us &ndash; we have <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">Dockerized</a> the new release of Ambari (1.7.0), <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">integrated</a> Periscope with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Cloudbreak</a> and just now we are announcing a new Cloudbreak <a href="https://cloudbreak.sequenceiq.com">release</a> which uses Ambari 1.7.0 and has full support for Hortonworks HDP 2.2 and Apache Bigtop stacks. But first &ndash; since this has been asked many times &ndash; see a <code>short</code> movie about Cloudbreak and Periscope in action.</p>

<h2>On-demand Hadoop cluster with autoscaling</h2>

<iframe width="640" height="480" src="//www.youtube.com/embed/E6bnEW76H_E" frameborder="0" allowfullscreen></iframe>




<!--more-->


<h2>Ambari 1.7.0</h2>

<p>The Ambari community recently released the 1.7.0 version which comes with lots of new features and bug fixes. We&rsquo;ve been testing the new version
internally for a while now and finally made it to Cloudbreak. Just to highlight the important ones:</p>

<ul>
<li>Ambari Views framework</li>
<li>Ambari Administration

<ul>
<li>Management of users/groups</li>
<li>Management of view instances</li>
<li>Management of cluster permissions</li>
</ul>
</li>
<li>Cancel/Abort background operation requests</li>
<li>Expose Ambari UI for config versioning, history and rollback</li>
<li>Ability to manage -env.sh configuration files</li>
<li>Recommendations and validations (via a &ldquo;Stack Advisor&rdquo;)</li>
<li>Export service configurations via Blueprint</li>
<li>Install + Manage Flume</li>
<li>HDFS Rebalance</li>
<li>ResourceManager HA</li>
</ul>


<p>These are nice features but for us one of the most important thing is that it allows you to install the latest versions of the Hadoop ecosystem.
As usual the Docker image is available for <em>local</em> deployments as well, described <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">here</a>.</p>

<p><code>Note: There were small changes around the API so if you built an application on top of it check your REST calls. The Ambari Shell and the
underlying Groovy rest client have been updated and will go into the Apache repository once it's passed the reviews.</code></p>

<h2>Hadoop 2.6</h2>

<p>Since with Ambari 1.7.0 we&rsquo;re able to install Hadoop 2.6 let&rsquo;s see what happened in <code>YARN</code> in the last couple of months (it&rsquo;s stunning):</p>

<ul>
<li>Support for long running services &ndash; install <em>Slider</em> with Ambari and scale your Hadoop services!

<ul>
<li>Service Registry for applications</li>
</ul>
</li>
<li>Support for rolling upgrades &ndash; wow!

<ul>
<li>Work-preserving restarts of ResourceManager</li>
<li>Container-preserving restart of NodeManager</li>
</ul>
</li>
<li>Supports node labels during scheduling &ndash; label based scaling is on the way with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Periscope</a></li>
<li>Support for time-based resource reservations in Capacity Scheduler (beta) &ndash; more on this awesome feature soon</li>
<li>Support running of applications natively in Docker containers (alpha) &ndash; <a href="http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker/">Docker in Docker</a></li>
</ul>


<p>I&rsquo;m excited about these great innovations (not, because we&rsquo;re involved in a few of them), but because people can leverage them by using Cloudbreak.</p>

<h2>HDP 2.2 blueprint</h2>

<p>I have created a blueprint which is not an <code>official</code> one, but it contains a few from the new services like: <code>SLIDER</code>, <code>KAFKA</code>, <code>FLUME</code>.
```
{
  &ldquo;configurations&rdquo;: [
  {</p>

<pre><code>"nagios-env": {
  "nagios_contact": "admin@localhost"
}
},
{
  "hive-site": {
    "javax.jdo.option.ConnectionUserName": "hive",
    "javax.jdo.option.ConnectionPassword": "hive"
  }
}
</code></pre>

<p>  ],
  &ldquo;host_groups&rdquo;: [</p>

<pre><code>{
  "name": "master_1",
  "components": [
    {
      "name": "NAMENODE"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "HBASE_MASTER"
    },
    {
      "name": "GANGLIA_SERVER"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_SERVER"
    },
    {
      "name": "FLUME_HANDLER"
    },
    {
      "name": "KAFKA_BROKER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_2",
  "components": [
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "HISTORYSERVER"
    },
    {
      "name": "HIVE_SERVER"
    },
    {
      "name": "SECONDARY_NAMENODE"
    },
    {
      "name": "HIVE_METASTORE"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MYSQL_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "WEBHCAT_SERVER"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_3",
  "components": [
    {
      "name": "RESOURCEMANAGER"
    },
    {
      "name": "APP_TIMELINE_SERVER"
    },
    {
      "name": "SLIDER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "master_4",
  "components": [
    {
      "name": "OOZIE_SERVER"
    },
    {
      "name": "ZOOKEEPER_SERVER"
    },
    {
      "name": "GANGLIA_MONITOR"
    }
  ],
  "cardinality": "1"
},
{
  "name": "slave_1",
  "components": [
    {
      "name": "HBASE_REGIONSERVER"
    },
    {
      "name": "NODEMANAGER"
    },
    {
      "name": "DATANODE"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "FALCON_CLIENT"
    },
    {
      "name": "OOZIE_CLIENT"
    }
  ],
  "cardinality": "${slavesCount}"
},
{
  "name": "gateway",
  "components": [
    {
      "name": "AMBARI_SERVER"
    },
    {
      "name": "NAGIOS_SERVER"
    },
    {
      "name": "ZOOKEEPER_CLIENT"
    },
    {
      "name": "PIG"
    },
    {
      "name": "OOZIE_CLIENT"
    },
    {
      "name": "HBASE_CLIENT"
    },
    {
      "name": "HCAT"
    },
    {
      "name": "SQOOP"
    },
    {
      "name": "HDFS_CLIENT"
    },
    {
      "name": "HIVE_CLIENT"
    },
    {
      "name": "YARN_CLIENT"
    },
    {
      "name": "MAPREDUCE2_CLIENT"
    },
    {
      "name": "GANGLIA_MONITOR"
    },
    {
      "name": "KNOX_GATEWAY"
    }
  ],
  "cardinality": "1"
}
],
"Blueprints": {
  "blueprint_name": "hdp-multinode-sequenceiq",
  "stack_name": "HDP",
  "stack_version": "2.2"
}
</code></pre>

<p>}
```</p>

<h2>What&rsquo;s next?</h2>

<p><blockquote><p>Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.</p></blockquote></p>

<p>We&rsquo;ve walked a long journey since we started the company almost a year ago to reach where we are now, but our products are not complete yet. We have big plans
with our product stacks. A couple of things from our roadmap:</p>

<h3>Cloudbreak</h3>

<ul>
<li>Cloudbreak currently supports homogeneous cluster deployments which we&rsquo;re going to change. The heterogeneous stack structure is more convenient
from Hadoop&rsquo;s perspective. The ability to define different type of cloud instances is a must, giving the users the option to use much more
powerful instances for the <code>ResourceManager</code> and <code>NameNodes</code>.</li>
<li>Service discovery and decentralization is always a key aspect. At the moment we&rsquo;re using Serf and dnsmasq, but we&rsquo;re already started the
integration with <a href="https://consul.io">Consul</a> which generally is a better fit. It provides service registration via DNS, key-value store and
decentralization across datacenters.</li>
<li>The deployment of Cloudbreak itself is going to change and use Consul with other side projects like <code>Consul templates</code> or <code>Registrator</code>. The
deployment is already based on Docker, but will be much more simplified.</li>
<li>Custom stack deployments with Ambari will be supported as <code>"recipes"</code>.</li>
<li>Generating reports of cloud instance usages and cost calculation.</li>
<li>Web hooks to subscribe to different cluster events.</li>
<li>Shared/company accounts.</li>
</ul>


<h3>Periscope</h3>

<ul>
<li>Add more <code>YARN</code> and <code>NameNode</code> related metrics.</li>
<li>Node label based scaling.</li>
<li>Pluggable metric system for custom metrics.</li>
<li>Application movement in Capacity Scheduler queues enforcing SLAs.</li>
<li>Time-based resource reservations in Capacity Scheduler for Applications.</li>
<li>Integration with <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">ELK</a>.</li>
</ul>


<h2>Happy Holidays</h2>

<p>We&rsquo;re taking a short break of writing new blog posts until next year. You can still reach us on the usual social sites, but you can expect
small delays for answering questions. <code>Happy Holidays everyone.</code></p>

<p><a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> <a href="https://twitter.com/sequenceiq">Twitter</a> <a href="https://www.facebook">Facebook</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak welcomes Periscope]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/"/>
    <updated>2014-12-12T14:13:33+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope</id>
    <content type="html"><![CDATA[<p>Today we have pushed out a new release of <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> &ndash; our Docker container based and cloud agnostic Hadoop as a Service solution &ndash; containing a few major changes. While there are many significant changes (both functional and architectural) in this blog post we&rsquo;d like to describe one of the most expected one &ndash; the <code>autoscaling</code> of Hadoop clusters.</p>

<p>Just to quickly recap, Cloudbreak allows you to provision clusters &ndash; <code>full stacks</code> &ndash; in all major cloud providers using a unified API, UI or CLI/shell. Currently we support provisioning of clusters in <code>AWS</code>, <code>Google Cloud</code>, <code>Azure</code> and <code>OpenStack</code> (in private beta) &ndash; new cloud providers can be added quite easily (as everything runs in Docker) using our SDK.</p>

<p><a href="http://sequenceiq.com/periscope/">Periscope</a> allows you to configure SLA policies for your Hadoop cluster and scale up or down on demand. You are able to set alarms and notifications for different metrics like <code>pending containers</code>, <code>lost nodes</code> or <code>memory usage</code>, etc and set SLA scaling policies based on these alarms.</p>

<p>Today&rsquo;s <a href="http://cloudbreak.sequenceiq.com/">release</a> made available the integration between the two projects (they work independently as well) and allows subscribers to enable autoscaling for their already deployed or newly created Hadoop cluster.</p>

<p>We would like to guide you through the UI and help you to set up an autoscaling Hadoop cluster.</p>

<!--more-->


<h2>Using Periscope</h2>

<p>Once you have created your Hadoop clusters with Cloudbreak you will now how the option to configure autoscaling policies.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/select.png" alt="" /></p>

<p>In order to configure autoscaling for your cluster you should go to <code>autoscaling SLA policies</code> tab and hit the <code>enable</code> button.</p>

<h3>Alarms</h3>

<p>Periscope allows you to configure two types of <code>alarms</code>.</p>

<p><strong>Metric based</strong> alarms are alarms based on different <code>YARN</code> metrics. A plugin mechanism will be available in case you&rsquo;d like to plug your own metrics. As a quick note, we have another project called <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">Baywatch</a> where we collect around 400 Hadoop metrics &ndash; and those will be all pluggable in Periscope.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-metric.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>metrics &ndash; currently the default YARN metrics we support are: <code>pending containers</code>, <code>pending applications</code>, <code>lost nodes</code>, <code>unhealthy nodes</code> and <code>global resources</code></li>
<li>period &ndash;  the time that the metric has to be sustained in order for an alarm to be triggered</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<p><strong>Time based</strong> alarms allow autoscaling of clusters based on the configured time. We have <a href="http://blog.sequenceiq.com/blog/2014/11/25/periscope-scale-your-cluster-on-time/">blogged</a> about this new feature recently &ndash; with this new release of <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> this feature is available through UI as well.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-time.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>time zone &ndash; the timezone for the <code>cron</code> expression</li>
<li>cron expression &ndash; the cron expression</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<h2>Scaling policies</h2>

<p>Once you have an alarm you can configure scaling policies based on it. Scaling policies defines the actions you&rsquo;d like Periscope to take in case of a triggered alarm.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/scaling.png" alt="" /></p>

<ul>
<li>policy name &ndash; the name of the SLA scaling policy</li>
<li>scaling adjustment &ndash; the adjustment counted in <code>nodes</code>, <code>percentage</code> or <code>exact</code> numbers of cluster nodes</li>
<li>host group &ndash; the <code>autoscaled</code> Ambari hostgroup</li>
<li>alarm &ndash; the configured alarm</li>
</ul>


<h2>Cluster scaling configurations</h2>

<p>A cluster has a default configuration which Periscope scaling policies can&rsquo;t override. This is due to avoid over or under scaling a Hadoop cluster with policies and also to definde a cooldown time period between two scaling actions.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/cluster-config.png" alt="" /></p>

<ul>
<li>cooldown time &ndash; the time spent between two scaling actions</li>
<li>cluster size min. &ndash; the minimum size (in nodes) of a cluster</li>
<li>cluster size max. &ndash; the maximum size (in nodes) of a cluster</li>
</ul>


<p>It&rsquo;s that simple. Happy autoscaling.</p>

<p>In case you&rsquo;d like to test autoscaling and generate some load on your cluster you can use these <code>stock</code> Hadoop examples and the scripts below:</p>

<p>```</p>

<h1>!/bin/bash</h1>

<p>export HADOOP_LIBS=/usr/lib/hadoop-mapreduce
export JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient-2.4.0.2.1.2.0-402-tests.jar</p>

<p>smalljobs(){
  echo &ldquo;############################################&rdquo;
  echo Running smalljobs tests..
  echo &ldquo;############################################&rdquo;</p>

<p>  CMD=&ldquo;hadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/hrt_qa/smallJobsBenchmark -numRuns 2 -maps 10 -reduces 5 -inputLines 10 -inputType ascending&rdquo;
  echo TEST 1: $CMD
  su hdfs -c &ldquo;$CMD&rdquo; 1> smalljobs-time.log 2> smalljobs.log
}</p>

<p>smalljobs
```</p>

<p>To test it you can run it with the following script:</p>

<p>```</p>

<h1>!/bin/bash</h1>

<p>for i in {1..10}
do
nohup /test.sh &amp;
done
```</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
