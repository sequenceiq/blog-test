<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Morphline | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/morphline/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-11-03T10:36:49+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data cleaning with MapReduce and Morphlines]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/"/>
    <updated>2014-03-11T09:21:07+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines</id>
    <content type="html"><![CDATA[<p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<p><code>
userID  movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
75      3       1       29       10           2006      23          17          16
75      32      4.5     29       10           2006      23          23          44
75      110     4       29       10           2006      23          30          8
75      160     2       29       10           2006      23          16          52
75      163     4       29       10           2006      23          29          30
75      165     4.5     29       10           2006      23          25          15
75      173     3.5     29       10           2006      23          17          37
</code></p>

<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<p>```
{</p>

<pre><code>readCSV {
    separator : "\t"
    columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
    ignoreFirstLine : false
    trim : true
    charset : UTF-8
}
</code></pre>

<p>}</p>

<p>{</p>

<pre><code>java {
  imports : "import java.util.*;"
  code: """
      record.removeAll("date_hour");
      record.removeAll("date_minute");
      record.removeAll("date_second");
      return child.process(record);
}
</code></pre>

<p>}
```</p>

<!-- more -->


<p>Now lets create our mapper only job to process the data. What we need to do is build the Morphlines command chain by parsing the configuration file as shown</p>

<p><code>java protected void setup(Context context)
File morphLineFile = new File(context.getConfiguration().get(MORPHLINE_FILE));
String morphLineId = context.getConfiguration().get(MORPHLINE_ID);
RecordEmitter recordEmitter = new RecordEmitter(context);
MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
Command morphline = new org.kitesdk.morphline.base.Compiler().compile(morphLineFile, morphLineId, morphlineContext, recordEmitter);
</code>
and pass the lines through it.
```java protected void map(Object key, Text value, Context context)
Record record = new Record()
record.put(Fields.ATTACHMENT_BODY, new ByteArrayInputStream(value.toString().getBytes()));
if (!morphline.process(record)) {</p>

<pre><code>LOGGER.info("Morphline failed to process record: {}", record);
</code></pre>

<p>}
record.removeAll(Fields.ATTACHMENT_BODY);
<code>
Notice that the compile method takes an important parameter called finalChild which is in our example the `RecordEmitter`.
The returned command will feed records into finalChild which means if this parameter is not provided a DropRecord command will
be assigned automatically. In Apache Flume there is a Collector command to avoid loosing any transformed record.
The only thing left is to outbox the processed record and write the results to HDFS. The RecordEmitter will serve this purpose:
</code>java
@Override
public boolean process(Record record) {</p>

<pre><code>line.set(record.toString());
</code></pre>

<p>  try {</p>

<pre><code>context.write(line, null);
</code></pre>

<p>  } catch (Exception e) {</p>

<pre><code>  LOGGER.warn("Cannot write record to context", e);
</code></pre>

<p>  }
  return true;
}
<code>
By default the readCSV command extracts the ATTACHMENT_BODY into headers with id provided in the columns field so the
transformed data will look like this (3 columns were dropped):
</code>
{date_day=[10], date_month=[10], date_year=[2008], movieID=[62049], rating=[4.5], userID=[71534]}
```
The source code is available in our samples repository on <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a>.
It is just a simple example but you can go further and download a much bigger dataset with 10 millions of lines and process it with multiple nodes to see how it scales.</p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
