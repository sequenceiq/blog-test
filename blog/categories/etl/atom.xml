<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ETL | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/etl/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-02T17:35:55+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data cleaning with MapReduce and Morphlines]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/"/>
    <updated>2014-03-11T09:21:07+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines</id>
    <content type="html"><![CDATA[<p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<p><code>
userID  movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
75      3       1       29       10           2006      23          17          16
75      32      4.5     29       10           2006      23          23          44
75      110     4       29       10           2006      23          30          8
75      160     2       29       10           2006      23          16          52
75      163     4       29       10           2006      23          29          30
75      165     4.5     29       10           2006      23          25          15
75      173     3.5     29       10           2006      23          17          37
</code></p>

<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<p>```
{</p>

<pre><code>readCSV {
    separator : "\t"
    columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
    ignoreFirstLine : false
    trim : true
    charset : UTF-8
}
</code></pre>

<p>}</p>

<p>{</p>

<pre><code>java {
  imports : "import java.util.*;"
  code: """
      record.removeAll("date_hour");
      record.removeAll("date_minute");
      record.removeAll("date_second");
      return child.process(record);
}
</code></pre>

<p>}
```</p>

<!-- more -->


<p>Now lets create our mapper only job to process the data. What we need to do is build the Morphlines command chain by parsing the configuration file as shown</p>

<p><code>java protected void setup(Context context)
File morphLineFile = new File(context.getConfiguration().get(MORPHLINE_FILE));
String morphLineId = context.getConfiguration().get(MORPHLINE_ID);
RecordEmitter recordEmitter = new RecordEmitter(context);
MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
Command morphline = new org.kitesdk.morphline.base.Compiler().compile(morphLineFile, morphLineId, morphlineContext, recordEmitter);
</code>
and pass the lines through it.
```java protected void map(Object key, Text value, Context context)
Record record = new Record()
record.put(Fields.ATTACHMENT_BODY, new ByteArrayInputStream(value.toString().getBytes()));
if (!morphline.process(record)) {</p>

<pre><code>LOGGER.info("Morphline failed to process record: {}", record);
</code></pre>

<p>}
record.removeAll(Fields.ATTACHMENT_BODY);
<code>
Notice that the compile method takes an important parameter called finalChild which is in our example the `RecordEmitter`.
The returned command will feed records into finalChild which means if this parameter is not provided a DropRecord command will
be assigned automatically. In Apache Flume there is a Collector command to avoid loosing any transformed record.
The only thing left is to outbox the processed record and write the results to HDFS. The RecordEmitter will serve this purpose:
</code>java
@Override
public boolean process(Record record) {</p>

<pre><code>line.set(record.toString());
</code></pre>

<p>  try {</p>

<pre><code>context.write(line, null);
</code></pre>

<p>  } catch (Exception e) {</p>

<pre><code>  LOGGER.warn("Cannot write record to context", e);
</code></pre>

<p>  }
  return true;
}
<code>
By default the readCSV command extracts the ATTACHMENT_BODY into headers with id provided in the columns field so the
transformed data will look like this (3 columns were dropped):
</code>
{date_day=[10], date_month=[10], date_year=[2008], movieID=[62049], rating=[4.5], userID=[71534]}
```
The source code is available in our samples repository on <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a>.
It is just a simple example but you can go further and download a much bigger dataset with 10 millions of lines and process it with multiple nodes to see how it scales.</p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ETL - producing better quality data]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/"/>
    <updated>2014-02-28T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality</id>
    <content type="html"><![CDATA[<p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>

<!-- more -->


<p>Since runs on Hadoop, scalability is not a problem &ndash; we have seen enterprises producing 50 terabytes data per day and missing the 24 hour ETL window, by not being able to scale horizontally their ETL processes. Morphlines is built on top of the Kite framework (a great framework for making easier to build systems on top of the Hadoop), and it&rsquo;s extremely easy to extend. We would like to show and give you examples about how to use and create a Morphlines Command to implement your custom transformation (if the many existing ones does not fit your requirement).</p>

<p>The implementation of a Command starts with implementing a CommandBuilder</p>

<p>Actually a new morphline command implementation is not that hard. You have to implement a builder class (see below), define the name of the command and  extend the AbstractCommand base class. That simple.</p>

<p>``` java ToLowerCaseBuilder implements CommandBuilder</p>

<p>@Override
  public Collection<String> getNames() {</p>

<pre><code>return Collections.singletonList("toLowerCase");
</code></pre>

<p>  }</p>

<p>```</p>

<p>``` java toLowerCase Morphlines command</p>

<p>@Override
protected boolean doProcess(Record record) {
  ListIterator iter = record.get(fieldName).listIterator();
  while (iter.hasNext()) {</p>

<pre><code>iter.set(transformFieldValue(iter.next()));
</code></pre>

<p>  }</p>

<pre><code>return super.doProcess(record);
</code></pre>

<p>}</p>

<p>  private Object transformFieldValue(Object value) {</p>

<pre><code>return value.toString().toLowerCase(locale);
</code></pre>

<p>  }</p>

<p>```</p>

<p>To configure your new morhline command</p>

<p>``` java toLowerCase config</p>

<p>morphlines : [
  {
   id : morphline1</p>

<pre><code>   importCommands : ["com.sequenceiq.samples.**"]

   commands : [
     {
       toLowerCase {
         field : Name
         locale : en_us
       }
     }
   ]
</code></pre>

<p>  }
]</p>

<p>```</p>

<p>There is a custom tester tool which waiting a file as an input, a file as a config and an expected output. Among our plans is to build a UI on top of Kite Morphlines as well &ndash; part of our product stack. While we consider the Morhplines configuration, and defining the transformations simple and easy to use, many of our users might prefer a custom UI whee you can define your own ETL process visually.</p>

<p>That’s it. You can find the samples at our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub page</a>.
Hope you enjoy it, let us know if you need help or have any questions.</p>
]]></content>
  </entry>
  
</feed>
