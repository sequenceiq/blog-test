<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HDP2 | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hdp2/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-15T14:08:57+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS and java.nio.channels]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs/"/>
    <updated>2014-03-07T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs</id>
    <content type="html"><![CDATA[<p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<p>```
java BufferedInputStream bufferedInputStream</p>

<p>/<em>*
 * For the sake of readability, try/cacth/finally blocks are removed
 * Don&rsquo;t Say We Didn&rsquo;t Warn You
 </em>/</p>

<p>FileSystem fs = FileSystem.get(configuration);</p>

<pre><code>        Path filePath = getFilePath(dataPath);
</code></pre>

<p>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));</p>

<pre><code>listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
                    CsvPreference.STANDARD_PREFERENCE);
</code></pre>

<p>```
The exception looks like this:</p>

<p>```
ERROR SimpleFeatureSelector:67 &ndash; Exception {}
java.lang.IllegalStateException: Must not use direct buffers with InputStream API</p>

<pre><code>at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)
</code></pre>

<p>```</p>

<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<!-- more -->


<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<p>``` java</p>

<p>@SuppressWarnings(&ldquo;deprecation&rdquo;)
public static BlockReader newBlockReader(</p>

<pre><code>                                 Conf conf,
                             Socket sock, String file,
                                 ExtendedBlock block, 
                                 Token&lt;BlockTokenIdentifier&gt; blockToken,
                                 long startOffset, long len,
                                 int bufferSize, boolean verifyChecksum,
                                 String clientName)
                                 throws IOException {
if (conf.useLegacyBlockReader) {
  return RemoteBlockReader.newBlockReader(
      sock, file, block, blockToken, startOffset, len, bufferSize, verifyChecksum, clientName);
} else {
  return RemoteBlockReader2.newBlockReader(
      sock, file, block, blockToken, startOffset, len, bufferSize, verifyChecksum, clientName);      
}
</code></pre>

<p>  }</p>

<p>```</p>

<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing HDP2 sandbox from the host]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox/"/>
    <updated>2014-03-05T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox</id>
    <content type="html"><![CDATA[<p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<p><code>bash
ssh root@127.0.0.1 -p 2222 -D 1099
</code></p>

<!-- more -->


<p>Once the proxy is up and running, edit the core-site.xml</p>

<p>``` xml</p>

<pre><code>    &lt;property&gt;
        &lt;name&gt;hadoop.socks.server&lt;/name&gt;
        &lt;value&gt;localhost:1099&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.rpc.socket.factory.class.default&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.net.SocksSocketFactory&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p>```</p>

<p>Now you can run the test client</p>

<p>``` bash</p>

<h1>You can use Maven</h1>

<p>mvn exec:java -Dexec.mainClass=&ldquo;com.sequenceiq.samples.SandboxTester&rdquo; -Dexec.args=&ldquo;hdfs sandbox 8020&rdquo; -Dhadoop.home.dir=/tmp</p>

<h1>or run from the JAR file</h1>

<p>java -jar sandbox-playground-1.0.jar hdfs sandbox 8020
```</p>

<p>As you see it&rsquo;s pretty easy and convenient to use the Hortonworks sandbox as a pre-configured development environment.</p>

<p>In case you&rsquo;d like to use (as we do most of the time) a Hadoop cluster in the cloud (Amazon EC2), check our previous blog post <a href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/">HDP2 on Amazon</a>.
In case you ever wondered whether it&rsquo;s possible to use Hadoop with Docker please follow this blog.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set up HDP2 on Amazon EC2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/"/>
    <updated>2014-02-07T16:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon</id>
    <content type="html"><![CDATA[<p>During the last years we have seen many blog entries and articles about how to set up Hadoop on Amazon EC2. All these tutorials and articles had one thing in common &ndash; you had to go through a large number of manual (and painful) steps, read screenshots and redo the whole thing all over again, in case you needed a new cluster.</p>

<p>Since we use Amazon EC2 quite a lot, and Hadoop as well (Hortonworks distribution) we have gone through these steps many times &ndash; and have scripted the whole process from the first steps up to launching an N node Hadoop/HDP2 cluster in less then five minutes.</p>

<p>Moreover, the cluster is a &lsquo;production ready&rsquo; setup from infrastructural point of view &ndash; it is provisioned in a logically isolated section of the cloud (Virtual Private Cloud), with his own IP address range, creation of subnets, and configuration of route tables and network gateways.</p>

<p>Once the instances are provisoned, the HDP2 setup is done by Apache Ambari &ndash; for more advanced users we will provide the setup thorugh Ambari&rsquo;s RESTful API &ndash; watch this space or our GitHub page.</p>

<p>All the EC2 instances are tagged with the user name &ndash; thus you can create different clusters for different employees, all under the same AWS account (with IAM).</p>

<p>We believe that this is the right way to provision Hadoop in the cloud &ndash; during development and testing we had to provision Hadoop clusters of different sizes, and going through these steps manually would take lots of time.
This way we are able to provision clusters in the cloud in the matter of minutes &ndash; independently of the size.</p>

<p>The script is available at: <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">https://github.com/sequenceiq/hadoop-cloud-scripts</a></p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
