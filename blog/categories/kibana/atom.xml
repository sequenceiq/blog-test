<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kibana | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/kibana/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-05T14:25:21+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time monitoring of Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/"/>
    <updated>2014-10-07T18:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring</id>
    <content type="html"><![CDATA[<p>Although various solutions have been created in software industry for monitoring of activities taking place in a cluster, but it turned out that only a very few of them satisfies the rest of our needs. When we made the decision about which monitoring libraries and components
are integrated to our stack we kept in mind that it needs to be:</p>

<ul>
<li><p><strong>scalable</strong> to be able to efficiently monitor small Hadoop clusters which are consisting of only a few nodes and also clusters which containing thousands of nodes</p></li>
<li><p><strong>flexible</strong> to be able provide overview about the health of the whole cluster or about the health individual node or even dive deeper into the internals of Hadoop, e.g. shall be able to visualize how our autoscaling solution for Hadoop YARN called  <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> moves running applications between <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a></p></li>
<li><p><strong>extensible</strong> to be able to use the gathered and stored data by extensions written by 3rd parties, e.g. a module which processes the stored (metrics) data and does real-time anomaly detection</p></li>
</ul>


<p>Based on the requirements above our choice was the:</p>

<ul>
<li><a href="http://logstash.net">Logstash</a> for log/metics enrichment, parsing and transformation</li>
<li><a href="http://www.elasticsearch.org">Elasticsearch</a> for data storage, indexing</li>
<li><a href="http://www.elasticsearch.org/overview/kibana">Kibana</a> for data visualization</li>
</ul>


<h2>High Level Architecture</h2>

<p>In our monitoring solution one of the design goal was to provide a <strong>generic, pluggable and isolated monitoring component</strong> to existing Hadoop deployments. We also wanted to make it non-invasive and avoid adding any monitoring related dependency to our Ambari, Hadoop or other Docker images. For that reason we have packaged the monitoring client component into its own Docker image which can be launched alongside with a Hadoop running in an other container or even alongside a Hadoop which is not even containerized.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png</a>"></p>

<p>In a nutshell the monitoring solution consist of client and server containers. The server contains the Elasticsearch and the Kibana module. The server container is horizontally scalable and it can be clustered trough the clustering capabilities of Elasticsearch.</p>

<p>The client container &ndash; which is deployed on the machine what is needed to be monitored &ndash; contains the Logstash and the collectd module. The Logstash connects to Elasticsearch cluster as client and stores the processed and transformed metrics data there.</p>

<!-- more -->


<h2>Hadoop metics</h2>

<p>The metrics data what we are collecting and visualizing are provided by <a href="http://blog.cloudera.com/blog/2012/10/what-is-hadoop-metrics2">Hadoop metrics</a>, what is a collection of runtime information that are exposed by all Hadoop daemons. We have configured the Metrics subsystem in that way that it writes the valuable metrics information into the filesystem.</p>

<p>In order to be able to access to the metrics data from the monitoring client component &ndash; which is running inside a different Docker container &ndash; we used the capability of <a href="https://docs.docker.com/userguide/dockervolumes">Docker Volumes</a> which basically let&rsquo;s you access a directory within one container form other container or even access directories from host systems.</p>

<p>For example if you would like mount the <code>/var/log</code> from the container named <code>ambari-singlenode</code> under the <code>/amb/log</code> in the monitoring client container then following sequence of commands needs to be executed:

<code>bash
EXPOSED_LOG_DIR=$(docker inspect --format='{{index .Volumes "/var/log"}}' ambari-singlenode)
docker run -i -t -v $EXPOSED_LOG_DIR:/amb/log  sequenceiq/docker-elk-client /etc/bootstrap.sh -bash
</code>
</p>

<p>Hundreds of different metrics are gathered form Hadoop metrics subsystem and all data is transformed by Logstash to JSON and stored to ElasticSearch to make it ready for querying or displaying it with Kibana.</p>

<p>The screenshot below has been created about one of our sample dashboard which is displaying Hadoop metrics for a little cluster which was started on my notebook. In this cluster the Yarn&rsquo;s Capacity Scheduler is used and for demonstration purposes I have created a queue called highprio alongside with the default queue. I have reduced the the capacity of default queue to 30 and defined the highprio queue with capacity of 70.
The red line in the screenshot belongs to the highprio queue the yellow line belongs to default the queue and the green line is the root queue which is the common ancestor both of them.
In the benchmark, the jobs were submitted to default queue and a bit later (somewhere around 17:48) the same jobs were submitted to highprio queue. As it is clearly observable for highprio queue the allocated Containers, Memory and VCores were higher and jobs were finished much more faster than those that were submitted to the default queue.</p>

<p>Such kind of dashboard is extremely useful when we are visualizing decisions made by Periscope and check e.g. how the Jobs moved across queues.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png</a>"></p>

<p>To see it in large, please <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">click here</a>.</p>

<p>Since all of the Hadoop metrics are stored in the Elasticsearch, therefore there are a lot of possibility to create different dashboards to that particular parameter of the cluster which is interesting for the operator. The dashboards can be configured on the fly and the metrics are displayed in real-time.</p>

<h2>System resources</h2>

<p>Beside Hadoop metrics, &ldquo;traditional&rdquo; system resource data (cpu, memory, io, network) are gathered with the aid of <a href="https://collectd.org">collectd</a>. This can also run inside the monitoring client container since due to the <a href="https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_example_managing_the_cpu_shares_of_a_container">resource management</a> in Docker the containers can access and gather information about the whole system and a container can even &ldquo;steal&rdquo; the network of other container if you start with: <code>--net=container:id-of-other-container</code> which is very useful if cases when network traffic is monitored.</p>

<p style="text-align:center;"> <img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png</a>"></p>

<h2>Summary</h2>

<p>So far the Hadoop metrics and system resources metrics has been processed, but it is planned to use the information written into the history file (or fetch from History server) an make it also queryable trough Elasticsearch and to be able to provide information about what is happening inside the Jobs.</p>

<p>The development preview of the monitoring server and client is already available on GitHub <a href="https://github.com/sequenceiq/docker-elk">here</a> and <a href="https://github.com/sequenceiq/docker-elk-client">here</a>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
