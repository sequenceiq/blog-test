
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SequenceIQ Blog</title>
  <meta name="author" content="SequenceIQ">

   
  <meta name="description" content="">
  
  <meta name="keywords" content="">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.sequenceiq.com/blog/page/8">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
    <link href="/stylesheets/sequenceiq.css" media="screen, projection" rel="stylesheet" type="text/css">
   <!-- <link href="/stylesheets/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">-->
    <link href="/stylesheets/bootstrap.css" rel='stylesheet' type='text/css'>
  <link href="/stylesheets/bootstrap-theme.css"rel='stylesheet' type='text/css'>
 <!-- <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">-->

  <link href="/blog/atom.xml" rel="alternate" title="SequenceIQ Blog" type="application/atom+xml">
  <script src="/js/jquery.js"></script>
  <script src="/js/bootstrap-collapse.js"></script>
  <script src="/js/modernizr-2.0.js"></script>
  <script src="/js/octopress.js" type="text/javascript"></script>
  <script src="/js/application.js"></script>
  <script src="/js/bootstrap.js"></script>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >

  <!--<div class="jumbotron seq-jumborton">-->
  <!--<div class="container">
      <a href="/">
        <img src="/images/logo.png" >
      </a>
    <h3 class="tagline">
      
        Our view on big data
      
    </h3>
  </div>-->
  <!--  <div class="navbar-static-top" id="company_div">
        <a href="http://sequenceiq.com/">
            <h5 style="margin: 0; margin-right: 5px;padding-bottom: 2px;padding-top: 2px; padding-right: 50px; font-weight: bolder;color: #003140;font-size: 10px;" class="pull-right" >SEQUENCEIQ.COM</h5>
        </a>
    </div>-->
    <header class="navbar navbar-static-top bs-docs-nav" id="top" role="banner" >
        <div class="container">
            <div class="navbar-header">
                <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="http://sequenceiq.com/" class="navbar-brand">
                    <img id="logo" src="http://sequenceiq.com/img/logo@2x.png" width="154" height="39" alt="SequenceIQ">
                </a>
            </div>
            <div class="collapse navbar-collapse" role="navigation" style="/* margin-right: 6.2em; */">
                <ul class="nav navbar-nav navbar-right" id="menu-tag">
                    <li><a href="http://blog.sequenceiq.com/">Blog</a></li>
                    <li><a href="http://blog.sequenceiq.com/archives/">Archives</a></li>
                </ul>

            </div>
        </div>
    </header>
  <div class="container social-jumbotron-container">
      <div class="row">
        
        <div class="col-md-1"><a class="social-link" href="http://github.com/sequenceiq" title="Github Profile"><i class="icon-github-sign social-navbar"></i></a></div>
        
        
        
        <div class="col-md-1"><a class="social-link" href="http://linkedin.com/company/sequenceiq" title="Linkedin Profile"><i class="icon-linkedin-sign social-navbar"></i></a></div>
        
        
        <div class="col-md-1"><a class="social-link" href="http://twitter.com/sequenceiq" title="Twitter Profile"><i class="icon-twitter-sign social-navbar"></i></a></div>
        
        
        
        <div class="col-md-1"><a class="social-link" href="http://facebook.com/sequenceiq" title="Facebook Profile"><i class="icon-facebook-sign social-navbar"></i></a></div>
        
        

        
     </div>
  </div>
<!--</div>-->


  <div id="silent-container">

  </div>
  <div class="container" style="width: 95%;">
      <div class="row" id="main">
              <div class="col-md-9" id="">
                  <div class="">
                   <!-- <div id="content">-->
                      <div class="blog-index">
  
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">04 April 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/docker/"><span class="label label-warning">Docker</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/hadoop-vm/"><span class="label label-warning">Hadoop VM</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/04/04/hadoop-docker-introduction/">Hadoop on Docker introduction</a>
          <span class="badge name-badge">Krisztian Horvath</span>

      </h1>
      <p>In the last few weeks we&rsquo;ve created and published several Docker images (<a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a>, <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a>, <a href="https://github.com/sequenceiq/tez-docker">Tez</a>) to help you to quick-start with Hadoop and the latest innovations using YARN.
While many people have downloaded and started to use these preconfigured images we&rsquo;ve been asked to give a short introduction of what Docker is, and how one can build Docker images. Also during the Hadoop Summit in Amsterdem we have been inquired in particular about running Hadoop on Docker, so this post is our answer for all the requests we received.</p>

<p>Docker is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.</p>

<h2>Installation</h2>

<p>First install Docker with a package manager. On Ubuntu there is an easy way to start with by running a simple curl script which will do it for you:
<code>curl -s https://get.docker.io/ubuntu/ | sudo sh</code>.
Unfortunately Mac, Windows and some Linux distributions cannot natively run Docker (yet). At <a href="http://sequenceiq.com/">SequenceIQ</a> we develop on OSX and run a 3-6 node Hadoop mini cluster on our laptops. To overcome the limitation of running Docker natively
you will have to install <code>boot2docker</code>. It is a Tiny Core Linux made specifically to run Docker containers and weights less than 24MB memory.
Initialize <em>(boot2docker init)</em> and start <em>(boot2docker up)</em> and you can SSH into the VM <em>(boot2docker ssh, pass: tcuser)</em>.</p>

<p>To verify the installation let&rsquo;s test it: <code>docker run ubuntu /bin/echo hello docker</code>. Docker did a bunch of things within seconds:</p>

<ul>
<li>Downloaded the base image from the docker.io index</li>
<li>Created a new LXC container</li>
<li>Allocated a filesystem for it</li>
<li>Mounted a read-write layer</li>
<li>Allocated a network interface</li>
<li>Setup an IP for it, with network address translation</li>
<li>Executed a process inside the container</li>
<li>Captured the output and printed it</li>
</ul>


<p>You can run an interactive shell as well <code>docker run -i -t ubuntu /bin/bash</code> and use this shell as you would use any other shell.</p>

<p>While there are lots of different Docker images available we would like to share how to create your own images.</p>


      
       <a href="/blog/2014/04/04/hadoop-docker-introduction/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/31/mahout-on-tez/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">31 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hortonworks/"><span class="label label-warning">Hortonworks</span></a>
            
            <a href="/blog/categories/mahout/"><span class="label label-warning">Mahout</span></a>
            
            <a href="/blog/categories/tez/"><span class="label label-warning">Tez</span></a>
            
            <a href="/blog/categories/yarn/"><span class="label label-warning">YARN</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/31/mahout-on-tez/">Using Mahout with Tez</a>
          <span class="badge name-badge">Marton Sereg</span>

      </h1>
      <p>At SequenceIQ we are always open to the latest innovations in Hadoop, and trying to find a way to offer a better performance and cluster utilization to our customers. We came in close touch with the <a href="http://hortonworks.com/labs/stinger/">Stinger initiative</a> last year at the Hadoop Summit in Amsterdam &ndash; and ever since we have followed up with the project progress (latest <a href="http://hortonworks.com/blog/apache-tez-0-3-released/">release</a> is 0.3). The project was initiated by Hortonworks with the goal of a 100x performance improvement of Hive.
Although Hive is not part of our product stack (we use other ways for SQL on Hadoop), there is one particular key component of the Stinger initiative which was very interesting to us: <a href="https://github.com/apache/incubator-tez">Apache Tez</a>.</p>

<p><a href="http://incubator.apache.org/projects/tez.html">Apache Tez</a> is a new application framework built on top of Hadoop Yarn that can execute complex directed acyclic graphs (DAGs) of general data processing tasks. In many ways it can be thought of as a more flexible and powerful successor of the map-reduce framework. This was exactly what draw our attention and made us start thinking about using Tez as our runtime for map-reduce jobs.</p>

<h2>Tez and MapReduce</h2>

<p>At SequenceIQ we have chains of map-reduce jobs which are scheduled individually and read the output of previous jobs from HBase or HDFS. Many times our map-reduce job flow can be represented as a map-reduce-reduce pattern, however building complex job chains with the current map-reduce framework is not that easy (nor saves on performance) &ndash; we combined the ChainMapper/ChainReducer and IdentityMapper trying to build MRR like DAG job flows.</p>

<p>In Tez data coming from reducers&#8217; output can be pipelined together and eliminates IO/sync barriers, as no temporary HDFS write is required. Jobs can also be chained and represented as MRR steps with no restriction.
In MapReduce disregarding the data size, the shuffle (internal step between the map and reducer) phase writes the sorted partitions to disk, merge-sorts them and feed into the reducers. All these steps are done <em>in memory</em> with Tez and saves on this I/O heavy step, avoiding unnecessary temporary writes and reads.</p>

<h2>Tez and Mahout</h2>

<p>Part of our system is running machine learning algorithms in batch, using Mahout (we do ML on streaming data using Scala, MLlib and Apache Spark as well). To improve the runtime performance of these Mahout algorithms, and decrease the cluster execution time we started to experiment with combining Tez and Mahout, and rewrite a few Mahout drivers in order to build DAGs of MR jobs (MRR in particular where applicable) and submit the jobs in a Tez runtime on a YARN cluster.</p>


      
       <a href="/blog/2014/03/31/mahout-on-tez/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">24 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hbase/"><span class="label label-warning">HBase</span></a>
            
            <a href="/blog/categories/hortonworks/"><span class="label label-warning">Hortonworks</span></a>
            
            <a href="/blog/categories/hoya/"><span class="label label-warning">Hoya</span></a>
            
            <a href="/blog/categories/yarn/"><span class="label label-warning">YARN</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/24/hoya-at-sequenceiq/">Using Hortonworks Hoya at SequenceIQ</a>
          <span class="badge name-badge">Janos Matyas</span>

      </h1>
      <p>With this blog post we are starting a series of articles where we&rsquo;d like to describe how we use YARN, why it is central to our product stack and why we believe that Hortonworks Hoya will be a determining building block in the Hadoop ecosystem.</p>

<p>While we don&rsquo;t want to get in details about <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>, we&rsquo;d like to briefly explain the advantages of running an application on YARN, and introduce you to <a href="https://github.com/hortonworks/hoya">Hortonworks Hoya</a>.</p>

<p>At SequenceIQ we are building a multi-tenant, scale on demand data platform, with unpredictable batch and streaming workloads.
Before YARN we have tried different cluster management frameworks with Hadoop and managed to have pretty good results with <a href="http://aws.amazon.com/autoscaling/">Amazon EC2 Autoscaling groups</a>. Being true believers in open source and the need to diversify of provisioning Hadoop on different environments we needed to find an open source and &lsquo;standardised&rsquo; solution &ndash; welcome YARN.
(for example we provision Hadoop on <a href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Docker</a>).</p>

<p>YARN separates the processing engine from the resource management &ndash; and acting effectively as an OS for Hadoop.
With YARN, you can now run multiple applications in Hadoop, all sharing a common resource management and improving cluster utilisation (we will release some metrics soon).
YARN also provides the following features out of the box:</p>

<ul>
<li>Multi-tenancy</li>
<li>Management and monitoring</li>
<li>High availability</li>
<li>Security</li>
<li>Failover and recovery</li>
</ul>


<p>All of the above and the effort of the Hadoop community and the wide adoption convinced us to start implementing our platform to run on top of YARN.
During our proof of concepts we went as far as starting all our non Hadoop (and not YARN compatible) applications on YARN &ndash; using <a href="https://github.com/hortonworks/hoya">Hoya</a>.</p>

<p>Hoya was introduced by Hortonworks mid last year &ndash; with the purpose to create Apache HBase clusters on YARN (since than it supports Apache Accumulo as well).
The code evolved pretty fast and now Hoya is a framework/application which allows you to deploy existing distributed applications on YARN &ndash; and benefit all the nice features of YARN.</p>

<p>In order to support different applications Hoya has a plugin provider architecture (supported plugins are in the <em>org.apache.hoya.providers</em> package).
Once a plugin is implemented (pretty straightforward, took us a few days only to understand and build a Flume and Tomcat plugin), the application is started in a YARN container and is monitored and controlled by YARN/Hoya.
The clusters can be started, stopped, frozen and re-sized dynamically &ndash; and in case of container failures Hoya deploys a replacement.</p>

<p>For a better architectural understanding of Hoya please read the following blog <a href="http://hortonworks.com/blog/hoya-hbase-on-yarn-application-architecture/">post</a>.</p>

<p>In this first post we would like to help you to get familiar with the benefits offered by Hoya and start an HBase cluster, re-scale it dynamically, freeze and stop.
First and foremost you will need an installation of Hadoop (2.3), the latest Hoya release (0.13.1) and HBase (0.98).
For your convenience we have put together an automated install script which lets you start with Hoya in a few minutes.</p>

<p>The script is available from our <a href="https://github.com/sequenceiq/hoya-docker/blob/master/hoya-centos-install.sh">GitHub page</a>. Also an official docker.io image is available at <a href="https://index.docker.io/u/sequenceiq/hoya-docker/">hoya-docker</a>, and the Dockerfile can be downloaded from our <a href="https://github.com/sequenceiq/hoya-docker">GitHub</a> page.</p>

<p>Once Hadoop, HBase and Hoya are installed you can create an HBase cluster.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>create-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya create hbase --role master 1 --role worker 1
</span><span class='line'>    --manager localhost:8032
</span><span class='line'>    --filesystem hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz
</span><span class='line'>    --appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf
</span><span class='line'>    --zkhosts localhost
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This will launch a 2 node HBase cluster (1 Master and 1 RegionServer). Now lets increase the number of RegionServers.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>flex-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  <span class="nv">num_of_workers</span><span class="o">=</span><span class="nv">$1</span>
</span><span class='line'>  hoya flex hbase --role worker <span class="nv">$num_of_workers</span> --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>



      
       <a href="/blog/2014/03/24/hoya-at-sequenceiq/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">19 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/docker/"><span class="label label-warning">Docker</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/devops/"><span class="label label-warning">devops</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Hadoop 2.3 with docker</a>
          <span class="badge name-badge">Lajos Papp</span>

      </h1>
      <p>You want to try out hadoop 2.3? Go to the zoo and <a href="http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html">shave a yak</a>.
Or simply just use <a href="https://www.docker.io/">docker</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># start ssh and hdfs</span>
</span><span class='line'><span class="nb">cd</span> <span class="nv">$HADOOP_PREFIX</span>
</span><span class='line'>
</span><span class='line'><span class="c"># run the mapreduce</span>
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar grep input output <span class="s1">&#39;dfs[a-z.]+&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># check the output</span>
</span><span class='line'>bin/hdfs dfs -cat output/*
</span></code></pre></td></tr></table></div></figure>





      
       <a href="/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">14 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/capacity-scheduler/"><span class="label label-warning">Capacity Scheduler</span></a>
            
            <a href="/blog/categories/hd2/"><span class="label label-warning">HD2</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/yarn/"><span class="label label-warning">YARN</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/14/yarn-capacity-scheduler/">YARN Capacity Scheduler</a>
          <span class="badge name-badge">Janos Matyas</span>

      </h1>
      <p>Since the emergence of Hadoop 2 and the YARN based architecture we have a platform where we can run multiple applications (of different types) not constrained only to MapReduce. Different applications or different MapReduce job profiles have different resource needs, however since Hadoop 2.0 is a multi tenant platform the different users could have different access patterns or need for cluster capacity. In Hadoop 2.0 this is achieved through YARN schedulers — to allocate resources to various applications subject to constraints of capacities and queues (for more information on YARN follow this <a href="http://hortonworks.com/hadoop/yarn/">link</a> or feel free to ask us should you have any questions).</p>

<p>In Hadoop 2.0, the scheduler is a pluggable piece of code that lives inside the <em>ResourceManager</em> (the JobTracker in MR1) &ndash; the ultimate authority that arbitrates resources among all the applications in the system. The scheduler in YARN does not perform monitoring or status tracking and offers no guarantees to restart failed tasks — check our sample <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> project to check how monitoring or progress can be tracked.</p>

<p>The Capacity Scheduler was designed to allow significantly higher cluster utilization while still providing predictability for Hadoop workloads, while sharing resources in a predictable and simple manner, using the common notion of <em>job queues</em>.</p>

<p>In our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">example</a> we show you how to use the Capacity Scheduler, configure queues with different priorities, submit MapReduce jobs into these queues, monitor and track the progress of the jobs &ndash; and ultimately see the differences between execution times and throughput of different queue setups.</p>

<p>First, let’s config the Capacity Scheduler (you can use xml, <a href="http://ambari.apache.org/">Apache Ambari</a> or you can configure queues programmatically). In this example we use a simple xml configuration.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.queues<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>default,highPriority,lowPriority<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.highPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>70<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.lowPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>20<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>We have 3 queues, with different queue setups/priorities. Each queue is given a <em>minimum</em> guaranteed percentage of total cluster capacity available &ndash; the total guaranteed capacity must equal 100%. In our example the <em>highPriority</em> queue has 70% of the resources, the <em>lowPriority</em> 20%, and the default queue has the remaining 10%. While it is not highlight in the example above, the Capacity Scheduler provides elastic resource scheduling, which means that if there are idle resources in the cluster, then one queue can take up more of the cluster capacity than was minimally allocated . In our case we could allocate a <em>maximum</em> capacity to the <em>lowPriority</em> queue:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root. lowPriority.maximum-capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>





      
       <a href="/blog/2014/03/14/yarn-capacity-scheduler/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">11 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/data-cleaning/"><span class="label label-warning">Data cleaning</span></a>
            
            <a href="/blog/categories/etl/"><span class="label label-warning">ETL</span></a>
            
            <a href="/blog/categories/hdfs/"><span class="label label-warning">HDFS</span></a>
            
            <a href="/blog/categories/mapreduce/"><span class="label label-warning">MapReduce</span></a>
            
            <a href="/blog/categories/morphline/"><span class="label label-warning">Morphline</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/">Data cleaning with MapReduce and Morphlines</a>
          <span class="badge name-badge">Krisztian Horvath</span>

      </h1>
      <p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>userID   movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
</span><span class='line'>75        3       1       29       10           2006      23          17          16
</span><span class='line'>75        32      4.5     29       10           2006      23          23          44
</span><span class='line'>75        110     4       29       10           2006      23          30          8
</span><span class='line'>75        160     2       29       10           2006      23          16          52
</span><span class='line'>75        163     4       29       10           2006      23          29          30
</span><span class='line'>75        165     4.5     29       10           2006      23          25          15
</span><span class='line'>75        173     3.5     29       10           2006      23          17          37</span></code></pre></td></tr></table></div></figure>


<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  readCSV {
</span><span class='line'>          separator : "\t"
</span><span class='line'>          columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
</span><span class='line'>          ignoreFirstLine : false
</span><span class='line'>          trim : true
</span><span class='line'>          charset : UTF-8
</span><span class='line'>  }
</span><span class='line'>}  
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>  java {
</span><span class='line'>        imports : "import java.util.*;"
</span><span class='line'>        code: """
</span><span class='line'>          record.removeAll("date_hour");
</span><span class='line'>          record.removeAll("date_minute");
</span><span class='line'>          record.removeAll("date_second");
</span><span class='line'>        return child.process(record);
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>



      
       <a href="/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">07 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hdfs/"><span class="label label-warning">HDFS</span></a>
            
            <a href="/blog/categories/hdp2/"><span class="label label-warning">HDP2</span></a>
            
            <a href="/blog/categories/namenode/"><span class="label label-warning">NameNode</span></a>
            
            <a href="/blog/categories/remoteblockreader2/"><span class="label label-warning">RemoteBlockReader2</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/07/read-from-hdfs/">HDFS and java.nio.channels</a>
          <span class="badge name-badge">Janos Matyas</span>

      </h1>
      <p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java BufferedInputStream bufferedInputStream
</span><span class='line'>
</span><span class='line'>/**
</span><span class='line'> * For the sake of readability, try/cacth/finally blocks are removed 
</span><span class='line'> * Don't Say We Didn't Warn You
</span><span class='line'> */
</span><span class='line'>
</span><span class='line'>FileSystem fs = FileSystem.get(configuration);
</span><span class='line'>          Path filePath = getFilePath(dataPath);
</span><span class='line'>
</span><span class='line'>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));
</span><span class='line'>  listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
</span><span class='line'>                      CsvPreference.STANDARD_PREFERENCE);
</span><span class='line'>                     </span></code></pre></td></tr></table></div></figure>


<p>The exception looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ERROR SimpleFeatureSelector:67 - Exception {}
</span><span class='line'>java.lang.IllegalStateException: Must not use direct buffers with InputStream API
</span><span class='line'>  at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
</span><span class='line'>  at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)</span></code></pre></td></tr></table></div></figure>


<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>


      
       <a href="/blog/2014/03/07/read-from-hdfs/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">05 March 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hdp2/"><span class="label label-warning">HDP2</span></a>
            
            <a href="/blog/categories/hortonworks-sandbox/"><span class="label label-warning">Hortonworks sandbox</span></a>
            
            <a href="/blog/categories/socks-proxy/"><span class="label label-warning">SOCKS proxy</span></a>
            
            <a href="/blog/categories/ssl/"><span class="label label-warning">SSL</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/05/access-hdp2-sandbox/">Accessing HDP2 sandbox from the host</a>
          <span class="badge name-badge">Laszlo Puskas</span>

      </h1>
      <p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh root@127.0.0.1 -p 2222 -D 1099
</span></code></pre></td></tr></table></div></figure>





      
       <a href="/blog/2014/03/05/access-hdp2-sandbox/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">28 February 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/data-cleaning/"><span class="label label-warning">Data cleaning</span></a>
            
            <a href="/blog/categories/etl/"><span class="label label-warning">ETL</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/28/etl-and-data-quality/">ETL - producing better quality data</a>
          <span class="badge name-badge">Richard Doktorics</span>

      </h1>
      <p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>


      
       <a href="/blog/2014/02/28/etl-and-data-quality/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">


      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/02/26/vote-for-us/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>

<span class="badge name-badge">26 February 2014</span>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hadoop-summit/"><span class="label label-warning">Hadoop Summit</span></a>
            
            <a href="/blog/categories/vote/"><span class="label label-warning">Vote</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/26/vote-for-us/">Vote for us - 2014 Hadoop Summit San Jose</a>
          <span class="badge name-badge">Janos Matyas</span>

      </h1>
      <p>While we are extremely proud that our abstract came 2nd (out of 107) in the 2014 Hadoop Summit in Amsterdam (see you all there in April 2-3), we will not stop there and our plan is to continue the hard work and we&rsquo;re looking forward to meet you at 2014 Hadoop Summit in San Jose.
We would like to ask for your support by submitting your vote for our session in the largest Hadoop conference in the world.</p>

<p>Please use the following link to vote, or read our abstract below.</p>

<p><a href="http://hadoopsummit.uservoice.com/forums/242807-hadoop-deployment-operations-track/suggestions/5568417-moving-to-hadoop-2-0-yarn-at-sequenceiq">Vote for us</a></p>

<p>Should you have any questions regarding our abstract, the technical solution or implementation detailsm feel free to contact us or
check our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<p><strong>Moving to Hadoop 2.0/YARN at SequenceIQ</strong></p>

<p>A showcase of our efforts to bring all our Hadoop based applications under one common cluster management framework &ndash; YARN.
Our deployment consists of MR2, HBase, Mahout and Hive-all running within one single auto-scaling cluster. We have faced many challenges such as load imbalances, SLA misses, cluster scheduling and VM container deployments &ndash; and would like to share our struggle and solution with the community.
As a startup, cost savings is important for us &ndash; switching to Hadoop 2.0 helped us save significant costs through better utilization of our hardware and cloud VMs. Our decision and investment of moving to YARN has paid off &ndash; and opened up new business and technical opportunities.</p>

      
      
    </div>
  </div>



    </article>
    
  
  <div class="pagination">
    
    <a class="prev" href="/blog/page/9/">&larr; Older</a>
    

    
    <a class="next" href="/blog/page/7/">Newer &rarr;</a>
    
  </div>
</div>


                    <!--</div>-->
                  </div>

              </div>
              <div class="col-md-3">
                 <section>
  <h2 class="blue">Recent Posts</h2>
  <ul id="recent_posts" class="list-group">
    
      <li class="list-group-item">
        <a href="/blog/2015/02/17/openstack-cloudbreak/">OpenStack integration with Cloudbreak</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2015/02/02/cluster-extensions-with-cloudbreak-recipes/">Cluster extensions with Cloudbreak recipes</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2015/01/30/spark-with-cloudbreak/">Install Apache Spark with Cloudbreak</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2015/01/28/cloudbreak-last-beta/">Cloudbreak - new release available</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2015/01/09/spark-1-2-0-docker/">Apache Spark 1.2.0 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2015/01/07/yarn-containers-docker/">Docker containers as Apache YARN containers</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/">New Cloudbreak release - support for HDP 2.2</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/12/12/cloudbreak-got-periscope/">Cloudbreak welcomes Periscope</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/12/04/multinode-ambari-1-7-0/">Multinode cluster with Ambari 1.7.0 - in Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/12/02/hadoop-2-6-0-docker/">Running Hadoop 2.6.0 in Docker containers</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/25/periscope-scale-your-cluster-on-time/">Periscope: time based autoscaling</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/24/hadoop-252-docker/">Apache Hadoop 2.5.2 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/20/yarn-containers-and-docker/">YARN containers as Docker containers in Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/17/datalake-cloudbreak-2/">Building the data lake in the cloud - Part2</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/13/kylin-on-docker/">Extreme OLAP Engine running in Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/10/new-yarn-features-part-1-label-based-scheduling/">New YARN features: Label based scheduling</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/06/securing-cloudbreak-with-oauth2-part-2/">Securing Cloudbreak with OAuth2 - part 2</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/04/yarn-timeline-service-tez/">YARN Timeline Service</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/11/02/spark-on-tez/">Spark on Tez execution context - running in Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/30/cloudbreak-devops/">Deploying a Hadoop Cluster - DevOps way</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/28/datalake-cloudbreak/">Building the data lake in the cloud - Part1</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/23/spark-operations-overview/">Apache Spark RDD operation examples</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/20/cascading-on-tez/">Cascading on Apache Tez</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/17/boot2docker-tls-workaround/">Boot2docker TLS workaround</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/16/using-uaa-as-an-identity-server/">Securing Cloudbreak with OAuth2</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/15/hadoop-metrics/">Real-time adjustments with Hadoop metrics</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/09/ngrok-docker/">Self hosted ngrok server in Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/10/07/hadoop-monitoring/">Real-time monitoring of Hadoop clusters</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/30/hortonworks-partnership/">SequenceIQ Joins Hortonworks Technology Partner Program</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/29/spark-correlation-and-testing/">Apache Spark - create and test jobs</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/26/database-upgrade-process/">Managing database upgrades with Liquibase and Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/25/strata-hadoop-world-2014/">Strata + Hadoop World 2014 Startup Showcase</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/25/euroventures-invests-in-sequenceiq/">Euroventures invests in SequenceIQ</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/24/edit-files-docker/">Edit files in Docker containers</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/23/topn-on-apache-tez/">TopK on Apache Tez</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/19/apache-tez-cluster/">Apache Tez cluster on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/18/custom-image-on-gcc/">Cloudbreak new provider implementation - Part I: Build your custom image</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/17/spark-1-1-0-docker/">Apache Spark 1.1.0 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/15/hadoop-2-5-1-docker/">Apache Hadoop 2.5.1 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/11/apache-drill-docker/">Apache Drill on Docker - query as a service </a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/09/yarn-schedulers-demystified-part-2-fair/">YARN Schedulers demystified - Part 2: Fair</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/05/apache-ambari-1-7-0-ea/">Apache Ambari 1.7.0 early access</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/04/sql-on-hbase-with-apache-phoenix/">SQL on HBase with Apache Phoenix</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/09/01/sla-samples-periscope/">SLA policies for autoscaling Hadoop clusters</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/29/aws-cloudformation-makes-everything-easier/">Infrastructure management with CloudFormation</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/27/announcing-periscope/">Periscope - autoscaling for Hadoop YARN</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/22/spark-submit-in-java/">Submit a Spark job to YARN from code</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/18/hadoop-2-5-0-docker/">Apache Hadoop 2.5.0 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/16/fairplay/">Fair play</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/12/docker-networking/">Docker intercontainer networking explained</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/07/clodubreak-shell/">Create Hadoop clusters in the cloud using a CLI</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/08/04/launch-docker-containers-on-azure/">Launch Docker containers on Azure</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/31/spark-mllib/">Apache Spark - MLlib Introduction</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/25/cloudbreak-technology/">Docker ships Hadoop to the cloud</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/22/schedulers-part-1/">YARN Schedulers demystified - Part 1: Capacity</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak - the Hadoop as a Service API</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/13/groovy-and-java-runtime-bug/">Groovy and Java, the runtime bug</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/09/ambari-configuration-service/">Apache Ambari configuration service</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/">Docker debug with nsenter on boot2docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/07/02/move-applications-between-queues/">Re-prioritize running jobs with YARN schedulers</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/25/hadoop-2-4-0-docker/">Apache Hadoop 2.4.1 on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/23/scalding-correlation-example/">Pearson correlation with Scalding</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">Multi-node Hadoop cluster on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/17/ambari-cluster-on-docker/">Ambari provisioned Hadoop cluster on Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/06/hadoop-summit-slides/">Hadoop Summit 2014 - SequenceIQ slides</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/26/ambari-shell/">Apache Ambari + Spring Shell = Ambari Shell</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/">Building the build environment with Ansible and Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/01/mapreduce-job-profiling-with-R/">Job profiling with R</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/17/apache-phoenix-sneak-peak/">Apache Phoenix (sneak peak)</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/14/mapreduce-with-scalding/">Writing MapReduce jobs in Scala</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/04/hadoop-docker-introduction/">Hadoop on Docker introduction</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/31/mahout-on-tez/">Using Mahout with Tez</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/24/hoya-at-sequenceiq/">Using Hortonworks Hoya at SequenceIQ</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Hadoop 2.3 with docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/14/yarn-capacity-scheduler/">YARN Capacity Scheduler</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/">Data cleaning with MapReduce and Morphlines</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/07/read-from-hdfs/">HDFS and java.nio.channels</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/05/access-hdp2-sandbox/">Accessing HDP2 sandbox from the host</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/28/etl-and-data-quality/">ETL - producing better quality data</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/26/vote-for-us/">Vote for us - 2014 Hadoop Summit San Jose</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/22/custom-flume-source/">Custom Apache Flume source</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/07/hdp2-on-amazon/">Set up HDP2 on Amazon EC2</a>
      </li>
    
  </ul>
</section>

              </div>
      </div>
  </div>
  <div class="row-fluid" id="footer-container">
    <div class="container">
        <footer class="footer-page" role="contentinfo">
            <div class="row">
                <div class="col-md-6">
                    <div class="row">
    
    <div class="col-md-1"><a class="social-link" href="http://github.com/sequenceiq" title="Github Profile"><i class="fa fa-github fa-lg"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://linkedin.com/company/sequenceiq" title="Linkedin Profile"><i class="fa fa-linkedin fa-lg"></i></a></div>
    
    
    <div class="col-md-1"><a class="social-link" href="http://twitter.com/sequenceiq" title="Twitter Profile"><i class="fa fa-twitter fa-lg"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://facebook.com/sequenceiq" title="Facebook Profile"><i class="fa fa-facebook fa-lg"></i></a></div>
    
    

    
    <div class="col-md-1"><a class="social-link" href="http://blog.sequenceiq.com/atom.xml" title="RSS"><i class="fa fa-rss fa-lg"></i></a></div>

</div>

                </div>
                <div class="col-md-5">
                    


<p class="pull-right" >
  <span class="credit">&copy; SequenceIQ Inc. 2014. All rights reserved. </span>
    <br><a href="pp.html" style="color: #508190;">Privacy Policy</a> &nbsp; <a href="tos.html" style="color: #508190;">Terms of Service</a></p>
</p>


                </div>
            </div>
        </footer>
    </div>

  </div>
  

<script type="text/javascript">
      var disqus_shortname = 'sequenceiqblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=625149054184531";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>




  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48528840-1', 'sequenceiq.com');
  ga('send', 'pageview');

</script>
</html>
