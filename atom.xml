<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-11-10T15:10:57+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[New YARN features: Label based scheduling]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/10/new-yarn-features-part-1-label-based-scheduling/"/>
    <updated>2014-11-10T15:14:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/10/new-yarn-features-part-1-label-based-scheduling</id>
    <content type="html"><![CDATA[<p>The release of Hadoop 2.6.0 is upon us thus it&rsquo;s time to highlight a few upcoming features, especially those which we are building/planning to use in our Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and our SLA policy based autoscaling API &ndash; <a href="http://sequenceiq.com/periscope/">Periscope</a>.</p>

<p>Recently we explained how the
<a href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/">CapacityScheduler</a> and the <a href="http://blog.sequenceiq.com/blog/2014/09/09/yarn-schedulers-demystified-part-2-fair/">FairScheduler</a>
works and the upcoming release is about to add a few really interesting functionality to them which you should be aware as they might
change the way we think about resource scheduling. The first one which we are going to discuss is the <code>label based scheduling</code> although it&rsquo;s
not fully finished, yet. You can track its progress here: <a href="https://issues.apache.org/jira/browse/YARN-796">YARN-796</a>.</p>

<h2>Motivation</h2>

<p>Hadoop clusters are usually not fully homogeneous which means that different nodes can have different parameters. For example some nodes
have more memory than the others while others have better cpu&rsquo;s or better network bandwidth. At the moment YARN doesn&rsquo;t have the
ability to segregate nodes in a cluster based on their architectural parameters. Applications which are aware of their resource usages
cannot choose which nodes they want to run their containers on. Labels are about to solve this problem. Administrators will have
the ability to <code>mark</code> the nodes with different labels like: cpu, memory, network, rackA, rackB so applications can specify where they&rsquo;d
like to run.</p>

<h2>Cloud</h2>

<p>Things are different in cloud environments as the composition of the Hadoop clusters are more homogeneous. By the nature of cloud it&rsquo;s
easier and more convenient to request nodes with the exact same capabilities. <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a>
our Hadoop as a service API will address this problem, by giving the ability to the users to specify their needs. Take one example: on AWS
users can launch <code>spot price</code> instances which EC2 can <code>take away any time</code>. Labeling them as <code>spot</code> we can avoid spinning up the
<code>ApplicationMasters</code> on those nodes, thus operate safely and re-launch new containers on different nodes in case it happens.
Furthermore <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> with its autoscaling capabilities will be able
to scale out with nodes that are marked with <code>cpu</code>.</p>

<!--more-->


<h2>Terminology</h2>

<p>To start with let&rsquo;s declare the different types of labels and expressions:</p>

<ul>
<li>node label &ndash; describes a node, multiple labels can be specified</li>
<li>queue label &ndash; determines on which nodes the queue can schedule containers</li>
<li>application label &ndash; defines on which nodes the application want to run its containers</li>
<li>label expression &ndash; logical combination of labels (&amp;&amp;, ||, !) e.g: cpu &amp;&amp; rackA</li>
<li>queue label policy &ndash; resolve conflicts on different queue and application labels</li>
</ul>


<h2>Technical details</h2>

<p>Labeling nodes itself is not enough. Schedulers cannot rely only on application requirements as administrators can configure the queues
to act differently. As we discussed earlier, schedulers are defined in a configuration file where you can specify the queues. Initial labeling
can be done in these files:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.alpha.label<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>cpuheavy||rackA<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The value is a <code>label expression</code> that means applications which are submitted to this queue can run either on nodes labeled as
<code>cpuheavy</code> or <code>rackA</code>. As I said the configuration files can be used as an initial configuration, but changing dynamically queue labels
and node labels is also not a problem as the <code>RMAdminCLI</code> <a href="https://issues.apache.org/jira/browse/YARN-2504">allows</a> that.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'> <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-addToClusterNodeLabels&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[label1,label2,label3] (label splitted by \&quot;,\&quot;)&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;add to cluster node labels &quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-removeFromClusterNodeLabels&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[label1,label2,label3] (label splitted by \&quot;,\&quot;)&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;remove from cluster node labels&quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-replaceLabelsOnNode&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[node1:port,label1,label2 node2:port,label1,label2]&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;replace labels on nodes&quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-directlyAccessNodeLabelStore&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;&quot;</span><span class="o">,</span> <span class="s">&quot;Directly access node label store, &quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot;with this option, all node label related operations&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; will not connect RM. Instead, they will&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; access/modify stored node labels directly.&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; By default, it is false (access via RM).&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; AND PLEASE NOTE: if you configured&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; yarn.node-labels.fs-store.root-dir to a local directory&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; (instead of NFS or HDFS), this option will only work&quot;</span>
</span><span class='line'>                  <span class="o">+</span>
</span><span class='line'>                  <span class="s">&quot; when the command run on the machine where RM is running.&quot;</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Declaring the labels is one thing, but how can the <code>ResourceManager</code> enforce that containers run on nodes where the application wants
it to? Let&rsquo;s think the other way around, how can the <code>ResourceManager</code> enforce that containers do not run on nodes where the
application doesn&rsquo;t want it to? The answer is already part of the RM. The <code>ApplicationMaster</code> can <strong>blacklist</strong> nodes. The
<code>AppSchedulingInfo</code> class can decide based on the <code>ApplicationLabelExpression</code> and the <code>QueueLabelExpression</code> whether the resource is
blacklisted or not.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="kd">synchronized</span> <span class="kd">public</span> <span class="kt">void</span> <span class="nf">updateBlacklist</span><span class="o">(</span>
</span><span class='line'>      <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">blacklistAdditions</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">blacklistRemovals</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Add to blacklist</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">blacklistAdditions</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">blacklist</span><span class="o">.</span><span class="na">addAll</span><span class="o">(</span><span class="n">blacklistAdditions</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Remove from blacklist</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">blacklistRemovals</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">blacklist</span><span class="o">.</span><span class="na">removeAll</span><span class="o">(</span><span class="n">blacklistRemovals</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Okay, we know how to add labels to queues and nodes, but who is going to handle them? A new service will be introduced as part of
the RM called <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java">LabelManager</a>.
Its responsibilities are:</p>

<ul>
<li>load node labels and maintain an internal map of nodes and their labels</li>
<li>dynamically update the label &ndash; node associations (RMAdminCLI, queue configs are reloaded automatically on change)</li>
<li>evaluate label logical expressions for both queue and application</li>
<li>evaluate label expressions against nodes</li>
</ul>


<p>How can applications specify on which nodes they want to run? The <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationSubmissionContext.java#L77">ApplicationSubmissionContext</a>
has been extended with an <code>appLabelExpression</code> and <code>amContainerLabelExpression</code> thus when submitting the job we can specify them. If
we know that our application consumes too much memory and the labels are properly defined it shouldn&rsquo;t be a problem. Providing
an invalid label obviously our application will be rejected. Fairly complex expressions can be given, e.g: (highmemory &amp;&amp; rackA) || master.
Labels can be provided for every <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceRequest.java#L80">ResourceRequest</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="nd">@Public</span>
</span><span class='line'>  <span class="nd">@Stable</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kd">static</span> <span class="n">ResourceRequest</span> <span class="nf">newInstance</span><span class="o">(</span><span class="n">Priority</span> <span class="n">priority</span><span class="o">,</span> <span class="n">String</span> <span class="n">hostName</span><span class="o">,</span>
</span><span class='line'>      <span class="n">Resource</span> <span class="n">capability</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numContainers</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">relaxLocality</span><span class="o">,</span>
</span><span class='line'>      <span class="n">String</span> <span class="n">labelExpression</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">ResourceRequest</span> <span class="n">request</span> <span class="o">=</span> <span class="n">Records</span><span class="o">.</span><span class="na">newRecord</span><span class="o">(</span><span class="n">ResourceRequest</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setPriority</span><span class="o">(</span><span class="n">priority</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setResourceName</span><span class="o">(</span><span class="n">hostName</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setCapability</span><span class="o">(</span><span class="n">capability</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setNumContainers</span><span class="o">(</span><span class="n">numContainers</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setRelaxLocality</span><span class="o">(</span><span class="n">relaxLocality</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setNodeLabelExpression</span><span class="o">(</span><span class="n">labelExpression</span><span class="o">);</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">request</span><span class="o">;</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>It only makes sense when the resource location is <code>ANY</code> or <code>rack</code> and not <code>data local</code>.</p>

<h2>Summary</h2>

<p>We&rsquo;re going to revisit this feature once it completely finished with a concrete example labeling multiple <code>docker</code> containers
and submit stock examples to see how it works in action. Besides labeling there are other important changes about to come to
the schedulers which will change the way we plan cluster capacities. The <code>CapacityScheduler</code> will be fully dynamic to create/remove/resize
queues, move applications on the fly to make room for the <code>AdmissionControl</code>.</p>

<p>Keep up with the latest news with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Securing Cloudbreak with OAuth2 - part 2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/06/securing-cloudbreak-with-oauth2-part-2/"/>
    <updated>2014-11-06T17:00:08+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/06/securing-cloudbreak-with-oauth2-part-2</id>
    <content type="html"><![CDATA[<p>A few weeks ago we&rsquo;ve published a <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">blog post</a> about securing our <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> infrastructure with OAuth2.
We&rsquo;ve discussed how we were setting up and configuring a new UAA OAuth2 identity server with Docker but we haven&rsquo;t detailed how to use this identity server in client applications.
And that&rsquo;s exactly what we&rsquo;ll do now: we&rsquo;ll show some code examples about how to obtain tokens from different clients and how to check these tokens in resource servers.</p>

<p>We&rsquo;re using almost every type of the OAuth2 flows in our infrastructure: Cloudbreak and <a href="http://sequenceiq.com/periscope/">Periscope</a> act as resource servers while <a href="https://github.com/sequenceiq/uluwatu">Uluwatu</a> and <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> for example are clients for these APIs.</p>

<h2>Obtaining an access token</h2>

<p>The main goal of an OAuth2 flow is to obtain an access token for the resource owner that can be used to access a resource server later.
There are multiple common flows depending on the client type, we&rsquo;ll have examples for three of them now: <em>implicit</em>, <em>authorization code</em> and <em>client credentials</em>.
If you&rsquo;re not familiar with the roles and expressions that take part in the OAuth2 flows I suggest to check out some <a href="http://aaronparecki.com/articles/2012/07/29/1/oauth2-simplified">&ldquo;Getting started&rdquo; resources</a> first before going forward with this post.</p>

<h3>Implicit flow</h3>

<p>This is not the most common flow with OAuth2 but it is the most simple one because only one request should be made to the identity server and the token will arrive directly in the response.
Two different types of this flow is supported by UAA. One for browser-based applications and one for those scenarios when there is no browser interaction (e.g.: CLIs).
The common part of these scenarios is that it would be useless to have a client secret because it couldn&rsquo;t be kept as a secret.</p>

<p>We are using the <em>implicit flow with credentials</em> in the <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak Shell</a>.
When using the shell you must provide your SequenceIQ credentials as environment variables and the shell uses those to obtain an access token.
Cloudbreak shell is written in Java but let&rsquo;s see a basic <code>curl</code> example instead &ndash; it does exactly the same as the Java code. (If you&rsquo;re still eager you can check out the code <a href="https://github.com/sequenceiq/cloudbreak-shell/blob/master/src/main/java/com/sequenceiq/cloudbreak/shell/configuration/ShellConfiguration.java#L122">here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -iX POST -H "accept: application/x-www-form-urlencoded"  \
</span><span class='line'> -d 'credentials={"username":"admin","password":"periscope"}' \
</span><span class='line'> "http://localhost:8080/oauth/authorize?response_type=token&client_id=cli&scope.0=openid&redirect_uri=http://cli"</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p><em>notes:</em></p>

<ul>
<li>the <code>response_type=token</code> part tells the identity server to return a token <em>implicitly</em></li>
<li>UAA must be running on <code>localhost:8080</code></li>
<li>there is a registered client in UAA with <code>implicit</code> as <em>authorized_grant_type</em>, <code>cli</code> as <em>client ID</em>, and <code>http://cli</code> as <em>redirect URI</em> (it doesn&rsquo;t need to be a valid URL but has to match the one in the request)</li>
<li>the <code>cli</code> client is configured in UAA as <em>autoapproved</em></li>
<li>a user with <code>admin</code> as username and <code>periscope</code> as password is registered in UAA</li>
</ul>


<p>If you&rsquo;re having a browser-based application and would like to use the implicit flow it is very similar.
The main difference is that you won&rsquo;t have to provide the credentials in the request body but redirect the user to the same URL.
User authentication will happen through a login form and the <code>access_token</code> will appear as a parameter in the redirect URI.
You can simply try this out by opening the same URL in a browser. (Of course the redirect won&rsquo;t be successful with the URI above but the redirect URL will appear in the browser with the access token as a parameter if UAA is properly configured)</p>

<h3>Authorization code flow</h3>

<p>The authorization code flow is the most common one &ndash; it is used mostly by standard web applications that have some server side code besides the frontend.
It&rsquo;s main advantage against the implicit flow is that the token doesn&rsquo;t show up in the browser, only an authorization code is sent back by the identity server to the browser and it will be exchanged for an access token later in some kind of server side code.
We&rsquo;re using the authorization code flow with Uluwatu that&rsquo;s written in <em>node.js</em> so I&rsquo;ll show some <em>node.js</em> examples here</p>

<p>The first part of the authorization code flow is almost exactly the same as the browser-based implicit flow: you&rsquo;ll have to redirect the user to the <code>oauth/authorize</code> endpoint, but with a different <code>response_type</code> (<em>code</em>). The response is a redirect again but instead of the access token an authorization code is sent back as a parameter. You can still try it out in a browser &ndash; of course the redirect won&rsquo;t be successful if there&rsquo;s nothing listening on the redirect URI but the code will appear in the browser.</p>

<p>A somewhat simplified version of the code we&rsquo;re using <a href="https://github.com/sequenceiq/uluwatu/blob/master/server.js#L123">in Uluwatu</a> to start the process looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  var authUrl = uaaAddress
</span><span class='line'>    + 'oauth/authorize?response_type=code'
</span><span class='line'>    + '&client_id=' + clientId
</span><span class='line'>    + '&scope=' + clientScopes
</span><span class='line'>    + '&redirect_uri=' + redirectUri
</span><span class='line'>  if (!req.session.token){
</span><span class='line'>    res.redirect(authUrl)
</span><span class='line'>  } else {
</span><span class='line'>    res.render('index');
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li>the <code>response_type=code</code> part tells the identity server to return an authorization code instead of an access token</li>
<li>UAA must be available on the address specified in the uaaAddress variable</li>
<li>there is a registered client in UAA with <code>authorization_code</code> as <em>authorized_grant_type</em>, and its <em>client ID</em> and <em>redirect URI</em> parameters must be specified in the <code>clientId</code> and <code>redirectUri</code> variables</li>
<li>we&rsquo;re using the <a href="http://expressjs.com">Express</a> web framework for node.js.</li>
</ul>


<p>The second part is about exchanging the authorization code for an access token. To try it out you&rsquo;ll need a web server that will handle the redirect URI. In our case it is done by the <a href="https://github.com/sequenceiq/uluwatu/blob/master/server.js#L100">Uluwatu backend</a> on the <code>/authorize</code> endpoint.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var optionsAuth = { user: clientId, password: clientSecret },
</span><span class='line'>    identityServerClient = new restClient.Client(optionsAuth);
</span><span class='line'>
</span><span class='line'>identityServerClient.registerMethod("retrieveToken", uaaAddress + "oauth/token", "POST");
</span><span class='line'>
</span><span class='line'>app.get('/authorize', function(req, res, next){
</span><span class='line'>  var args = {
</span><span class='line'>    headers: { "Content-Type": "application/x-www-form-urlencoded" },
</span><span class='line'>    data:
</span><span class='line'>      'grant_type=authorization_code'
</span><span class='line'>      + '&redirect_uri=' + redirectUri
</span><span class='line'>      + '&code=' + req.query.code
</span><span class='line'>  }
</span><span class='line'>  identityServerClient.methods.retrieveToken(args, function(data, response){
</span><span class='line'>    req.session.token=data.access_token;
</span><span class='line'>    res.redirect('/');
</span><span class='line'>  });
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li>the <em>POST</em> request must be made to the <code>oauth/token</code> endpoint</li>
<li>the client must authenticate himself by putting the <em>client id</em> and <em>client secret</em> in a standard basic authentication header. The base64 encoding is done by the client library we&rsquo;re using.</li>
<li>the access token arrives in the response body along with a refresh token that can be used to renew the access token when it expires</li>
</ul>


<h3>Client credentials flow</h3>

<p>This one is a bit different from the previous ones because this flow is used when a client would like to access some resources by itself, not on behalf of a user.
A common use case with UAA is when we&rsquo;d like to access the <a href="http://www.simplecloud.info/">SCIM</a> endpoints for describing or registering users.
<a href="https://github.com/sequenceiq/sultans">Sultans</a> is the user management service for the SequenceIQ platform, it uses the client credentials flow in its <a href="https://github.com/sequenceiq/sultans/blob/master/main.js#L216">source</a> to obtain a token that&rsquo;s used for example to register new users later.
Just to keep it simple that&rsquo;s how it looks like in <code>curl</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -u &lt;CLIENT_ID&gt;:&lt;CLIENT_SECRET&gt; -d 'grant_type=client_credentials' http://localhost:8080/oauth/token</span></code></pre></td></tr></table></div></figure>


<p>And that&rsquo;s how a correct sample response looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{"access_token":"&lt;ACCESS_TOKEN&gt;","token_type":"bearer","expires_in":43199,"scope":"password.write scim.write scim.read uaa.resource","jti":"&lt;JTI&gt;"}</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li><p>the scopes of a client is described in the authorities property in the UAA configuration.</p></li>
<li><p>an UAA server must be available on localhost:8080</p></li>
<li><p>a client must be registered in UAA&rsquo;s database with <code>client_credentials</code> as <em>grant_type</em></p></li>
</ul>


<h2>Using the access token to make requests to a resource server</h2>

<p>The <a href="http://self-issued.info/docs/draft-ietf-oauth-v2-bearer.html">Bearer Token Usage part</a> of the OAuth 2.0 specification talks about how to include the access token in a request. According to the specification there are several ways to send the token:</p>

<ul>
<li>in the Authorization request header field</li>
<li>in a form-encoded body parameter</li>
<li>in a URI query parameter</li>
</ul>


<p>Only the first of these (the Authorization header) <em>must</em> be supported by resource servers, the others are only optional.
Here&rsquo;s how the Authorization header should look like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /resource HTTP/1.1
</span><span class='line'>Host: server.example.com
</span><span class='line'>Authorization: Bearer mF_9.B5f-4.1JqM</span></code></pre></td></tr></table></div></figure>


<h2>Checking the token in the resource server</h2>

<p>Now that we are able to deploy and configure an UAA identity server, obtain tokens from it in client applications and send these in resource server requests there&rsquo;s only one thing left: how should we implement the resource server part of our infrastructure to handle the token requests. The OAuth 2.0 specification leaves it up to the implementor but with UAA there is one recommended way, the <code>/check_token</code> <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#oauth2-token-validation-service-post-check_token">endpoint</a>.</p>

<p>The boundaries of resource servers and OAuth2 providers are often blurred and they are in the same application therefore checking a token can be implemented in place by going directly to a token store or decoding the JWT token. If the components are correctly separated this can only be done if the token is encrypted with a shared secret between the provider and the resource server. If it&rsquo;s not the case the resource server must reach out to the identity server to check the validity of the token. In case of UAA this can be achieved with the help of the <code>check_token</code> endpoint.</p>

<p>Our resource servers are implemented in Java and are using Spring. Spring has great support for <a href="http://projects.spring.io/spring-security-oauth/">OAuth</a> but it could feel like <strong>magic</strong> if you don&rsquo;t know what&rsquo;s behind it.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@Configuration
</span><span class='line'>@EnableResourceServer
</span><span class='line'>protected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter {
</span><span class='line'>
</span><span class='line'>    @Bean
</span><span class='line'>    RemoteTokenServices remoteTokenServices() {
</span><span class='line'>        RemoteTokenServices rts = new RemoteTokenServices();
</span><span class='line'>        rts.setClientId(clientId);
</span><span class='line'>        rts.setClientSecret(clientSecret);
</span><span class='line'>        rts.setCheckTokenEndpointUrl(identityServerUrl + "/check_token");
</span><span class='line'>        return rts;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    @Override
</span><span class='line'>    public void configure(ResourceServerSecurityConfigurer resources) throws Exception {
</span><span class='line'>        resources.resourceId("cloudbreak");
</span><span class='line'>        resources.tokenServices(remoteTokenServices());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    @Override
</span><span class='line'>    public void configure(HttpSecurity http) throws Exception {
</span><span class='line'>        http
</span><span class='line'>            .authorizeRequests()
</span><span class='line'>            .antMatchers("/user/blueprints").access("#oauth2.hasScope('cloudbreak.blueprints')")
</span><span class='line'>            .antMatchers("/user/templates").access("#oauth2.hasScope('cloudbreak.templates')");
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>With these few lines you&rsquo;ll have a fully functioning resource server that checks every incoming token on the two endpoints defined in the second <code>configure</code> method.
The <code>EnableResourceServer</code> annotation will include a new <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/authentication/OAuth2AuthenticationProcessingFilter.java#L95">filter</a> in the security filter chain that will use the <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/token/RemoteTokenServices.java#L95">RemoteTokenServices class</a> to make a request to the <code>check_token</code> endpoint. If the response doesn&rsquo;t contain errors it uses a custom <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/authentication/OAuth2AuthenticationManager.java#L77">authentication manager</a> to put the authentication in the Spring authentication context (username will be available through the <code>Principal</code> object). It is also very easy to configure which scopes are needed for specific endpoints &ndash; the expressions used in the configuration are processed by the <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/expression/OAuth2SecurityExpressionMethods.java">OAuth2SecurityExpressionMethods class</a>.</p>

<p>The <code>check_token</code> endpoint in UAA uses basic authentication with the <strong>resource server&rsquo;s</strong> client id and client secret as username and password. That&rsquo;s why the <em>resource server must be configured in UAA as a client</em> as well. The access token must be included in the request body:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>POST /check_token HTTP/1.1
</span><span class='line'>Host: server.example.com
</span><span class='line'>Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==
</span><span class='line'>Content-Type: application/x-www-form-encoded
</span><span class='line'>
</span><span class='line'>token=eyJ0eXAiOiJKV1QiL</span></code></pre></td></tr></table></div></figure>


<p>A successful response will include the decoded parts of the JWT token such as <code>exp</code>, <code>scope</code>, <code>user_name</code> or <code>client_id</code>. See an example <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#oauth2-token-validation-service-post-check_token">here</a>.</p>

<h2>Resources</h2>

<p>I haven&rsquo;t included an example for the password grant type because we are not using it in our projects but you can check it out in the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-Tokens.md#getting-started"><strong>Tokens part</strong></a> of the UAA documentation. If you&rsquo;d like to learn more about UAA, check out its <a href="https://github.com/cloudfoundry/uaa/tree/master/docs">documentation</a> or the source code of our projects in our <a href="https://github.com/sequenceiq/">Github repo</a>. Also feel free to ask anything in the comments section.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Timeline Service]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/04/yarn-timeline-service-tez/"/>
    <updated>2014-11-04T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/04/yarn-timeline-service-tez</id>
    <content type="html"><![CDATA[<p>As you may know from our earlier <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">blogposts</a> we are continuously monitoring and trying to find out what happens inside our YARN clusters, let it be MapReduce jobs, TEZ DAGs, etc&hellip; We&rsquo;ve analyzed our clusters from various aspects so far; now it&rsquo;s the time to take a look at the information provided by the built YARN <code>timeline</code> service.</p>

<p>This post is about how to set up a YARN cluster so that the Timeline Server is available and how to configure applications running in the cluster to report information to it. As an example we&rsquo;ve chosen to run a simple TEZ example. (MapReduce2 also reports to the <code>timeline</code> service)</p>

<p>As a playground we will use a multinode cluster set up on the local machine; alternatively one could do the same on a cluster provisioned with <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a>. Cluster nodes run in Docker containers, YARN / TEZ provisioning and configuration is done with <a href="http://ambari.apache.org/">Apache Ambari</a>.</p>

<h2>Building a multinode cluster</h2>

<p>To build a multinode cluster we use a set of commodity functions that you can install by running the following in a terminal:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari && . .amb</span></code></pre></td></tr></table></div></figure>


<p>(The commodity functions use our docker-ambari image: sequenceiq/ambari:1.6.0)</p>

<!-- more -->


<p>With the functions installed, you can start your cluster by running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-start-cluster 3</span></code></pre></td></tr></table></div></figure>


<p>After a couple of seconds you&rsquo;ll have a running 3-node Ambari cluster.</p>

<h2>Create an Ambari blueprint with the Timeline Server configuration entries</h2>

<p>To provision and configure Hadoop services we use Ambari and Ambari blueprints. Check this <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">blogpost</a> about how to setup an multi-node Hadoop cluster.</p>

<p>To enable the Timeline Server in the cluster, we&rsquo;ve created a blueprint which contains a few overrides of the related configuration properties. A detailed description of the configuration settings for the Timeline Server are described <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">here</a> and <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.5/bk_system-admin-guide/content/ch_application-timeline-server.html">here</a>.</p>

<p>We used <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/timeline-server/blueprints/multi-node-hdfs-yarn-tez-timeline-service.json">this</a> blueprint for the experiment.</p>

<p>Please note, that the blueprint here only contains those configuration entries that differ from the defaults; the assumption is that the other defaults are similar to those described in the documentation. It&rsquo;s always possible to override any of the defaults by adding them to the blueprint, or using the Ambari UI.</p>

<h1>Create the YARN cluster with ambari-shell</h1>

<p>Now it&rsquo;s time to provision our cluster with YARN, TEZ and the Timeline Server enabled. For this let&rsquo;s start the <code>ambari-shell</code>, which, surprisingly runs in a docker container as well.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-shell</span></code></pre></td></tr></table></div></figure>


<p>Following the instructions below you can provision the Timeline Server enabled cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#add the blueprint
</span><span class='line'>blueprint add --url https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/timeline-server/blueprints/multi-node-hdfs-yarn-tez-timeline-service.json
</span><span class='line'>#build the cluster
</span><span class='line'>cluster build --blueprint multi-node-hdfs-yarn-tez-timeline-service
</span><span class='line'>#auto assign nodes
</span><span class='line'>cluster autoAssign
</span><span class='line'>#create the cluster
</span><span class='line'>cluster create</span></code></pre></td></tr></table></div></figure>


<p>After services start, you can reach the Timeline Server on the port 8188 of the ambari host.</p>

<p>There is some more configuration needed for the Timeline Server to work properly, we have to set the following entries to the address where the timeline service is running. You can get the proper value from Ambari &ndash;  the Timeline Server runs where the resource manager is.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yarn.timeline-service.webapp.address
</span><span class='line'>yarn.timeline-service.webapp.https.address
</span><span class='line'>yarn.timeline-service.address</span></code></pre></td></tr></table></div></figure>


<p>This can be done from the Ambari web UI; a restart of the YARN services is needed after the values are saved.</p>

<h1>Check the information in the Timeline Server</h1>

<p>With the cluster and the Timeline Server set up every MR2 and TEZ application starts reporting to the <code>timeline</code> service. Information is made available at <code>http://&lt;ambari-host:8188&gt;</code>. You can also inspect application related information using the command line, as described in the aforementioned documentation.</p>

<p>As we mentioned at the beginning of this post, we choose TEZ to show you how to use the Timeline Server. After running the Tez application in the Timeline Server web UI you will have fine grained generic information about the application, application attempt, containers used by the application, etc.</p>

<p>You can find a few screenshots of the web ui  <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/timeline-server/ts-screenshots/gen-app-logs">here</a>.</p>

<p>If you&rsquo;d like to have a vizualized view of the application   you can use the <em>swimlanes</em> tez tool. Based on the information provided by the Timeline Server this generates images similar to <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/timeline-server/ts-screenshots/swimlanes/topk_topk_stark_application_1415093602516_0016.svg">this</a></p>

<p>If you are curious what framework related information have been logged, you can access the Timeline Server RESTful interface.
You can get very deep details similar to the ones in <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/timeline-server/ts-screenshots/framework-logs">these</a> screenshots</p>

<p>For further details follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark on Tez execution context - running in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez/"/>
    <updated>2014-11-02T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez</id>
    <content type="html"><![CDATA[<p>Last week Hortonworks <a href="http://hortonworks.com/blog/improving-spark-data-pipelines-native-yarn-integration/">announced</a> improvements for running Apache Spark at scale by introducing a new pluggable <code>execution context</code> and has <a href="https://github.com/hortonworks/spark-native-yarn-samples">open sourced</a> it.</p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are always trying to work and offer the latest technology solutions for our clients and help them to choose their favorite technology/option. We are running a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; to be open sourced soon &ndash; with the goal (among many others) to abstract and allow our customers to use their favorite big data runtime: MR2, Spark or Tez. Along this process we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well (all of these containers have made <strong>top</strong> downloads on the official Docker repository). For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>We have always been big fans on Apache Spark &ndash; due to the simplicity of development and at the same time we are big fans of Apache Tez, for reasons we have <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">blogged before</a>.</p>

<p>When the <a href="https://issues.apache.org/jira/browse/SPARK-3561">SPARK-3561</a> has been submitted we were eager to get our hands on the WIP and early implementation &ndash; and this time we&rsquo;d like to help you with a quick ramp-up and easy solution to have a Spark Docker <a href="https://github.com/sequenceiq/docker-spark-native-yarn">container</a> where the <code>execution context</code> has been changed to <a href="http://tez.apache.org/">Apache Tez</a> and everything is preconfigured. The only thing you will need to do is to follow these easy steps.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/spark-native-yarn</span></code></pre></td></tr></table></div></figure>


<p>Once you have pulled the container you are ready to run the image.</p>

<h3>Run the image</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t -h sandbox sequenceiq/spark-native-yarn /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>You have now a fully configured Apache Spark, where the <code>execution context</code> is <a href="http://tez.apache.org/">Apache Tez</a>.</p>

<h3>Test the container</h3>

<p>We have pushed sample data and tests from the <a href="https://github.com/hortonworks/spark-native-yarn-samples">code repository</a> into the Docker container, thus you can start experimenting right away without writing one line of code.</p>

<h4>Calculate PI</h4>

<p>Simplest example to test with is the <code>PI calculation</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-examples-1.1.0.2.1.5.0-702-hadoop2.4.0.2.1.5.0-695.jar</span></code></pre></td></tr></table></div></figure>


<p>You should expect something like the following as the result:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Pi is roughly 3.14668</span></code></pre></td></tr></table></div></figure>


<h4>Run a KMeans example</h4>

<p>Run the <code>KMeans</code> example using the sample dataset.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.KMeans --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/kmeans_data.txt</span></code></pre></td></tr></table></div></figure>


<p>You should expect something like the following as the result:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Finished iteration (delta = 0.0)
</span><span class='line'>Final centers:
</span><span class='line'>DenseVector(0.15000000000000002, 0.15000000000000002, 0.15000000000000002)
</span><span class='line'>DenseVector(9.2, 9.2, 9.2)
</span><span class='line'>DenseVector(0.0, 0.0, 0.0)
</span><span class='line'>DenseVector(9.05, 9.05, 9.05)</span></code></pre></td></tr></table></div></figure>


<h4>Other examples (Join, Partition By, Source count, Word count)</h4>

<p>Join</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.Join --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/join1.txt /sample-data/join2.txt</span></code></pre></td></tr></table></div></figure>


<p>Partition By</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.PartitionBy --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/partitioning.txt</span></code></pre></td></tr></table></div></figure>


<p>Source count</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.SourceCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt</span></code></pre></td></tr></table></div></figure>


<p>Word count</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.WordCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt 1</span></code></pre></td></tr></table></div></figure>


<p>Note that the last argument (1) is the number of <code>reducers</code>.</p>

<h3>Using the Spark Shell</h3>

<p>The Spark shell works out of the box with the new Tez <code>executor context</code>, the only thing you will need to do is run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-shell --master execution-context:org.apache.spark.tez.TezJobExecutionContext</span></code></pre></td></tr></table></div></figure>


<h3>Summary</h3>

<p>Right after the next day that <a href="https://github.com/hortonworks/spark-native-yarn-samples">SPARK-3561</a> has been made available we have started to test at scale using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and run performance tests by using the same Spark jobs developed in Banzai (over 50 individual jobs) using the same input sets, cluster size and Scala code &ndash; but changing the default <code>Spark context</code> to a <code>Tez context</code>. Follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> as we will release these test results and the lessons we have learned in the coming weeks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying a Hadoop Cluster - DevOps way]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops/"/>
    <updated>2014-10-30T12:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops</id>
    <content type="html"><![CDATA[<p>A while ago we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider &ndash; record the process and automate it.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Are we <code>obsessed with automation</code>? Definitely yes &ndash; all the step which are candidates of doing it twice we script or automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/cloudbreak-shell.git
</span><span class='line'>cd cloudbreak-shell
</span><span class='line'>mvn clean package</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p><em>Note: In case you use the hosted version of Cloudbreak you should use the <code>latest-release.sh</code> to get the right version of the CLI.
In case you build your own Cloudbreak from the <code>master</code> branch you should use the <code>latest-snap.sh</code> to get the right version of the CLI.</em></p>

<!--more-->


<h2>Sign in and connect to Cloudbreak</h2>

<p>There are several different ways to use the shell. First of all you&rsquo;ll need a Cloudbreak instance you can connect to. The easiest way is to use our hosted solution &ndash; you can access it with your SequenceIQ credentials. If you don&rsquo;t have an account, you can subscribe <a href="https://accounts.sequenceiq.com/register">here</a>.</p>

<p>Alternatively you can host your own Cloudbreak instance &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. If you&rsquo;re hosting your own Cloudbreak server you can still use your SequenceIQ credentials and our identity server, but then you&rsquo;ll have to configure your Cloudbreak installation with proper client credentials that will be accepted by our identity server. It is currently not supported to register your Cloudbreak application through an API (but it is planned), so contact us if you&rsquo;d like to use this solution.</p>

<p>The third alternative is to deploy our whole stack locally in your organization along with <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">Cloudbreak</a>, our OAuth2 based <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">Identity Server</a>, and our user management application, <a href="https://github.com/sequenceiq/sultans">Sultans</a>.</p>

<p>We suggest to try our hosted solution as in case you have any issues we can always help you. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).</p>

<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
</span><span class='line'>  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar --cmdfile=&lt;FILE&gt; : Cloudbreak executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --cloudbreak.address=&lt;http[s]://HOSTNAME:PORT&gt;  Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].
</span><span class='line'>  --identity.address=&lt;http[s]://HOSTNAME:PORT&gt;    Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].
</span><span class='line'>  --sequenceiq.user=&lt;USER&gt;                        Username of the SequenceIQ user [default: user@sequenceiq.com].
</span><span class='line'>  --sequenceiq.password=&lt;PASSWORD&gt;                Password of the SequenceIQ user [default: password].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  You should specify at least your username and password.</span></code></pre></td></tr></table></div></figure>


<p>Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use <code>hint</code>. You can always use <code>TAB</code> for completion. Note that all commands are <code>context aware</code> &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential select --id #ID of the credential</span></code></pre></td></tr></table></div></figure>


<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template createEC2 --name awstemplate --description aws-template  --region EU_WEST_1 --instanceType M3Xlarge --volumeSize 100 --volumeCount 2</span></code></pre></td></tr></table></div></figure>


<p>You can check whether the template was created successfully by using the <code>template list</code> command. Check the template and select it if you are happy with it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template show --id #ID of the template
</span><span class='line'>
</span><span class='line'>template select --id #ID of the template</span></code></pre></td></tr></table></div></figure>


<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stack create --name “myStackName" --nodeCount 10</span></code></pre></td></tr></table></div></figure>


<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blueprint list
</span><span class='line'>
</span><span class='line'>blueprint select --id #ID of the blueprint</span></code></pre></td></tr></table></div></figure>


<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster create --description “my cluster desc"</span></code></pre></td></tr></table></div></figure>


<p>You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the data lake in the cloud - Part1]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/"/>
    <updated>2014-10-28T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak</id>
    <content type="html"><![CDATA[<p>A while ago we have released our cloud agnostic and Docker container based Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Though the purpose of <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> is to quickly provision arbitrary sized Hadoop clusters in the cloud, the project emerged from bare metal Hadoop provisioning in Docker containers. We were (still doing it) <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">provisioning</a> Hadoop on bare metal using Docker &ndash; and because of this legacy the data was always stored in HDFS. Recently we have been asked to run a proof-of-concept project and build an <code>always on</code> data lake using a cloud <code>object storage</code>.</p>

<p>This post is the first in this series and will cover the connectivity, interoperability and access of data from an <code>object storage</code> and work with that in Hadoop. For this post we choose to create a <code>data lake</code> on Google Cloud Compute and guide you through the steps, run performance tests and understand the benefits/drawbacks of such a setup.</p>

<p><em>Next post will be about sharing the <code>data lake</code> among multiple clusters, using <a href="http://hortonworks.com/hadoop/hcatalog/">Apache HCatalog</a>.</em></p>

<h2>Object storage</h2>

<p>An object storage usually is an <code>internet service</code> to store data in the cloud and comes with a programming interface which allows to retrieve data in a secure, durable and highly-scalable way. The most well know object storage is <strong>Amazon S3</strong> &ndash; with a pretty well covered literature, thus in this example we will use the <strong>Google Cloud Storage</strong>. Google Cloud Storage enables application developers to store their data on Google’s infrastructure with very high reliability, performance and availability, and can be used to distribute large data objects &ndash; like HDFS. In many occasions companies stores their data in objects storages &ndash; but for analytics they would like to access it from their Hadoop cluster. There are several options available:</p>

<ul>
<li>replicate the full dataset in HDFS</li>
<li>read and write from <code>object storage</code> at start/stop of the flow and use HDFS for intermediary data</li>
<li>use a connector such as Google Cloud Storage Connector for Hadoop</li>
</ul>


<h2>Google Cloud Storage Connector for Hadoop</h2>

<p>Using <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">this</a> connector developed by Google allows you to choose <code>Google Cloud Storage</code> as the default file system for Hadoop, and run all your jobs on top (we will come up with MR2 and Spark examples). Using the connector can have several benefits, to name a few:</p>

<ul>
<li>Direct data access &ndash; data is stored in GCS, no need to transfer it into HDFS</li>
<li>HDFS compatibility &ndash; data stored in HDFS can be accessed through the connector</li>
<li>Data accessibility &ndash; data is always accessible, even when the Hadoop cluster is shut down</li>
<li>High data availability &ndash; data is highly available and globally replicated</li>
</ul>


<!-- more -->


<h2>DIY &ndash; build your data lake</h2>

<p>Follow these steps in order to create your own <code>data lake</code>.</p>

<ol>
<li>Create your <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak account</a></li>
<li>Configure your Google Cloud account following these <a href="http://sequenceiq.com/cloudbreak/#accounts">steps</a></li>
<li>Copy the appropriate version of the <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">connector jar</a> to the Hadoop classpath and the key file for auth on every node of the cluster &ndash; use this <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/copyscripts.sh">script</a> to automate the process</li>
<li>Use this Ambari <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/gcs-con-multi-node-hdfs-yarn.blueprint">blueprint</a> to configure the connector</li>
<li>Restart the following services: HDFS, YARN and MapReduce2</li>
</ol>


<p>That&rsquo;s it &ndash; you are done, you can work on your data stored in Google Storage. The next release of <a href="https://github.com/sequenceiq/cloudbreak">Cloudbreak</a> will incorporate and automate these steps for you &ndash; and will use HCatalog to allow you to configure an <code>always on</code> data store using object storages.</p>

<h2>Performance results</h2>

<p>We configured two identical clusters with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> on Google Cloud with the following parameters</p>

<ul>
<li>Number of nodes: 1 master node + 10 slave nodes</li>
<li>2 * 200 GB rotating HDD (where appropriate)</li>
<li>2 Virtual CPU</li>
<li>7.5 GB of memory</li>
</ul>


<p>First of all we run all the Hadoop and the certification tests in order to validate the correctness of the setups. For the tests we have provisioned an <strong>Hortonwork&rsquo;s HDP 2.1</strong> cluster.</p>

<p>After these steps we have switched to the <code>standard</code> performance test &ndash; <strong>TeraGen, TeraSort and TeraValidate</strong>. Please see the results below.</p>

<table>
<thead>
<tr>
<th></th>
<th> File System           </th>
<th> TeraGen </th>
<th> TeraSort </th>
<th> TeraValidate</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> HDFS                  </td>
<td>58mins, 58sec</td>
<td>4hrs, 59mins, 6sec</td>
<td>35mins, 58sec</td>
</tr>
<tr>
<td></td>
<td> Google Cloud Storage  </td>
<td>34mins, 36sec</td>
<td>4hrs, 34mins, 52sec</td>
<td> 29mins, 22sec</td>
</tr>
</tbody>
</table>


<h2>Summary</h2>

<p>There is a pretty good literature about HDFS and object storages and lots of debates around. At <a href="http://sequenceiq.com">SequenceIQ</a> we support both &ndash; and we also believe that each and every company or use case has his own rationale behind choosing one of them. When we came up with the mission statement of simplifying how people work with Hadoop and stated that we&rsquo;d like to give the broadest available options to developers we were pretty serious about.</p>

<p><a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> was designed around being cloud agnostic &ndash; running on Docker and being able to ship those containers to bare metal or any cloud provider with a very easy integration process: currently we support <strong>Amazon AWS, Microsoft Azure and Google Cloud</strong> in public beta and <strong>OpenStack, Digital Ocean</strong> integration in progress/private beta.
As for the supported Hadoop distribution we provision <strong>Apache Hadoop and Hortonworks HDP</strong> in public and <strong>Cloudera CDH</strong> in private beta.</p>

<p>All the private betas will emerge into public programs and will be in GA &ndash; and open sourced under an Apache2 license during Q4.</p>

<p><a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> will be released quite soon &ndash; stay tuned &ndash; will support one API/representation of your big data pipeline and running on multiple runtimes: <strong>MR2, Spark and Tez</strong>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or
<a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark RDD operation examples]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/23/spark-operations-overview/"/>
    <updated>2014-10-23T14:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/23/spark-operations-overview</id>
    <content type="html"><![CDATA[<p>Recently we blogged about how you can write simple Apache Spark jobs and how to test them. Now we&rsquo;d like to introduce all basic RDD operations with easy examples (our goal is to come up with examples as simply as possible). The Spark <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations">documentation</a> explains well what each operations is doing in detail. We made tests for most of the RDD operations with good ol&#8217; <code>TestNG</code>. e.g.:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="nd">@Test</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">testRightOuterJoin</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input1</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="mi">4</span><span class="o">)))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input2</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="sc">&#39;1&#39;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="sc">&#39;2&#39;</span><span class="o">)))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">expectedOutput</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="mi">4</span><span class="o">),</span> <span class="sc">&#39;1&#39;</span><span class="o">)),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="o">(</span><span class="nc">None</span><span class="o">,</span> <span class="sc">&#39;2&#39;</span><span class="o">)))</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">rightOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">Assert</span><span class="o">.</span><span class="n">assertEquals</span><span class="o">(</span><span class="n">output</span><span class="o">.</span><span class="n">collect</span><span class="o">(),</span> <span class="n">expectedOutput</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Sample</h2>

<p>Get the code from our GitHub repository <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> and build the project inside the <code>spark-samples</code> directory. For running the examples, you do not need any pre-installed Hadoop/Spark clusters or anything else.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/sequenceiq/sequenceiq-samples.git
</span><span class='line'><span class="nb">cd </span>sequenceiq-samples/spark-samples/
</span><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>All the other RDD operations are covered in the example (makes no sense listing them here).</p>

<h2>Spark on YARN</h2>

<p>Should you want to run your Spark code on a YARN cluster you have several options.</p>

<ul>
<li>Use our Spark Docker <a href="https://github.com/sequenceiq/docker-spark">container</a></li>
<li>Use our multi-node Hadoop <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">cluster</a></li>
<li>Use <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to provision a YARN cluster on your favorite cloud provider</li>
</ul>


<p>In order to help you get on going with Spark on YARN read our previous blog post about how to <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">submit a Spark</a> job into a cluster.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cascading on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez/"/>
    <updated>2014-10-20T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez</id>
    <content type="html"><![CDATA[<p>In one of our previous <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">posts</a> we showed how to do a TopK using directly the Apache Tez API. In this post we’d like to show how to do a similarly complex algorithm with Cascading &ndash; running on Apache Tez.
At <a href="http://sequenceiq.com">SequenceIQ</a> we use Scalding, Cascading and Spark to write most of our jobs. For a while our big data pipeline API called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> offers a unified API over different runtimes: MR2, Spark and Tez; recently Cascading has announced support for Apache Tez and we’d like to show you that by writing a detailed example.</p>

<h2>Cascading Application &ndash; GroupBy, Each, Every</h2>

<p>Cascading data flows are to be constructed from Source taps (input), Sink taps (output) and Pipes.
At first, we have to setup our properties for the Cascading flow.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="n">AppProps</span><span class="o">.</span><span class="na">appProps</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setJarClass</span><span class="o">(</span><span class="n">Main</span><span class="o">.</span><span class="na">class</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">buildProperties</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">properties</span> <span class="o">=</span> <span class="n">FlowRuntimeProps</span><span class="o">.</span><span class="na">flowRuntimeProps</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setGatherPartitions</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">buildProperties</span><span class="o">(</span><span class="n">properties</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then in order to use Apache Tez, setup the Tez specific <code>Flow Connector</code>.</p>

<!-- more -->




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">FlowConnector</span> <span class="n">flowConnector</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hadoop2TezFlowConnector</span><span class="o">(</span><span class="n">properties</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>After that we do the algorithm part of the flow. We need an input and output which comes as command-line arguments.
We are going to work on CSV files for the sake of simplicity, so we will use the <code>TextDelimited</code> scheme. Also we need to define our input pipe and taps (<code>source/sink</code>).
Suppose that we want to count the occurrences of users and keep them only if they occur more than once. We can compute this with 2 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N205A3">GroupBy</a>, 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Every</a> and 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Each</a> operation.
First, we group by user ids (count them with every operation), then in the second grouping we need to sort on the whole data set (by <code>count</code>) and use the <a href="http://docs.cascading.org/cascading/2.5/javadoc/cascading/operation/Filter.html">Filter</a> operation to remove the unneeded lines. (here we grouping by <code>Fields.NONE</code>, that means we take all data into 1 group, in other words we force to use 1 reducer)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="kd">final</span> <span class="n">String</span> <span class="n">inputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">String</span> <span class="n">outputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">fields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">,</span> <span class="s">&quot;data1&quot;</span><span class="o">,</span> <span class="s">&quot;data2&quot;</span><span class="o">,</span> <span class="s">&quot;data3&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Scheme</span> <span class="n">scheme</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TextDelimited</span><span class="o">(</span><span class="n">fields</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Pipe</span> <span class="n">inPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Pipe</span><span class="o">(</span><span class="s">&quot;inPipe&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Tap</span> <span class="n">inTap</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hfs</span><span class="o">(</span><span class="n">scheme</span><span class="o">,</span> <span class="n">inputPath</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">groupFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Pipe</span> <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GroupBy</span><span class="o">(</span><span class="s">&quot;usersWithCount&quot;</span><span class="o">,</span> <span class="n">inPipe</span><span class="o">,</span> <span class="n">groupFields</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Every</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">groupFields</span><span class="o">,</span> <span class="k">new</span> <span class="n">Count</span><span class="o">(),</span> <span class="n">Fields</span><span class="o">.</span><span class="na">ALL</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GroupBy</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">Fields</span><span class="o">.</span><span class="na">NONE</span><span class="o">,</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="s">&quot;userId&quot;</span><span class="o">),</span> <span class="kc">true</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Each</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">),</span> <span class="k">new</span> <span class="n">RegexFilter</span><span class="o">(</span> <span class="s">&quot;^(?:[2-9]|(?:[1-9][0-9]+))&quot;</span> <span class="o">));</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">resultFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">,</span> <span class="s">&quot;count&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Scheme</span> <span class="n">outputScheme</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TextDelimited</span><span class="o">(</span><span class="n">resultFields</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="n">Tap</span> <span class="n">sinkTap</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hfs</span><span class="o">(</span><span class="n">outputScheme</span><span class="o">,</span> <span class="n">outputPath</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Finally, setup the flow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">FlowDef</span> <span class="n">flowDef</span> <span class="o">=</span> <span class="n">FlowDef</span><span class="o">.</span><span class="na">flowDef</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setName</span><span class="o">(</span><span class="s">&quot;Cascading-TEZ&quot;</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">inPipe</span><span class="o">,</span> <span class="n">inTap</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addTailSink</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">sinkTap</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Flow</span> <span class="n">flow</span> <span class="o">=</span> <span class="n">flowConnector</span><span class="o">.</span><span class="na">connect</span><span class="o">(</span><span class="n">flowDef</span><span class="o">);</span>
</span><span class='line'>    <span class="n">flow</span><span class="o">.</span><span class="na">complete</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see the codebase is a bit simpler than using directly the Apache Tez API, however you loose the low level features of the expressive data flow API. Basically it&rsquo;s up to the personal preference of a developer whether to use and build directly on top of the Tez API or use Cascading (we have our own internal debate among colleagues) &ndash; as Apache Tez improves the performance by multiple times.</p>

<p>Get the code from our GitHub repository <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> and build the project inside the <code>cascading-tez-sample</code> directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>Once your jar is ready upload it onto a Tez cluster and run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>hadoop jar cascading-tez-sample-1.0.jar /input /output
</span></code></pre></td></tr></table></div></figure>


<p>Sample data can be generated in the same way as in <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez">this</a> example.</p>

<p>We have put together a Tez enabled Docker container, you can get it from <a href="https://github.com/sequenceiq/docker-tez">here</a>. Pull the container, and follow the instructions.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Boot2docker TLS workaround]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/17/boot2docker-tls-workaround/"/>
    <updated>2014-10-17T16:46:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/17/boot2docker-tls-workaround</id>
    <content type="html"><![CDATA[<p>Docker 1.3.0 has been released with the invaluable <code>docker exec</code>
<a href="https://docs.docker.com/reference/commandline/cli/#exec">command</a>.</p>

<p>Boot2docker 1.3.0 delivered also some really neat features such
as <a href="https://github.com/boot2docker/boot2docker#virtualbox-guest-additions">Folder sharing</a>
with virtualbox guest additions. So finally OSX users are able to for example serve local html files in a container:
<code>docker run -v /Users/lalyos/webapp/:/usr/share/nginx/html:ro nginx</code></p>

<h2>Issue</h2>

<p>Boot2docker also changed Docker listening from <a href="http://0.0.0.0:2375">http://0.0.0.0:2375</a> to <a href="https://0.0.0.0:2376.">https://0.0.0.0:2376.</a>
While switching on TLS is highly recommended, but its not backward compatible.
Some tools or environments are relying to be able to connect to Docker
via simple http. So after upgrading to 1.3.0 something might be broken.</p>

<h2>Workaround</h2>

<h3>Update 2014-11-01</h3>

<p>Please note that since Boot2docker 1.3.1 is released, you can simply use <code>DOCKER_TLS=no</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ boot2docker ssh -t 'sudo vi /var/lib/boot2docker/profile'</span></code></pre></td></tr></table></div></figure>


<p>If you still want to run the daemon with TLS enabled, but need temporary access on
plain http, read on:</p>

<h3>Socat</h3>

<p>One alternative solution is to start a container which uses <code>socat</code> to proxy the unix
socket file <code>/var/run/docker.sock</code> as a tcp port. It is containerized for you:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$(docker run sequenceiq/socat)</span></code></pre></td></tr></table></div></figure>


<p>Now you can reach Docker the <em>old</em> way:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://192.168.59.103:2375/_ping
</span><span class='line'>OK</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>tl;dr</h2>

<p>The one-liner <code>$(docker run sequenceiq/socat)</code> does the following trick under the hood:</p>

<ul>
<li>it start the <strong>sequenceiq/socat</strong> container without any volume/port or CMD specification</li>
<li>so the default <code>/start</code> bash script takes command</li>
<li>the <code>/start</code> script (<a href="https://github.com/sequenceiq/docker-socat/blob/master/start">see source</a>) recognizes that it didn&rsquo;t started the proper way, so it
<strong>prints the correct docker command</strong> to stdout (see below)</li>
<li>the <code>$( ... )</code> notation executes the correct docker command</li>
</ul>


<p>So for reference, or you want to start it by hand, here is the full command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d \
</span><span class='line'>  -p 2375:2375
</span><span class='line'>  --volume=/var/run/docker.sock:/var/run/docker.sock \
</span><span class='line'>  --name=docker-http \
</span><span class='line'>  sequenceiq/socat</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Securing Cloudbreak with OAuth2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/"/>
    <updated>2014-10-16T14:23:59+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server</id>
    <content type="html"><![CDATA[<p>When we first released <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; our Hadoop as a Service API &ndash; it contained its own authentication and user management layer.
We were using basic authentication for the API calls so every request had to contain a username and a password <em>Base64</em> encoded in the authorization header.
Cloudbreak also had its own user representation and we were binding the resources &ndash; like clusters &ndash; to these users.</p>

<p>This approach had multiple flaws. As we were starting to develop multiple <a href="http://sequenceiq.com/periscope/">projects</a> for our future Platform as a Service solution it became obvious that we will have to refactor our whole user management layer out from Cloudbreak and <strong>share it across our projects</strong>.
Base64 encoding of usernames and passwords is not the best solution either even if transport layer security is working.</p>

<p>What comes into play almost instantly when dealing with these kind of problems is <strong>OAuth2</strong> but it&rsquo;s not as trivial as it first sounds.</p>

<h2>OAuth2</h2>

<p>The main &ldquo;problem&rdquo; with OAuth2 is that its <a href="http://tools.ietf.org/html/rfc6749">specification</a> leaves a lot of decisions up to the implementations.
First of all it does not speak at all about authentication, only authorization. It also leaves out details such as how to manage users, how scopes and tokens look like or how these tokens should be checked by a resource server.</p>

<p>Because of all these reasons implementing a full OAuth2 solution from scratch means a <em>lot</em> of work and reinventing the wheel and of course we didn&rsquo;t want to do that.
Luckily there are a few specifications that complement the original standard and there are also some solutions that implement not only the basic specification but these complementary specifications too.</p>

<p><strong><a href="https://github.com/cloudfoundry/uaa">UAA</a> is CloudFoundry&rsquo;s fully open source identity management service.</strong>
According to the documentation its primary role is as an OAuth2 provider that can issue tokens for client applications, but it can also authenticate users and can manage user accounts and OAuth2 clients through an HTTP API.
To achieve these things it uses these specifications:</p>

<ul>
<li><p><a href="http://openid.net/connect/">OpenID Connect</a> for authentication</p></li>
<li><p><a href="http://www.simplecloud.info/">SCIM</a> for user management</p></li>
<li><p><a href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html">JWT</a> for token representation</p></li>
</ul>


<p>UAA adds a few more things on top of these like client management endpoints which makes it a complete solution as an identity server.
And the best thing is that it is <strong>fully configurable through environment variables and a YAML file</strong>.</p>

<!-- more -->


<h2>Deploying the UAA server</h2>

<p>UAA is a Spring-based Java web application that runs on Tomcat. The first thing we did was to create a <a href="https://registry.hub.docker.com/u/sequenceiq/uaa/">Docker image</a> that deploys a UAA server so it became this easy:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d --link uaa-db:db -e UAA_CONFIG_URL=https://raw.githubusercontent.com/sequenceiq/docker-uaa/master/uaa.yml sequenceiq/uaa:1.8.1</span></code></pre></td></tr></table></div></figure>


<p>There are two ways to provide an UAA configuration file: you can specify an URL like above, or via volume sharing. You can simply put your configuration in the shared directory (<code>/tmp/uaa</code> in the example):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d --name uaa --link uaa-db:db -v /tmp/uaa:/uaa sequenceiq/uaa:1.8.1</span></code></pre></td></tr></table></div></figure>


<p>Linking a database container is only necessary if you&rsquo;re using a configuration like we did <a href="https://github.com/sequenceiq/docker-uaa/blob/master/uaa.yml">in this example</a>.
If you&rsquo;d like to create a postgresql database to try out the sample configuration on your local environment run the following command first that creates a default postgresql database:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d --name uaa-db postgres</span></code></pre></td></tr></table></div></figure>


<h2>UAA Configuration</h2>

<p>The UAA <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/Sysadmin-Guide.rst#configuration">documentation</a> covers the configuration part pretty well, but I&rsquo;ll share my own experiences through some examples.</p>

<h3>Database</h3>

<p>The first part of the configuration file describes where the data will be stored. Environment variables can be used inside the YAML file, they will be expanded when UAA processes the file.
When linking Docker containers the address and the exposed ports of the linked container show up as environment variables in the other container so we can make use of it and provide the postgresql address like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>database:
</span><span class='line'>  driverClassName: org.postgresql.Driver
</span><span class='line'>  url: jdbc:postgresql://${DB_PORT_5432_TCP_ADDR}:${DB_PORT_5432_TCP_PORT}/${DB_ENV_DB:postgres}
</span><span class='line'>  username: ${DB_ENV_USER:postgres}
</span><span class='line'>  password: ${DB_ENV_PASS:}</span></code></pre></td></tr></table></div></figure>


<h3>Default clients</h3>

<p>Default clients and users can also be described in the configuration, but they can be added or modified later through the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#user-account-management-apis">User Management API</a> and the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#client-registration-administration-apis">Client Administration API</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>oauth:
</span><span class='line'>  clients:
</span><span class='line'>    mywebapp:
</span><span class='line'>      id: mywebapp
</span><span class='line'>      secret: changeme
</span><span class='line'>      authorized-grant-types: authorization_code
</span><span class='line'>      scope: myresourceserver.scope1,myresourceserver.scope2,openid,password.write
</span><span class='line'>      authorities: uaa.none
</span><span class='line'>      redirect-uri: http://localhost:3000/authorize</span></code></pre></td></tr></table></div></figure>


<p>Every client should have an <code>authorized-grant-types</code> attribute that tells which OAuth2 flow the client can use to obtain a token. The most common is the <em>authorization code flow</em> that is typically used by web applications. The other possible values are <code>implicit</code>, <code>password</code> and <code>client_credentials</code>.</p>

<p>A <code>secret</code> is not needed for a client with an implicit grant type (implicit flow is typically used from client-side web apps where a secret cannot be used), and of course a <code>redirect-uri</code> is not needed for a client with a <code>client_credentials</code> grant type.</p>

<p>The client can request the <code>scopes</code> described here from the user. These scopes are arbitrary strings that mean something only to the resource server, but UAA uses the base name (anything before the first dot) of the scopes as the <a href="http://tools.ietf.org/html/draft-ietf-oauth-json-web-token-25#section-4.1.3">audience field</a> in the JWT token, so it&rsquo;s recommended to use this kind of naming convention.</p>

<p><code>authorities</code> are basically scopes but only used when the token represents the client itself. It can be useful for example when a client wants to use the SCIM endpoints of the UAA server &ndash; there are built-in scopes for that: <code>scim.read</code> and <code>scim.write</code>.</p>

<p>There are some clients where the user should not be asked to approve a token grant explicitly (e.g.: a command line shell). To surpass the confirmation and accept the permission request automatically, add the following to the <code>oauth</code> section:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>client:
</span><span class='line'>    override: true
</span><span class='line'>    autoapprove:
</span><span class='line'>      - mycommandlineshell</span></code></pre></td></tr></table></div></figure>


<h3>Default users</h3>

<p>The users defined in this section are populated in the database after startup.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scim:
</span><span class='line'>  username_pattern: '[a-z0-9+\-_.@]+'
</span><span class='line'>  users:
</span><span class='line'>    - paul|wombat|paul@test.org|Paul|Smith|openid,myresourceserver.scope1,myresourceserver.scope2</span></code></pre></td></tr></table></div></figure>


<p>This one is quite straightforward. The users are added in the specified format:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>username|password|email|given name|last name|groups</span></code></pre></td></tr></table></div></figure>


<p>The SCIM specification does not speak about roles, scopes or accounts, it only knows <em><a href="http://www.simplecloud.info/specs/draft-scim-core-schema-01.html#group-resource">groups</a></em> besides <em>users</em> where users can be <em>members</em> of a group.
UAA handles scopes as groups, but groups can also be used for other things like adding users to a company account.</p>

<h2>Resources</h2>

<p>If you&rsquo;d like to learn more about UAA, check out its <a href="https://github.com/cloudfoundry/uaa/tree/master/docs">documentation</a> or its <a href="https://github.com/cloudfoundry/uaa/tree/master/samples">sample applications</a>.
We&rsquo;ll also have another blog post soon where I&rsquo;ll show some code examples of the OAuth2 flows we&rsquo;re using with UAA as an identity server so check back in a few days if you&rsquo;re interested.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real-time adjustments with Hadoop metrics]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/15/hadoop-metrics/"/>
    <updated>2014-10-15T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/15/hadoop-metrics</id>
    <content type="html"><![CDATA[<p>To properly understand and to be fully aware of the state of our Hadoop clusters at any time we needed a scalable and flexible solution
to monitor our Hadoop nodes. After investigating the possible solutions we realized that there is no available solution which satisfies
all our needs thus we&rsquo;ve created one and recently just open sourced it, called <a href="http://sequenceiq.com/periscope/#monitoring">Baywatch</a>. Baywatch is capable to capture and visualize real-time changes on Hadoop clusters to understand and make adjustments based on the submitted jobs resource
allocation needs. To plan ahead, viewing and comparing old and new metrics is just as
important as analyzing real-time ones, not to mention that we can find possible weaknesses and defects in our clusters.</p>

<p>To be able to do all of the above mentioned, Baywatch processes the metrics information produced by the Hadoop daemons. This might already sound familiar as we have another project called <a href="http://sequenceiq.com/periscope/">Periscope</a> where you can create alarms and cluster scaling activities making use of the same metrics, but just consuming it in a different way. Combine these 2
components and you&rsquo;ll have a powerful tool and you&rsquo;ll be able to view your cluster&rsquo;s state and based on that <code>make smart decisions</code>
to scale up or down, or simply just set alarms. If you&rsquo;re thrilled to see it in action we are at <a href="http://strataconf.com/stratany2014">Strata</a> and happy to show you a quick demo.</p>

<h2>Hadoop metrics</h2>

<p>So what are these metrics? As I mentioned it earlier metrics are collections of information about Hadoop daemons, e.g:
the <code>ResourceManager</code> produces information about the queue statuses which we use in Periscope when we <code>re-prioritise applications</code>.
To distinguish these metrics they are grouped into named contexts, e.g <code>jvm</code> for java virtual machine metrics, <code>rpc</code> for debugging
rcp calls, but there are many more:</p>

<ul>
<li>yarn</li>
<li>rpcdetailed</li>
<li>metricssystem</li>
<li>mapred</li>
<li>dfs</li>
<li>ugi</li>
</ul>


<p>This <code>Metrics2</code> framework is designed to collect and dispatch per-process metrics to monitor the overall status of the Hadoop system.
In Hadoop related technologies it is a common design to use sources and sinks, just like in this case. Metrics sources are where the
metrics are generated and metrics sinks consume the records generated by the metrics sources. A metrics system would poll the metrics
sources periodically and pass the metrics records to metrics sinks.</p>

<p><img src="http://yuml.me/0faf3738"></p>

<!-- more -->


<p>It is really easy to implement new sinks and sources, just for reference here&rsquo;s the <code>FileSink</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">putMetrics</span><span class="o">(</span><span class="n">MetricsRecord</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="na">timestamp</span><span class="o">());</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="na">context</span><span class="o">());</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">&quot;.&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="na">name</span><span class="o">());</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">separator</span> <span class="o">=</span> <span class="s">&quot;: &quot;</span><span class="o">;</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">MetricsTag</span> <span class="n">tag</span> <span class="o">:</span> <span class="n">record</span><span class="o">.</span><span class="na">tags</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">separator</span><span class="o">);</span>
</span><span class='line'>      <span class="n">separator</span> <span class="o">=</span> <span class="s">&quot;, &quot;</span><span class="o">;</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">tag</span><span class="o">.</span><span class="na">name</span><span class="o">());</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">&quot;=&quot;</span><span class="o">);</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">tag</span><span class="o">.</span><span class="na">value</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">AbstractMetric</span> <span class="n">metric</span> <span class="o">:</span> <span class="n">record</span><span class="o">.</span><span class="na">metrics</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">separator</span><span class="o">);</span>
</span><span class='line'>      <span class="n">separator</span> <span class="o">=</span> <span class="s">&quot;, &quot;</span><span class="o">;</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">metric</span><span class="o">.</span><span class="na">name</span><span class="o">());</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">&quot;=&quot;</span><span class="o">);</span>
</span><span class='line'>      <span class="n">writer</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">metric</span><span class="o">.</span><span class="na">value</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">writer</span><span class="o">.</span><span class="na">println</span><span class="o">();</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>and the <code>FairSchedulerQueueMetrics</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Fair share of memory in MB&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">fairShareMB</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Fair share of CPU in vcores&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">fairShareVCores</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Steady fair share of memory in MB&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">steadyFairShareMB</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Steady fair share of CPU in vcores&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">steadyFairShareVCores</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Minimum share of memory in MB&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">minShareMB</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Minimum share of CPU in vcores&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">minShareVCores</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Maximum share of memory in MB&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">maxShareMB</span><span class="o">;</span>
</span><span class='line'>  <span class="nd">@Metric</span><span class="o">(</span><span class="s">&quot;Maximum share of CPU in vcores&quot;</span><span class="o">)</span> <span class="n">MutableGaugeInt</span> <span class="n">maxShareVCores</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Hadoop comes by default with 3 sinks:</p>

<ul>
<li>FileSink</li>
<li>GraphiteSink</li>
<li>GangliaSink30</li>
</ul>


<h2>Configuration</h2>

<p>The Metrics2 framework uses the <code>PropertiesConfiguration</code> thus the metrics sinks needs to be defined in a configuration-file:
<code>hadoop-metrics2.properties</code>. The declaration should be familiar for those who used <code>Apache Flume</code> before. Here is an example
taken from our <a href="link">Ambari docker image</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">*.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">class</span><span class="o">=</span><span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">metrics2</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">FileSink</span>
</span><span class='line'><span class="n">namenode</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">namenode</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">secondarynamenode</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">secondarynamenode</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">datanode</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">datanode</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">resourcemanager</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">resourcemanager</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">nodemanager</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">nodemanager</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">maptask</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">maptask</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">reducetask</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">reducetask</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span><span class='line'><span class="n">mrappmaster</span><span class="o">.</span><span class="na">sink</span><span class="o">.</span><span class="na">logstash</span><span class="o">.</span><span class="na">filename</span><span class="o">=/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="n">metrics</span><span class="o">/</span><span class="n">mrappmaster</span><span class="o">-</span><span class="n">metrics</span><span class="o">.</span><span class="na">out</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Hadoop WebServices</h3>

<p>There is another way to obtain these metrics without any configuration which the Periscope leverages. It&rsquo;s the <code>WebServices</code> provided
by Hadoop. <code>Jax-RS</code> is used to define the mappings, e.g collect the <code>ResourceManager</code> queue related metrics on mapping <code>/ws/v1/cluster</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="nd">@GET</span>
</span><span class='line'>  <span class="nd">@Path</span><span class="o">(</span><span class="s">&quot;/scheduler&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="nd">@Produces</span><span class="o">({</span> <span class="n">MediaType</span><span class="o">.</span><span class="na">APPLICATION_JSON</span><span class="o">,</span> <span class="n">MediaType</span><span class="o">.</span><span class="na">APPLICATION_XML</span> <span class="o">})</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">SchedulerTypeInfo</span> <span class="nf">getSchedulerInfo</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">init</span><span class="o">();</span>
</span><span class='line'>    <span class="n">ResourceScheduler</span> <span class="n">rs</span> <span class="o">=</span> <span class="n">rm</span><span class="o">.</span><span class="na">getResourceScheduler</span><span class="o">();</span>
</span><span class='line'>    <span class="n">SchedulerInfo</span> <span class="n">sinfo</span><span class="o">;</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">rs</span> <span class="k">instanceof</span> <span class="n">CapacityScheduler</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">CapacityScheduler</span> <span class="n">cs</span> <span class="o">=</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">)</span> <span class="n">rs</span><span class="o">;</span>
</span><span class='line'>      <span class="n">CSQueue</span> <span class="n">root</span> <span class="o">=</span> <span class="n">cs</span><span class="o">.</span><span class="na">getRootQueue</span><span class="o">();</span>
</span><span class='line'>      <span class="n">sinfo</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CapacitySchedulerInfo</span><span class="o">(</span><span class="n">root</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">rs</span> <span class="k">instanceof</span> <span class="n">FairScheduler</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">FairScheduler</span> <span class="n">fs</span> <span class="o">=</span> <span class="o">(</span><span class="n">FairScheduler</span><span class="o">)</span> <span class="n">rs</span><span class="o">;</span>
</span><span class='line'>      <span class="n">sinfo</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FairSchedulerInfo</span><span class="o">(</span><span class="n">fs</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">rs</span> <span class="k">instanceof</span> <span class="n">FifoScheduler</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">sinfo</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FifoSchedulerInfo</span><span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">rm</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">throw</span> <span class="k">new</span> <span class="nf">NotFoundException</span><span class="o">(</span><span class="s">&quot;Unknown scheduler configured&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="k">new</span> <span class="nf">SchedulerTypeInfo</span><span class="o">(</span><span class="n">sinfo</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The only difference is that you&rsquo;re application have to poll now, while the other way you can create forwarders to create push events
just like we did with Baywatch. To which to use depends on you&rsquo;re needs.</p>

<h2>Summary and resources</h2>

<p>As you see using <strong>Baywatch</strong> and <strong>Periscope</strong> you can monitor and scale your cluster based on the configured policies &ndash; all available open sources in our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<ul>
<li><a href="http://sequenceiq.com/periscope/">Periscope</a></li>
<li><a href="https://github.com/sequenceiq/docker-baywatch-client">Baywatch client</a></li>
<li><a href="https://github.com/sequenceiq/docker-baywatch">Baywatch</a></li>
</ul>


<p>For updates follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or
<a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self hosted ngrok server in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/09/ngrok-docker/"/>
    <updated>2014-10-09T08:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/09/ngrok-docker</id>
    <content type="html"><![CDATA[<p><a href="vhttps://ngrok.com/">Ngrok</a> is used for <code>introspected</code> tunnels to localhost.
In integration testing situations is really common that you want to bind some webhooks
to localhost. For example you want AWS SNS deliver messages to your service,
but is not reachable publicly, as it runs only on localhost.</p>

<p>So its really 2 in 1: <strong>local tunnel</strong> and <strong>introspection</strong>. Sometimes you
just want to use its <strong>introspection</strong> feature, to get insight about how a
specific API works. It&rsquo;s like a local <a href="https://www.runscope.com/">runscope</a>.</p>

<p>While you can always use the free hosted version: <a href="https://ngrok.com/">ngrok</a>,
there are reasons to roll you own:</p>

<ul>
<li>Sometimes the free hosted version has <strong>availability</strong> issues,when it gets heavy traffic</li>
<li>Yo don&rsquo;t want your messages/calls go through a public free service, for
<strong>security</strong> concerns</li>
<li>You just want to use its <strong>introspection</strong> feature, and want to avoid the
extra <strong>network</strong> round trip to ngrok.com and back.</li>
</ul>


<p>There is documentation about <a href="https://github.com/inconshreveable/ngrok/blob/master/docs/SELFHOSTING.md">self hosting ngrok</a>
But it include steps, like:</p>

<ul>
<li>create an SSL certificate</li>
<li>build server/client binaries using the cert above</li>
<li>configure, and install it on your server</li>
</ul>


<p>How about using a <strong>single click</strong> version of this? Easy: we have already containerized
this process and made it available in the official Docker
<a href="https://registry.hub.docker.com/u/sequenceiq/ngrokd/">repository</a>.</p>

<!-- more -->


<h2>Running</h2>

<p>To launch the ngrok daemon, you just have to start the <code>sequenceiq/ngrokd</code> Docker image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d --name ngrokd \
</span><span class='line'>  -p 4480:4480 \
</span><span class='line'>  -p 4444:4444 \
</span><span class='line'>  -p 4443:4443 \
</span><span class='line'>  sequenceiq/ngrokd \
</span><span class='line'>    -httpAddr=:4480 \
</span><span class='line'>    -httpsAddr=:4444 \
</span><span class='line'>    -domain=ngrok.mydomain.com</span></code></pre></td></tr></table></div></figure>


<p>It will expose 3 ports:</p>

<ul>
<li><strong>4444</strong>: that is the so called control port, ngrok clients connect there</li>
<li><strong>4480/4443</strong>: this to port is used for the tunneled http/https connections</li>
<li><strong>domain</strong>: this is the domain name, clients need to use to connect to the
server, and the ngrok server will assign <code>&lt;SUBDOMAIN&gt;.ngrok.mydomain.com</code>
addresses to each tunnel.</li>
</ul>


<h2>Install the custom <code>ngrok</code> client</h2>

<p>You remember we have a custom ngrok daemon inside the Docker image. Based on the
<a href="https://gist.github.com/lyoshenka/002b7fbd801d0fd21f2f">self hosting documentation</a></p>

<blockquote><p>Since the client and server executables are paired, you won&rsquo;t be able to use
  any other ngrok to connect to this ngrokd, and vice versa.</p></blockquote>

<p>So we need the <em>paired</em> ngrok client</p>

<h3>OSX</h3>

<p>If you use brew:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>brew cask install https://raw.githubusercontent.com/sequenceiq/docker-ngrokd/master/ngrok.rb</span></code></pre></td></tr></table></div></figure>


<p>otherwise:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -o /usr/local/bin/ngrok https://s3-eu-west-1.amazonaws.com/sequenceiq/ngrok_darwin
</span><span class='line'>chmod +x /usr/local/bin/ngrok</span></code></pre></td></tr></table></div></figure>


<h3>Linux</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -o /usr/local/bin/ngrok https://s3-eu-west-1.amazonaws.com/sequenceiq/ngrok_linux
</span><span class='line'>chmod +x /usr/local/bin/ngrok</span></code></pre></td></tr></table></div></figure>


<p>Please make sure you check the ngrok version:</p>

<p>You should see the <code>1.7.2</code> on client side:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; ngrok version
</span><span class='line'>
</span><span class='line'>1.7.2</span></code></pre></td></tr></table></div></figure>


<h2>Client configuration</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat &gt; ~/.ngrok &lt;&lt;EOF
</span><span class='line'>server_addr: ngrok.mydomain.com:4443
</span><span class='line'>trust_host_root_certs: false
</span><span class='line'>EOF</span></code></pre></td></tr></table></div></figure>


<h2>Hostname workaround</h2>

<p>If you want to run ngrokd internally just use a new entry
in <code>/etc/hosts</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;NGROKD_IP&gt; ngrok.mydomain.com subdomain1.mydomain.com subdomain2.mydomain.com</span></code></pre></td></tr></table></div></figure>


<p>If you want a proper subdomain you need an <code>A record</code> such as:
<code>*.ngrok.mydomain.com 54.72.21.93</code></p>

<h2>Usage</h2>

<p>Starting ngrok is business as usual, just use <code>ngrok &lt;port&gt;</code>.
Pretty much that’s it, you have a self-hosted ngrok server in Docker.
Then you can introspect the tunnel on <a href="http://127.0.0.0:4444">http://127.0.0.0:4444</a></p>

<h2>Sample use case</h2>

<p>To fully understand how Docker works, sometimes it&rsquo;s useful to see how the
Docker client communicates with the Docker server. You can just use ngrok
to introspect the Docker API.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ngrok -subdomain=docker 127.0.0.1:2375</span></code></pre></td></tr></table></div></figure>


<p>then if you want to record API calls you have to configure</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alias docker='docker --host=tcp://docker.ngrok.mydomain.com:4480'</span></code></pre></td></tr></table></div></figure>


<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real-time monitoring of Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/"/>
    <updated>2014-10-07T18:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are running Hadoop clusters on different environments using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and apply <a href="http://sequenceiq.com/periscope/">SLA autoscaling</a> policies on the fly, thus monitoring the cluster is a key operation.</p>

<p>Although various solutions have been created in the software industry for monitoring of activities taking place in a cluster, but it turned out that only a very few of them satisfies most of our needs. When we made the decision about which monitoring libraries and components to integrate in our stack we kept in mind that it needs to be:</p>

<ul>
<li><p><strong>scalable</strong> to be able to efficiently monitor small Hadoop clusters which are consisting of only a few nodes and also clusters which containing thousands of nodes</p></li>
<li><p><strong>flexible</strong> to be able to provide overview about the health of the whole cluster or about the health of individual nodes or even dive deeper into the internals of Hadoop, e.g. shall be able to visualize how our autoscaling solution for Hadoop YARN called  <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> moves running applications between <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a></p></li>
<li><p><strong>extensible</strong> to be able to use the gathered and stored data by extensions written by 3rd parties, e.g. a module which processes the stored (metrics) data and does real-time anomaly detection</p></li>
<li><p><strong>zero-configuration</strong> to be able to plug into any existing Hadoop cluster without additional configuration, component installation</p></li>
</ul>


<p>Based on the requirements above our choice were the followings:</p>

<ul>
<li><a href="http://logstash.net">Logstash</a> for log/metrics enrichment, parsing and transformation</li>
<li><a href="http://www.elasticsearch.org">Elasticsearch</a> for data storage, indexing</li>
<li><a href="http://www.elasticsearch.org/overview/kibana">Kibana</a> for data visualization</li>
</ul>


<h2>High Level Architecture</h2>

<p>In our monitoring solution one of the design goal was to provide a <strong>generic, pluggable and isolated monitoring component</strong> to existing Hadoop deployments. We also wanted to make it non-invasive and avoid adding any monitoring related dependency to our Ambari, Hadoop or other Docker images. For that reason we have packaged the monitoring client component into its own Docker image which can be launched alongside with a Hadoop running in another container or even alongside a Hadoop which is not even containerized.</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png"></p>

<p>In a nutshell the monitoring solution consist of client and server containers. The <code>server</code> contains the Elasticsearch and the Kibana module. The server container is horizontally scalable and it can be clustered trough the clustering capabilities of Elasticsearch.</p>

<p>The <code>client</code> container &ndash; which is deployed on the machine what is needed to be monitored &ndash; contains the Logstash and the collectd module. The Logstash connects to Elasticsearch cluster as client and stores the processed and transformed metrics data there.</p>

<!-- more -->


<h2>Hadoop metrics</h2>

<p>The metrics data what we are collecting and visualizing are provided by <a href="http://blog.cloudera.com/blog/2012/10/what-is-hadoop-metrics2">Hadoop metrics</a>, which is a collection of runtime information that are exposed by all Hadoop daemons. We have configured the Metrics subsystem in that way that it writes the valuable metrics information into the filesystem.</p>

<p>In order to be able to access the metrics data from the monitoring client component &ndash; which is running inside a different Docker container &ndash; we used the capability of <a href="https://docs.docker.com/userguide/dockervolumes">Docker Volumes</a> which basically let&rsquo;s you access a directory within one container form other container or even access directories from host systems.</p>

<p>For example if you would like mount the <code>/var/log</code> from the container named <code>ambari-singlenode</code> under the <code>/amb/log</code> in the monitoring client container then the following sequence of commands needs to be executed:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">EXPOSED_LOG_DIR</span><span class="o">=</span><span class="k">$(</span>docker inspect --format<span class="o">=</span><span class="s1">&#39;{{index .Volumes &quot;/var/log&quot;}}&#39;</span> ambari-singlenode<span class="k">)</span>
</span><span class='line'>docker run -i -t -v <span class="nv">$EXPOSED_LOG_DIR</span>:/amb/log  sequenceiq/baywatch-client /etc/bootstrap.sh -bash
</span></code></pre></td></tr></table></div></figure>


<p>Hundreds of different metrics are gathered form Hadoop metrics subsystem and all data is transformed by Logstash to JSON and stored in ElasticSearch to make it ready for querying or displaying it with Kibana.</p>

<p>The screenshot below has been created from one of our sample dashboard which is displaying Hadoop metrics for a small cluster which was started on my notebook. In this cluster the Yarn&rsquo;s Capacity Scheduler is used and for demonstration purposes I have created a queue called <code>highprio</code> alongside the <code>default</code> queue. I have reduced the capacity of the <code>default</code> queue to 30 and defined the <code>highprio</code> queue with a capacity of 70.
The red line in the screenshot belongs to the <code>highprio</code> queue, the yellow line belongs to the <code>default</code> queue and the green line is the <code>root</code> queue which is the common ancestor both of them.
In the benchmark, the jobs were submitted to the <code>default</code> queue and a bit later (somewhere around 17:48) the same jobs were submitted to the <code>highprio</code> queue. As it is clearly observable for <code>highprio</code> queue the allocated Containers, Memory and VCores were higher and jobs were finished much more faster than those that were submitted to the default queue.</p>

<p>Such kind of dashboard is extremely useful when we are visualizing decisions made by <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> and check e.g. how the applications are moved across <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a>, or additional nodes are added or removed dynamically from the cluster.</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png"></p>

<p>To see it in large, please <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">click here</a>.</p>

<p>Since all of the Hadoop metrics are stored in the Elasticsearch, therefore there are a lot of possibilities to create different dashboards using that particular parameter of the cluster which is interesting for the operator. The dashboards can be configured on the fly and the metrics are displayed in real-time.</p>

<h2>System resources</h2>

<p>Beside Hadoop metrics, &ldquo;traditional&rdquo; system resource data (cpu, memory, io, network) are gathered with the aid of <a href="https://collectd.org">collectd</a>. This can also run inside the monitoring client container since due to the <a href="https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_example_managing_the_cpu_shares_of_a_container">resource management</a> in Docker the containers can access and gather information about the whole system and a container can even &ldquo;steal&rdquo; the network of other container if you start with: <code>--net=container:id-of-other-container</code> which is very useful if cases when network traffic is monitored.</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png"></p>

<h2>Summary</h2>

<p>So far the Hadoop metrics and system resource metrics have been processed, but it is planned to use the information written into the history file (or fetch from History server) and make it also <code>queryable</code> trough Elasticsearch to be able to provide information about what is happening inside the jobs.</p>

<p>The development preview of the monitoring server and client is already available on our GitHub <a href="https://github.com/sequenceiq/docker-elk">here</a> and <a href="https://github.com/sequenceiq/docker-elk-client">here</a>. In the next release this will be part of <strong>Periscope</strong> and <strong>Cloudbreak</strong>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SequenceIQ Joins Hortonworks Technology Partner Program]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/30/hortonworks-partnership/"/>
    <updated>2014-09-30T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/30/hortonworks-partnership</id>
    <content type="html"><![CDATA[<p>Integration of Hortonworks Data Platform with Cloudbreak enables Hadoop to run in Docker containers &ndash; shipped to the cloud.</p>

<p><strong>SAN FRANCISCO, September 30, 2014</strong> — <a href="http://sequenceiq.com/">SequenceIQ ,Inc.</a> today announced that it has joined the <a href="http://hortonworks.com/partners/become-a-partner/">Hortonworks® Technology Partner Program</a>. <a href="http://hortonworks.com/">Hortonworks</a> is the leading contributor to and provider of Apache™ Hadoop®. SequenceIQ will integrate <a href="http://hortonworks.com/hdp/">Hortonworks Data Platform</a> (HDP) with Cloudbreak to enable a cloud agnostic, autoscaling and <a href="https://www.docker.com/">Docker</a> container based provisioning of HDP.</p>

<p>By joining the Hortonworks Technology Partner program, SequenceIQ will work to enable and accelerate the deployment of a modern data architecture, integrating with the Hortonworks Data Platform—the industry’s only 100 percent open source Hadoop distribution, explicitly architected, built, and tested for enterprise-grade deployments.</p>

<p>SequenceIQ’s technology enables organizations to have a DevOps friendly way to ease and automate provisioning of on-demand Hadoop clusters and services using their favorite cloud provider. With the integration of HDP, users can now leverage all the available features of a 100 percent open source commercial Hadoop distribution.</p>

<p>“SequenceIQ and Hortonworks share a common goal of making the provisioning of Apache Hadoop clusters easier on different cloud and Docker container based environments,” said Janos Matyas, chief technology officer of SequenceIQ. “Cloudbreak and HDP help enterprises to minimize the cost of their Hadoop deployments, and create on-demand autoscaling Hadoop clusters.”</p>

<!-- more -->


<p>Hortonworks Data Platform was built by the core architects, builders and operators of Apache Hadoop and includes all of the necessary components to manage a cluster at scale and uncover business insights from existing and new big data sources. With a <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>-based architecture, HDP is an important component of the modern data architecture, helping organizations mine, process and analyze large batches of unstructured data sets to make more informed business decisions.</p>

<p>“Hortonworks is dedicated to expanding and empowering the Apache Hadoop ecosystem, accelerating innovation and adoption of 100 percent open source enterprise Hadoop,” said John Kreisa, vice president of strategic marketing at Hortonworks. “We welcome SequenceIQ to the Hortonworks Technology Partner Program and look forward to working with them to strengthen Hadoop’s role as the foundation of the next-generation data architecture.”</p>

<p>About SequenceIQ</p>

<p>SequenceIQ is an innovative big data startup with the mission statement to simplify and automate provisioning of on-demand Hadoop clusters running on different environments. Leaders in “containerizing” Hadoop, SequenceIQ is the first company who provisions and runs the full Hadoop stack and components on Docker containers. The company has further plans to create the industry’s first cloud agnostic, autoscaling and use case driven Platform as a Service API, and break the common significant barriers of entry that exist with most big data projects through a series of As-a-Service API offerings.
For more information follow up with us on @SequenceIQ and <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>.</p>

<p><a href="&#x6d;&#97;&#105;&#108;&#116;&#111;&#x3a;&#105;&#110;&#102;&#x6f;&#x40;&#x73;&#101;&#x71;&#x75;&#101;&#110;&#x63;&#x65;&#105;&#x71;&#x2e;&#99;&#111;&#109;">&#x69;&#x6e;&#102;&#111;&#64;&#x73;&#x65;&#x71;&#x75;&#x65;&#x6e;&#99;&#x65;&#x69;&#113;&#x2e;&#x63;&#x6f;&#x6d;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark - create and test jobs]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/29/spark-correlation-and-testing/"/>
    <updated>2014-09-29T13:42:24+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/29/spark-correlation-and-testing</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use different runtimes (MR2, Spark, Tez) when submitting jobs from <a href="http://docs.banzai.apiary.io/reference">Banzai</a> to a YARN clusters.
Some of these jobs are quite simple (filtering, sorting, projection etc.), but most of them can be complicated or not so oblivious at first (e.g.: complex machine learning algorithms).
From Banzai’s perspective/looking from outside a YARN cluster, what only matters is the input and the output dataset &ndash; as we have abstracted all the pipeline steps &ndash;  so testing of this steps properly is a must.
In this post we’d like to show such an example that &ndash; a correlation job on vectors with <a href="https://spark.apache.org/">Apache Spark</a> and how we test it.</p>

<h2>Correlation example (on vectors) with Apache Spark</h2>

<p>Suppose that we have an input dataset (CSV file for the sake of simplicity of the sample code) and we want to reveal the dependency between all of the columns. (all data is vectorized, if not you will have to vectorize your data first).
If we want to build a <code>testable</code> job, we have to focus only on the algorithm part. Our goal here is to work only on the Resilient Distributed Dataset and take the context creation outside of the job.
This way you cab run and create your <code>SparkContext</code>locally and substitute an HDFS data source (or something else) with simple objects.</p>

<p>Interface: (output: vector index pairs with their correlation coefficient)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">abstract</span> <span class="k">class</span> <span class="nc">CorrelationJob</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">d2d</span><span class="o">(</span><span class="n">d</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="nc">DecimalFormat</span><span class="o">(</span><span class="s">&quot;#.######&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="n">d</span><span class="o">).</span><span class="n">toDouble</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Below we show you how a Pearson correlation job implementation looks like with RDD functions. First, you need to gather all combinations of the vector indices and count the size of the dataset.
After that, the only thing what you need is to compute the <a href="http://www.statisticshowto.com/what-is-the-correlation-coefficient-formula/">correlation coefficient</a> on all column combinations (based on the square, dot product and sum of the fields per line). It takes 1 map and 1 reduce operation per pairs. (<code>iterative</code> &ndash;> typical example where you need to use Spark instead of MR2)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">numbersInput</span> <span class="k">=</span> <span class="n">input</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">combinedFields</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0</span> <span class="n">to</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">first</span><span class="o">().</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="o">).</span><span class="n">combinations</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">size</span> <span class="k">=</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">res</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="n">field</span> <span class="k">&lt;-</span> <span class="n">combinedFields</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">col1Index</span> <span class="k">=</span> <span class="n">field</span><span class="o">.</span><span class="n">head</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">col2Index</span> <span class="k">=</span> <span class="n">field</span><span class="o">.</span><span class="n">last</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">tempData</span> <span class="k">=</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="n">arr</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">data1</span> <span class="k">=</span> <span class="n">arr</span><span class="o">(</span><span class="n">col1Index</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">data2</span> <span class="k">=</span> <span class="n">arr</span><span class="o">(</span><span class="n">col2Index</span><span class="o">)</span>
</span><span class='line'>        <span class="o">(</span><span class="n">data1</span><span class="o">,</span> <span class="n">data2</span><span class="o">,</span> <span class="n">data1</span> <span class="o">*</span> <span class="n">data2</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">data1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">data2</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
</span><span class='line'>      <span class="o">}}</span>
</span><span class='line'>      <span class="k">val</span> <span class="o">(</span><span class="n">sum1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sum2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="n">tempData</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="o">((</span><span class="n">a1</span><span class="o">,</span> <span class="n">a2</span><span class="o">,</span> <span class="n">aDot</span><span class="o">,</span> <span class="n">a1sq</span><span class="o">,</span> <span class="n">a2sq</span><span class="o">),</span> <span class="o">(</span><span class="n">b1</span><span class="o">,</span> <span class="n">b2</span><span class="o">,</span> <span class="n">bDot</span><span class="o">,</span> <span class="n">b1sq</span><span class="o">,</span> <span class="n">b2sq</span><span class="o">))</span> <span class="k">=&gt;</span>
</span><span class='line'>          <span class="o">(</span><span class="n">a1</span> <span class="o">+</span> <span class="n">b1</span><span class="o">,</span> <span class="n">a2</span> <span class="o">+</span> <span class="n">b2</span><span class="o">,</span> <span class="n">aDot</span> <span class="o">+</span> <span class="n">bDot</span><span class="o">,</span> <span class="n">a1sq</span> <span class="o">+</span> <span class="n">b1sq</span><span class="o">,</span> <span class="n">a2sq</span> <span class="o">+</span> <span class="n">b2sq</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">corr</span> <span class="k">=</span> <span class="n">pearsonCorr</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">sq1</span><span class="o">,</span> <span class="n">sq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span>
</span><span class='line'>      <span class="o">(</span><span class="n">col1Index</span><span class="o">,</span> <span class="n">col2Index</span><span class="o">,</span> <span class="n">d2d</span><span class="o">(</span><span class="n">corr</span><span class="o">))</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">res</span><span class="o">.</span><span class="n">toArray</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// correlation formula</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">pearsonCorr</span><span class="o">(</span><span class="n">size</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">sum1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sum2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">numerator</span> <span class="k">=</span> <span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">dotProduct</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">sum1</span> <span class="o">*</span> <span class="n">sum2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">denominator</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq1</span> <span class="o">-</span> <span class="n">sum1</span> <span class="o">*</span> <span class="n">sum1</span><span class="o">)</span> <span class="o">*</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq2</span> <span class="o">-</span> <span class="n">sum2</span> <span class="o">*</span> <span class="n">sum2</span><span class="o">)</span>
</span><span class='line'>    <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>MLlib Statistics</h2>

<p>By the way <a href="https://spark.apache.org/releases/spark-release-1-1-0.html">Spark Release 1.1.0</a> contains an algorithm for correlation computation, thus we now show you how to use that instead of the above one.
With <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/stat/Statistics.scala">Statistics</a> you can produce a correlation matrix from vectors. For obtaining the correlation coefficient pairs, we just need to get the upper triangular matrix without diagonal. It looks much simpler, isn&rsquo;t is?</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">vectors</span> <span class="k">=</span> <span class="n">input</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">corr</span><span class="k">:</span> <span class="kt">Matrix</span> <span class="o">=</span> <span class="nc">Statistics</span><span class="o">.</span><span class="n">corr</span><span class="o">(</span><span class="n">vectors</span><span class="o">,</span> <span class="s">&quot;pearson&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">num</span> <span class="k">=</span> <span class="n">corr</span><span class="o">.</span><span class="n">numRows</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// upper triangular matrix without diagonal</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">res</span> <span class="k">=</span> <span class="k">for</span> <span class="o">((</span><span class="n">x</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">corr</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">zipWithIndex</span> <span class="k">if</span> <span class="o">(</span><span class="n">i</span> <span class="o">/</span> <span class="n">num</span><span class="o">)</span> <span class="o">&lt;</span> <span class="n">i</span> <span class="o">%</span> <span class="n">num</span> <span class="o">)</span>
</span><span class='line'>    <span class="k">yield</span> <span class="o">((</span><span class="n">i</span> <span class="o">/</span> <span class="n">num</span><span class="o">),</span> <span class="o">(</span><span class="n">i</span> <span class="o">%</span> <span class="n">num</span><span class="o">),</span> <span class="n">d2d</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">res</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>For testing Spark jobs we use the Specs2 framework. We do not want to start a Spark context before every test case, so we just start/end it before/after steps.
In order to run Spark locally set master to &ldquo;local&rdquo;. In our example (for demonstration purposes) we do not turn off Spark logging (or set to warn level) but it is recommended.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">abstract</span> <span class="k">class</span> <span class="nc">SparkJobSpec</span> <span class="k">extends</span> <span class="nc">SpecificationWithJUnit</span> <span class="k">with</span> <span class="nc">BeforeAfterExample</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="nd">@transient</span> <span class="k">var</span> <span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span> <span class="o">=</span> <span class="k">_</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">beforeAll</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.driver.port&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.hostPort&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">afterAll</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">sc</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">sc</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
</span><span class='line'>      <span class="n">sc</span> <span class="k">=</span> <span class="kc">null</span>
</span><span class='line'>      <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.driver.port&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.hostPort&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">fs</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="nc">Fragments</span><span class="o">)</span> <span class="k">=</span> <span class="nc">Step</span><span class="o">(</span><span class="n">beforeAll</span><span class="o">)</span> <span class="o">^</span> <span class="k">super</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">fs</span><span class="o">)</span> <span class="o">^</span> <span class="nc">Step</span><span class="o">(</span><span class="n">afterAll</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In our test specification we check that both correlation implementations are correct or not.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="nd">@RunWith</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">JUnitRunner</span><span class="o">])</span>
</span><span class='line'><span class="k">class</span> <span class="nc">CorrelationJobTest</span> <span class="k">extends</span> <span class="nc">SparkJobSpec</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="s">&quot;Spark Correlation implementations&quot;</span> <span class="n">should</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&quot;1,2,9,5&quot;</span><span class="o">,</span> <span class="s">&quot;2,7,5,6&quot;</span><span class="o">,</span><span class="s">&quot;4,5,3,4&quot;</span><span class="o">,</span><span class="s">&quot;6,7,5,6&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">correctOutput</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mf">0.620299</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="o">-</span><span class="mf">0.627215</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.11776</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="o">-</span><span class="mf">0.70069</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.552532</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.207514</span><span class="o">)</span>
</span><span class='line'>      <span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="s">&quot;case 1 : return with correct output (custom spark correlation)&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">customCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CustomCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">customCorr</span> <span class="n">must_==</span> <span class="n">correctOutput</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="s">&quot;case 2: return with correct output (stats spark correlation)&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">statCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatsCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">statCorr</span> <span class="n">must_==</span> <span class="n">correctOutput</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="s">&quot;case 3: equal to each other&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">statCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatsCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">customCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CustomCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">statCorr</span> <span class="n">must_==</span> <span class="n">customCorr</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To build and test the project use this command from our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> spark-correlation directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>You can run this correlation example in our free Docker based Apache Spark container as well. (with <a href="https://github.com/apache/spark/blob/master/bin/spark-submit">spark-submit</a> script). You can get the Spark container from the official <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Docker registry</a> or from our <a href="https://github.com/sequenceiq/docker-spark">GitHub</a> repository. The source code is available at <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-correlation">SequenceIQ&rsquo;s GitHub repository</a>.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing database upgrades with Liquibase and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process/"/>
    <updated>2014-09-26T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are <em>crazy</em> about automating everything &ndash; let it be the provisioning of a thousand nodes Hadoop
cluster using <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> or a simple database change.
We apply the same automated CI/CD process to all our projects, including plain old RDBMS schema changes &hellip; yes, though we are a <em>big data</em> technology
company sometimes we do use JPA as well.</p>

<p>As applications evolve their underlying data model change. New functionalities often need data model changes and the initial design
needs to be adapted to the ever changing demands. These changes usually are of two types : structural changes
(e.g.: addition/removal of tables, columns, constraints etc &hellip;)
and migration of the existing data to the new version of the datamodel.
As the data model gets more and more complex  &ndash; this will happen in-spite of trying to keep it as simple as possible &ndash;
the complexity of these tasks grow proportionally. This happens here at SequenceIQ too; the post is about how we address some of these problems.</p>

<!-- more -->


<h2>Directives</h2>

<ul>
<li>We need a process to follow each time such changes arise</li>
<li>Use appropriate tools that do the job (instead of reinventing the wheel)</li>
<li>Make the process <strong>automated</strong> as much as possible</li>
</ul>


<h3>The process</h3>

<p>The process &ndash; as the common sense suggests &ndash; could be split in the following steps:</p>

<ul>
<li>start from the initial version of the database (the version in production)</li>
<li>perform changes required by the new version of the application</li>
<li>capture and store differences between the two versions of the database</li>
<li>(automatically) apply changes to the initial database version</li>
<li>perform tests</li>
<li>apply changes to production</li>
</ul>


<h3>Tools</h3>

<ul>
<li>Dockerized (PostgreSQL) database</li>
<li>Dockerized Liquibase</li>
<li>Jenkins</li>
</ul>


<h3>Implementation</h3>

<h4>Start from the initial version of the database</h4>

<p>To start with, you need a database that&rsquo;s (structurally) identical with the production one. There are several ways to achieve this;we use Postgres and try to
keep it simple, so here&rsquo;s what we do:</p>

<ul>
<li>we always have a QA database which is identical with the production (obliviously the data is not the same)</li>
<li>we make a copy of the <em>data</em> folder of the postgres installation into an arbitrary location on the host</li>
<li>we pass it as a volume to a Docker container running Postgres</li>
</ul>


<p>We run the following command each time we need a fresh database:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d \
</span><span class='line'>  --name $CONTAINER_NAME \
</span><span class='line'>  -v /$WORKING_DIR/data:/data \
</span><span class='line'>  -p 5432:5432 \
</span><span class='line'>  -e "USER=$DB_USER" \
</span><span class='line'>  -e "PASS=$DB_PASS" \
</span><span class='line'>  -e "DB=$DB_NAME" \
</span><span class='line'>paintedfox/postgresql</span></code></pre></td></tr></table></div></figure>


<p>where the passed in variables are the following:</p>

<ul>
<li>CONTAINER_NAME &ndash; the name of the database Docker container</li>
<li>DB_USER &ndash; the database user name</li>
<li>DB_PASS &ndash; the database password</li>
<li>DB_NAME &ndash; the database schema</li>
</ul>


<p>We have a running database now (in less than a minute) &ndash; same as the prod; we can connect to it with a client on your localhost, port 5432 with the given username/password.</p>

<h4>Perform changes required by the new version of the application</h4>

<p>As expected, this is the most challenging part in the process: changes need to be implemented and also
captured so that they can be applied any time (preferably in an <strong>automated</strong> way)
As we&rsquo;re using JPA (with Hibernate as JPA provider) incremental structural changes are executed with the
SchemaUpdate tool. This can be done during the application startup or using <em>ant</em> or <em>maven</em>.
As we continuously test the application we choose to start the application configured to update the database based on the
changed data model (annotations). Alternatively we could regenerate the whole schema. (See the SchemaUpdate tool documentation:
<a href="http://docs.jboss.org/hibernate/core/3.6/reference/en-US/html/toolsetguide.html">here</a>)</p>

<p>At this point we have a database that aligns with the new version of the application. Please note here, that only <code>incremental</code> changes have been applied to
the database till now, meaning that for example new fields have been added,
 but old/deprecated fields haven&rsquo;t been deleted.</p>

<p>Other type of scripts need to be implemented manually:</p>

<ul>
<li><p>changes that couldn&rsquo;t be performed by the SchemaUpdate tool, such as cleanup (SQL) scripts. This being done, differences till this phase
can be automatically generated by running the Liquibase Docker container &ndash; see the next section. Differences are stored under version control,
in form of <em>Liquibase changelogs</em></p></li>
<li><p>data migration scripts, that adapt the existing data to the new structure. Think of cases
when for example a field becomes a new entity and instead of a value you need to store a reference to the new entity. We store these kind of scripts along
with the generated diff files under version control in form of <em>Liquibase changelogs</em></p></li>
</ul>


<h4>Dockerized Liquibase</h4>

<p>Speaking of tools, we found that <a href="http://www.liquibase.org/index.html">Liquibase</a> addresses many of our requirements, such as</p>

<ul>
<li>track database changes (changes being stored in VCS)</li>
<li>automatically generate diffs between two versions of the database</li>
<li>automatically update a database based on changelogs</li>
</ul>


<p>We have created a docker image with a <em>liquibase</em> installation. You can find the project <a href="https://github.com/sequenceiq/docker-liquibase">here</a></p>

<p>The image can be built locally with the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build -t sequenceiq/docker-liquibase .</span></code></pre></td></tr></table></div></figure>


<p>or from the project root, or pulled from the Docker repository:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/docker-liquibase</span></code></pre></td></tr></table></div></figure>


<p>Containers built from this image can be used to perform <em>liquibase</em> operations on any host.
This saves us a lot of time by having the installation and configuration shipped and helps us to automate most of the tasks.
You can use the container for performing liquibase tasks manually in a terminal, or you can start the container to
automatically perform specific tasks (and quit eventually). To start the container linked to the previously started database
container and perform manual operations, run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it \
</span><span class='line'>--name $LIQUIBASE_CONTAINER \
</span><span class='line'>--link $DB_CONTAINER:db \
</span><span class='line'>--entrypoint /bin/bash \
</span><span class='line'>-v /$LIQUIBASE_CHANGELOGS:/changelogs \
</span><span class='line'>$LIQUIBASE_DOCKER_IMAGE \
</span><span class='line'>/bin/bash</span></code></pre></td></tr></table></div></figure>


<p>See the description of the variables:</p>

<ul>
<li>LIQUIBASE_CONTAINER the name of the Liquibase Docker container</li>
<li>DB_CONTAINER the name of the database container the Liquibase container is to be linked to</li>
<li>LIQUIBASE_CHANGELOGS the folder holding the liquibase changelogs (Liquibase will read and write here)</li>
<li>LIQUIBASE_DOCKER_IMAGE the name of the dockerized Liquibase Docker image</li>
</ul>


<p>Some of the Liquibase tasks can be scripted. We scripted the diff generation and changelog application. Liquibase offers more advanced features too.</p>

<h4>Testing</h4>

<p>We write tests that can be run automatically to check the process. Each <code>changeset</code>, especially those related to data migration / transformation is covered.</p>

<h4>Apply liquibase changelogs to the production database</h4>

<p>After the application is tested upon applying the database changes &ndash; that ensures that changelogs are correct, it&rsquo;s easy to set up a <code>jenkins</code> job that:</p>

<ul>
<li>checks out the proper version of changelogs</li>
<li>starts a docker container linked to the (production) database and applies changelogs</li>
</ul>


<p>Obviously this step needs to be designed carefully and adapted to the custom application deployment needs.</p>

<h4>Notes</h4>

<ul>
<li>Thanks to Docker, all the work described here can be done offline (setting up the infrastructure can be done fast, on a dev&rsquo;s machine for example)</li>
<li>Liquibase changelogs can be executed individually or in group (by including subsets of changelogs) thus during the whole process we can adopt a
step-by step approach</li>
</ul>


<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Strata + Hadoop World 2014 Startup Showcase]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/25/strata-hadoop-world-2014/"/>
    <updated>2014-09-25T07:42:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/25/strata-hadoop-world-2014</id>
    <content type="html"><![CDATA[<p><a href="http://sequenceiq.com">SequenceIQ</a> is happy to announce that it has been selected among the top 10 leading big data startups by a team of investors, entrepreneurs, and industry analysts and will present live at the Startup Showcase at <strong>Strata + Hadoop World 2014, New York.</strong></p>

<p>We will have a space in the showcase to win over the judges, and pitch our company and innovative technology to the thousands of developers, founders, executives, investors and researchers that attend <a href="http://strataconf.com/stratany2014">Strata</a>.</p>

<p>Come and watch our showcase on Wednesday, October 15 or catch up with us during the conference (@sequenceiq).</p>

<p>Stay in touch with us through <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Euroventures invests in SequenceIQ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/25/euroventures-invests-in-sequenceiq/"/>
    <updated>2014-09-25T07:42:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/25/euroventures-invests-in-sequenceiq</id>
    <content type="html"><![CDATA[<p>We are happy to announce that Euroventures has invested in SequenceIQ. Euroventure&rsquo;s investment is intended to foster the company&rsquo;s growth in terms of staffing, technology stack and expansion into the U.S. market.</p>

<p>Euroventures&#8217; press release is available <a href="http://www.euroventures.hu/news.php?id=296">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Edit files in Docker containers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker/"/>
    <updated>2014-09-24T13:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker</id>
    <content type="html"><![CDATA[<p>I wish I get 1 dollar each time I install vi in a docker container &hellip; I wanted
an easier way to edit files in a running docker container. First of all try to
<strong>avoid</strong> editing files at all, as it is against the container philosophy
(see the last paragraph).</p>

<p>But if you have a valid reason, here comes the how-to.</p>

<h2>Why Zedapp</h2>

<p>Most of the time I use either vi or <a href="https://atom.io/">Atom</a>, but a few months
ago I stumbled upon <a href="http://zedapp.org/">Zedapp</a> an opinionated editor. It aims to
reduce cognitive load while editing, by simplifying things, like deliberately
not using tabs.</p>

<p>It stands out with its <strong>first-class support of remote editing</strong> let it be a
remote server, or even directly editing github repositories.</p>

<p>Zedapp just reached version 1.0 and if you like it, consider help Zef Hemmel
at <a href="https://gratipay.com/zefhemel/">gratipay</a>, who was brave enough to quit his
regular job, and work full time on an open-source project!</p>

<h2>Install Zedapp</h2>

<p>You can use zedapp as a <em>chrome plugin</em> or a <em>standalone</em> app. Downloads are
available at: <a href="http://zedapp.org/download/">zedapp.org</a>. I recommend to
go for the <strong>standalone</strong> version.</p>

<!-- more -->


<h2>Install zedrem</h2>

<p>For <a href="http://zedapp.org/features/edit-remote-files/">remote editing</a>,
you need zedrem, a small process serving files to be edited in Zedapp.
Zedrem is packaged into a docker image:
<a href="https://github.com/sequenceiq/docker-zedapp">sequenceiq/zedapp</a></p>

<p>To start a local zed-server, and zed-client in the target container, there
is a helper script: <strong>zed</strong></p>

<p>To install the docker image and the shell script run this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm \
</span><span class='line'>  -v /usr/local/bin:/target \
</span><span class='line'>  -v /usr/local/bin/docker:/usr/local/bin/docker \
</span><span class='line'>  -v /var/run/docker.sock:/var/run/docker.sock \
</span><span class='line'>  sequenceiq/zedapp</span></code></pre></td></tr></table></div></figure>


<p>Actually there is only a single binary called <strong>zedrem</strong>, i just use the
terminology: zed-server and zed-client to
distinguish when you use it with or without the <code>--server</code> option.</p>

<p>Now you are ready to start a zedrem session, to edit files in Zedapp which are
inside of a Docker container&rsquo;s directory.</p>

<h2>Start a zedrem session</h2>

<p>To start a zedrem client in a container</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed &lt;container&gt; &lt;directory&gt;</span></code></pre></td></tr></table></div></figure>


<p>This will:
&ndash; start a <code>zedrem-server</code> if not already running.
&ndash; copy and start <code>zedrem-client</code> into the selected container and print out
  the zedrem session&rsquo;s <strong>remote-url</strong>.</p>

<p>Navigate to the project list window by: <code>Command-Shift-O</code>/<code>Ctrl-Shift-O</code>. Select
 <code>Remote Folder</code>, enter the remote-url into <code>Zedrem URL</code> input field and press
 <code>Open</code>.</p>

<p>Thats all enyoj! All the following paragraphs are for the curious only.</p>

<h2>Boot2docker helper function</h2>

<p>The <code>Install zedrem</code> step should have detected that you are using Boot2docker,
and instructed you to create a helper function, but in case you missed it, or
for reference:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed() { boot2docker ssh "sudo zed $@" ; }</span></code></pre></td></tr></table></div></figure>


<p>This is needed as the helper script called <code>zed</code> is installed inside of
Boot2docker, so you need the ususal <code>boot2docker ssh</code> workaround.</p>

<p>after that you can issue directly on OSX:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed &lt;container&gt; &lt;directory&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Local zedrem server.</h2>

<p>By default when you want to use Zedapp for remote editing, you need two
other components then Zedapp:</p>

<ul>
<li><strong>zedrem-server</strong> Zedapp gets file content, and sends edit commands
on webservices protocol. It maintains sessions with zedrem-clients.</li>
<li><strong>zedrem-client</strong> a small process serving files from a specified directory.</li>
</ul>


<p>When you use zedrem-client via the official server, all the editing commands/content
travel around the blobe:</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-remote.png" alt="zedrem remote" /></p>

<p>Compare it with the dockerized local server setup, which is more quick and
secure:
<img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-docker.png" alt="zedrem docker" /></p>

<h2>nsenter</h2>

<p>You might wonder about the step: <strong>copy zedrem into the container</strong>. How is it
possible? Docker&rsquo;s <code>cp</code> command only supportts the other direction: copy from a
container into a local dir.</p>

<p>There is an <a href="https://github.com/docker/docker/issues/5846">open issue</a>, so it
will be fixed soon, but meanwhile you can use nsenter to the rescue. Jérôme
Petazzoni prepared us a canned <a href="https://github.com/jpetazzo/nsenter">nsenter</a>
with the helper script: <code>docker-enter</code>. We can missuse docker-enter to copy
a file from local fs into the container by:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat local-file | docker-enter $container sh -c 'cat&gt;/zedrem'</span></code></pre></td></tr></table></div></figure>


<p>btw: <code>docker exec</code> is already merged into the master branch, so it will replace
nsenter completely.</p>

<h2>Don&rsquo;t do this at all</h2>

<p>Let&rsquo;s make it clear, that most of the time you don&rsquo;t need this.
First of all editing files in a container, other than development or debug
considered bad practice.</p>

<p>You find yourself editing nginx config files? Don&rsquo;t do it, use the great generic
<a href="https://github.com/progrium/nginx-appliance">nginx appliance</a> from Jeff Lindsay.</p>

<p>If you <strong>really</strong> need to edit files in a docker container, just use volumes.</p>

<p>This process comes handy if you&rsquo;ve already started a container, and the file in
question doesn&rsquo;t sits on a volume. [30]</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TopK on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/"/>
    <updated>2014-09-23T17:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez</id>
    <content type="html"><![CDATA[<p>The Apache Tez community draw attention last week with their latest release <a href="http://tez.apache.org/releases/0.5.0/release-notes.txt">0.5.0</a>
of the application framework. At SequenceIQ we always try to find and provide the best solutions to our customers and share the experience we gain by
being involved in many open source Apache projects. We are always looking for the latest innovations, and try to apply them to our projects.
For a while we have been working hard on a new project called
<a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> which we&rsquo;ll open source in the near future. One handy feature of the projects is the ability to run the same pipes on <code>MR2</code>, <code>Spark</code> and <code>Tez</code> &ndash; your choice.
In the next couple of posts we&rsquo;ll compare these runtimes using different jobs and as the first example to implement we chose TopK. Before going into
details let&rsquo;s revisit what Apache Tez is made of.</p>

<h2>Apache Tez key concepts</h2>

<ul>
<li>One of the most important feature is that there is no heavy deployment phase which otherwise could go wrong in many ways &ndash; probably sounds familiar
for most of us. There is a nice <a href="http://tez.apache.org/install.html">install guide</a> on the project&rsquo;s page which you can follow, but basically
you have to copy a bunch of jars to HDFS and you&rsquo;re almost good to go.</li>
<li>Multiple versions of Tez can be used at the same time which solves a common problem, the rolling upgrades.</li>
<li>Distributed data processing jobs typically look like <code>DAGs</code> (directed acyclic graphs) and Tez relies on this concept to define your jobs.
DAGs are made from <code>Vertices</code> and <code>Edges</code>. Vertices in the graph represent data transformations while edges represent the data movement
from producers to consumers. The DAG itself defines the structure of the data processing and the relationship between producers and consumers.</li>
</ul>


<p>Tez provides faster execution and higher predictability because:</p>

<ul>
<li>Eliminates replicated write barriers between successive computations</li>
<li>Eliminates the job launch overhead</li>
<li>Eliminates the extra stage of map reads in every workflow job</li>
<li>Provides better locality</li>
<li>Capable to re-use containers which reduces the scheduling time and speeds up incredibly the short running tasks</li>
<li>Can share in-memory data across tasks</li>
<li>Can run multiple DAGs in one session</li>
<li>The core engine can be customized (vertex manager, DAG scheduler, task scheduler)</li>
<li>Provides an event mechanism to communicate between tasks (data movement events to inform consumers by the data location)</li>
</ul>


<p>If you&rsquo;d like to try Tez on a fully functional multi-node cluster we put together an Ambari based Docker image. Click
<a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a> for details.</p>

<!-- more -->


<h2>TopK</h2>

<p>The goal is to find the top K elements of a dataset. In this example&rsquo;s case is a simple CSV and we&rsquo;re looking for the top elements in a given column.
In order to do that we need to <code>group</code> and <code>sort</code> them to <code>take</code> the K elements. The implementation can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a> repository. The important part starts
with the <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L136">DAG creation</a>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">DataSourceDescriptor</span> <span class="n">dataSource</span> <span class="o">=</span> <span class="n">MRInput</span><span class="o">.</span><span class="na">createConfigBuilder</span><span class="o">(</span><span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="n">tezConf</span><span class="o">),</span>
</span><span class='line'>            <span class="n">TextInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">inputPath</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">DataSinkDescriptor</span> <span class="n">dataSink</span> <span class="o">=</span> <span class="n">MROutput</span><span class="o">.</span><span class="na">createConfigBuilder</span><span class="o">(</span><span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="n">tezConf</span><span class="o">),</span>
</span><span class='line'>            <span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">outputPath</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">tokenizerVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">TokenProcessor</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">columnIndex</span><span class="o">))))</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addDataSource</span><span class="o">(</span><span class="n">INPUT</span><span class="o">,</span> <span class="n">dataSource</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="kt">int</span> <span class="n">topK</span> <span class="o">=</span> <span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">top</span><span class="o">);</span>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">sumVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">SUM</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">SumProcessor</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">topK</span><span class="o">)),</span> <span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">numPartitions</span><span class="o">));</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">writerVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">WRITER</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">Writer</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">topK</span><span class="o">)),</span> <span class="mi">1</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addDataSink</span><span class="o">(</span><span class="n">OUTPUT</span><span class="o">,</span> <span class="n">dataSink</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">OrderedPartitionedKVEdgeConfig</span> <span class="n">tokenSumEdge</span> <span class="o">=</span> <span class="n">OrderedPartitionedKVEdgeConfig</span>
</span><span class='line'>            <span class="o">.</span><span class="na">newBuilder</span><span class="o">(</span><span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span> <span class="n">IntWritable</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span>
</span><span class='line'>                    <span class="n">HashPartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">()).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">UnorderedKVEdgeConfig</span> <span class="n">sumWriterEdge</span> <span class="o">=</span> <span class="n">UnorderedKVEdgeConfig</span>
</span><span class='line'>            <span class="o">.</span><span class="na">newBuilder</span><span class="o">(</span><span class="n">IntWritable</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">()).</span><span class="na">build</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>First of all we define a <code>DataSourceDescriptor</code> which represents our dataset and a <code>DataSinkDescriptor</code> where we&rsquo;ll
write the results to. As you can see there are plenty of utility classes to help you define your DAGs. Now that the input and output is
ready let&rsquo;s define our <code>Vertices</code>. You&rsquo;ll see the actual data transformation is really easy as Hadoop will take care of the heavy
lifting. The first Vertex is a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L198">tokenizer</a>
which does nothing more than splitting the rows of the CSV and emit a record with the selected column as the key and <code>1</code> as the value.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">initialize</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>  <span class="kt">byte</span><span class="o">[]</span> <span class="n">payload</span> <span class="o">=</span> <span class="n">getContext</span><span class="o">().</span><span class="na">getUserPayload</span><span class="o">().</span><span class="na">deepCopyAsArray</span><span class="o">();</span>
</span><span class='line'>  <span class="n">ByteArrayInputStream</span> <span class="n">bis</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ByteArrayInputStream</span><span class="o">(</span><span class="n">payload</span><span class="o">);</span>
</span><span class='line'>  <span class="n">DataInputStream</span> <span class="n">dis</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DataInputStream</span><span class="o">(</span><span class="n">bis</span><span class="o">);</span>
</span><span class='line'>  <span class="n">columnIndex</span> <span class="o">=</span> <span class="n">dis</span><span class="o">.</span><span class="na">readInt</span><span class="o">();</span>
</span><span class='line'>  <span class="n">dis</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'>  <span class="n">bis</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">KeyValueWriter</span> <span class="n">kvWriter</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValueWriter</span><span class="o">)</span> <span class="n">getOutputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">WRITER</span><span class="o">).</span><span class="na">getWriter</span><span class="o">();</span>
</span><span class='line'>  <span class="n">KeyValuesReader</span> <span class="n">kvReader</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValuesReader</span><span class="o">)</span> <span class="n">getInputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">).</span><span class="na">getReader</span><span class="o">();</span>
</span><span class='line'>  <span class="k">while</span> <span class="o">(</span><span class="n">kvReader</span><span class="o">.</span><span class="na">next</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">Text</span> <span class="n">word</span> <span class="o">=</span> <span class="o">(</span><span class="n">Text</span><span class="o">)</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentKey</span><span class="o">();</span>
</span><span class='line'>    <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">Object</span> <span class="n">value</span> <span class="o">:</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentValues</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">sum</span> <span class="o">+=</span> <span class="o">((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">value</span><span class="o">).</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">kvWriter</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="n">sum</span><span class="o">),</span> <span class="n">word</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The interesting part here is the <code>initialize</code> method which reads the <code>UserPayload</code> to find out in which column we&rsquo;re looking for
the top K elements. What happens after the first Vertex is that Hadoop will <code>group</code> the records by key, so we&rsquo;ll have all the keys
with a bunch of 1s. In the next Vertex we
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L243">sum</a>
these values so we&rsquo;ll have all the words in the given column counted. We could emit all the values to make Hadoop sort them for us,
but we can optimize this process by reducing the data we need to send over the network. And how we are going to do that? If we
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L340">maintain</a>
a <code>local</code> top K of the data which the sum tasks process and emit only these values all we have left in the last Vertex is to
select the top K results from a much smaller data set. Another important improvement here is that we can use the <code>UnorderedKVEdgeConfig</code>
to avoid sorting the data at the task output and the merge sort at the next tasks input. We used the <code>OrderedPartitionedKVEdgeConfig</code>
between the first 2 Vertices to demonstrate the different edges and as partition can be a huge asset.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">Preconditions</span><span class="o">.</span><span class="na">checkArgument</span><span class="o">(</span><span class="n">getInputs</span><span class="o">().</span><span class="na">size</span><span class="o">()</span> <span class="o">==</span> <span class="mi">1</span><span class="o">);</span>
</span><span class='line'>        <span class="n">Preconditions</span><span class="o">.</span><span class="na">checkArgument</span><span class="o">(</span><span class="n">getOutputs</span><span class="o">().</span><span class="na">size</span><span class="o">()</span> <span class="o">==</span> <span class="mi">1</span><span class="o">);</span>
</span><span class='line'>        <span class="n">KeyValueWriter</span> <span class="n">kvWriter</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValueWriter</span><span class="o">)</span> <span class="n">getOutputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">WRITER</span><span class="o">).</span><span class="na">getWriter</span><span class="o">();</span>
</span><span class='line'>        <span class="n">KeyValuesReader</span> <span class="n">kvReader</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValuesReader</span><span class="o">)</span> <span class="n">getInputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">).</span><span class="na">getReader</span><span class="o">();</span>
</span><span class='line'>        <span class="k">while</span> <span class="o">(</span><span class="n">kvReader</span><span class="o">.</span><span class="na">next</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">Text</span> <span class="n">currentWord</span> <span class="o">=</span> <span class="o">(</span><span class="n">Text</span><span class="o">)</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentKey</span><span class="o">();</span>
</span><span class='line'>            <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">Object</span> <span class="n">value</span> <span class="o">:</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentValues</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">sum</span> <span class="o">+=</span> <span class="o">((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">value</span><span class="o">).</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="n">localTop</span><span class="o">.</span><span class="na">store</span><span class="o">(</span><span class="n">sum</span><span class="o">,</span> <span class="n">currentWord</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">localTop</span><span class="o">.</span><span class="na">getTopK</span><span class="o">();</span>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">top</span> <span class="o">:</span> <span class="n">result</span><span class="o">.</span><span class="na">keySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">IntWritable</span> <span class="n">topWritable</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="n">top</span><span class="o">);</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">string</span> <span class="o">:</span> <span class="n">result</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">top</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">word</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">string</span><span class="o">);</span>
</span><span class='line'>                <span class="n">kvWriter</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">topWritable</span><span class="o">,</span> <span class="n">word</span><span class="o">);</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>All we have left is to <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L318">take</a>
the first K elements of the reduced data set (from the local top Ks) and write it to HDFS and we&rsquo;re done. Except that we have to
define the data movements with edges.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">DAG</span> <span class="n">dag</span> <span class="o">=</span> <span class="n">DAG</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;topk&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">dag</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">tokenizerVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">sumVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">writerVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addEdge</span><span class="o">(</span><span class="n">Edge</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">tokenizerVertex</span><span class="o">,</span> <span class="n">sumVertex</span><span class="o">,</span> <span class="n">tokenSumEdge</span><span class="o">.</span><span class="na">createDefaultEdgeProperty</span><span class="o">()))</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addEdge</span><span class="o">(</span><span class="n">Edge</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">sumVertex</span><span class="o">,</span> <span class="n">writerVertex</span><span class="o">,</span> <span class="n">sumWriterEdge</span><span class="o">.</span><span class="na">createDefaultBroadcastEdgeProperty</span><span class="o">()));</span>
</span></code></pre></td></tr></table></div></figure>


<p>The execution of this DAG looks something like this:</p>

<p><img src="http://yuml.me/b6bf74a3" alt="" /></p>

<p>In the last Vertex we start collecting the grouped sorted data so we can take the first K elements. This part kills the parallelism as
we need to see the global picture here, that&rsquo;s why you can see that the parallelism is
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L129">set</a> to <code>1</code>.
Leaving the parallelism undefined or -1 results that the number of launched tasks will be determined by the <code>ApplicationMaster</code>.</p>

<h3>TopK DataGen</h3>

<p>You also can generate an arbitrary size of dataset with the
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDataGen.java">TopKDataGen</a>
job. This is a special DAG which has only 1 Vertex and no Edges.</p>

<h3>How to run the examples</h3>

<p>First of all you will need a Tez cluster &ndash; we have put together a real one, you can get it from <a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a>. Pull the container, and follow the instructions below.</p>

<p>Build the project <code>mvn clean install</code> which will generate a jar. Copy this jar to HDFS and you are good to go. In order to make this jar
runnable we also created a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDriver.java">driver</a>
class.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">hadoop</span> <span class="n">jar</span> <span class="n">tez</span><span class="o">-</span><span class="n">topk</span><span class="o">-</span><span class="mf">1.0</span><span class="o">.</span><span class="na">jar</span> <span class="n">topkgen</span> <span class="o">/</span><span class="n">data</span> <span class="mi">1000000</span>
</span><span class='line'><span class="n">hadoop</span> <span class="n">jar</span> <span class="n">tez</span><span class="o">-</span><span class="n">topk</span><span class="o">-</span><span class="mf">1.0</span><span class="o">.</span><span class="na">jar</span> <span class="n">topk</span> <span class="o">/</span><span class="n">data</span> <span class="o">/</span><span class="n">result</span> <span class="mi">0</span> <span class="mi">10</span>
</span></code></pre></td></tr></table></div></figure>


<h2>What&rsquo;s next</h2>

<p>In the next post we&rsquo;ll see how we can achieve the same with Spark and we&rsquo;ll do a performance comparison on a large dataset.
Cascading also works on the Tez integration, so we&rsquo;ll definitely report on that too.
If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
