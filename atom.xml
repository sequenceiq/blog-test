<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-10-02T20:29:54+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SequenceIQ Joins Hortonworks Technology Partner Program]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/30/hortonworks-partnership/"/>
    <updated>2014-09-30T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/30/hortonworks-partnership</id>
    <content type="html"><![CDATA[<p>Integration of Hortonworks Data Platform with Cloudbreak enables Hadoop to run in Docker containers &ndash; shipped to the cloud.</p>

<p><strong>SAN FRANCISCO, September 30, 2014</strong> — <a href="http://sequenceiq.com/">SequenceIQ ,Inc.</a> today announced that it has joined the <a href="http://hortonworks.com/partners/become-a-partner/">Hortonworks® Technology Partner Program</a>. <a href="http://hortonworks.com/">Hortonworks</a> is the leading contributor to and provider of Apache™ Hadoop®. SequenceIQ will integrate <a href="http://hortonworks.com/hdp/">Hortonworks Data Platform</a> (HDP) with Cloudbreak to enable a cloud agnostic, autoscaling and <a href="https://www.docker.com/">Docker</a> container based provisioning of HDP.</p>

<p>By joining the Hortonworks Technology Partner program, SequenceIQ will work to enable and accelerate the deployment of a modern data architecture, integrating with the Hortonworks Data Platform—the industry’s only 100 percent open source Hadoop distribution, explicitly architected, built, and tested for enterprise-grade deployments.</p>

<p>SequenceIQ’s technology enables organizations to have a DevOps friendly way to ease and automate provisioning of on-demand Hadoop clusters and services using their favorite cloud provider. With the integration of HDP, users can now leverage all the available features of a 100 percent open source commercial Hadoop distribution.</p>

<p>“SequenceIQ and Hortonworks share a common goal of making the provisioning of Apache Hadoop clusters easier on different cloud and Docker container based environments,” said Janos Matyas, chief technology officer of SequenceIQ. “Cloudbreak and HDP help enterprises to minimize the cost of their Hadoop deployments, and create on-demand autoscaling Hadoop clusters.”</p>

<!-- more -->


<p>Hortonworks Data Platform was built by the core architects, builders and operators of Apache Hadoop and includes all of the necessary components to manage a cluster at scale and uncover business insights from existing and new big data sources. With a <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>-based architecture, HDP is an important component of the modern data architecture, helping organizations mine, process and analyze large batches of unstructured data sets to make more informed business decisions.</p>

<p>“Hortonworks is dedicated to expanding and empowering the Apache Hadoop ecosystem, accelerating innovation and adoption of 100 percent open source enterprise Hadoop,” said John Kreisa, vice president of strategic marketing at Hortonworks. “We welcome SequenceIQ to the Hortonworks Technology Partner Program and look forward to working with them to strengthen Hadoop’s role as the foundation of the next-generation data architecture.”</p>

<p>About SequenceIQ</p>

<p>SequenceIQ is an innovative big data startup with the mission statement to simplify and automate provisioning of on-demand Hadoop clusters running on different environments. Leaders in “containerizing” Hadoop, SequenceIQ is the first company who provisions and runs the full Hadoop stack and components on Docker containers. The company has further plans to create the industry’s first cloud agnostic, autoscaling and use case driven Platform as a Service API, and break the common significant barriers of entry that exist with most big data projects through a series of As-a-Service API offerings.
For more information follow up with us on @SequenceIQ and <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>.</p>

<p><a href="&#109;&#97;&#105;&#108;&#116;&#x6f;&#x3a;&#105;&#110;&#102;&#111;&#64;&#x73;&#x65;&#113;&#117;&#x65;&#110;&#99;&#101;&#105;&#113;&#46;&#99;&#x6f;&#x6d;">&#x69;&#110;&#102;&#111;&#x40;&#115;&#x65;&#x71;&#117;&#x65;&#x6e;&#x63;&#x65;&#x69;&#113;&#x2e;&#99;&#x6f;&#109;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark - create and test jobs]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/29/spark-correlation-and-testing/"/>
    <updated>2014-09-29T13:42:24+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/29/spark-correlation-and-testing</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use different runtimes (MR2, Spark, Tez) when submitting jobs from <a href="http://docs.banzai.apiary.io/reference">Banzai</a> to a YARN clusters.
Some of these jobs are quite simple (filtering, sorting, projection etc.), but most of them can be complicated or not so oblivious at first (e.g.: complex machine learning algorithms).
From Banzai’s perspective/looking from outside a YARN cluster, what only matters is the input and the output dataset &ndash; as we have abstracted all the pipeline steps &ndash;  so testing of this steps properly is a must.
In this post we’d like to show such an example that &ndash; a correlation job on vectors with <a href="https://spark.apache.org/">Apache Spark</a> and how we test it.</p>

<h2>Correlation example (on vectors) with Apache Spark</h2>

<p>Suppose that we have an input dataset (CSV file for the sake of simplicity of the sample code) and we want to reveal the dependency between all of the columns. (all data is vectorized, if not you will have to vectorize your data first).
If we want to build a <code>testable</code> job, we have to focus only on the algorithm part. Our goal here is to work only on the Resilient Distributed Dataset and take the context creation outside of the job.
This way you cab run and create your <code>SparkContext</code>locally and substitute an HDFS data source (or something else) with simple objects.</p>

<p>Interface: (output: vector index pairs with their correlation coefficient)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">abstract</span> <span class="k">class</span> <span class="nc">CorrelationJob</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">d2d</span><span class="o">(</span><span class="n">d</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="nc">DecimalFormat</span><span class="o">(</span><span class="s">&quot;#.######&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="n">d</span><span class="o">).</span><span class="n">toDouble</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Below we show you how a Pearson correlation job implementation looks like with RDD functions. First, you need to gather all combinations of the vector indices and count the size of the dataset.
After that, the only thing what you need is to compute the <a href="http://www.statisticshowto.com/what-is-the-correlation-coefficient-formula/">correlation coefficient</a> on all column combinations (based on the square, dot product and sum of the fields per line). It takes 1 map and 1 reduce operation per pairs. (<code>iterative</code> &ndash;> typical example where you need to use Spark instead of MR2)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">numbersInput</span> <span class="k">=</span> <span class="n">input</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">combinedFields</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0</span> <span class="n">to</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">first</span><span class="o">().</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="o">).</span><span class="n">combinations</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">size</span> <span class="k">=</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">res</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="n">field</span> <span class="k">&lt;-</span> <span class="n">combinedFields</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">col1Index</span> <span class="k">=</span> <span class="n">field</span><span class="o">.</span><span class="n">head</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">col2Index</span> <span class="k">=</span> <span class="n">field</span><span class="o">.</span><span class="n">last</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">tempData</span> <span class="k">=</span> <span class="n">numbersInput</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="n">arr</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">data1</span> <span class="k">=</span> <span class="n">arr</span><span class="o">(</span><span class="n">col1Index</span><span class="o">)</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">data2</span> <span class="k">=</span> <span class="n">arr</span><span class="o">(</span><span class="n">col2Index</span><span class="o">)</span>
</span><span class='line'>        <span class="o">(</span><span class="n">data1</span><span class="o">,</span> <span class="n">data2</span><span class="o">,</span> <span class="n">data1</span> <span class="o">*</span> <span class="n">data2</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">data1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">data2</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
</span><span class='line'>      <span class="o">}}</span>
</span><span class='line'>      <span class="k">val</span> <span class="o">(</span><span class="n">sum1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sum2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="n">tempData</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="o">((</span><span class="n">a1</span><span class="o">,</span> <span class="n">a2</span><span class="o">,</span> <span class="n">aDot</span><span class="o">,</span> <span class="n">a1sq</span><span class="o">,</span> <span class="n">a2sq</span><span class="o">),</span> <span class="o">(</span><span class="n">b1</span><span class="o">,</span> <span class="n">b2</span><span class="o">,</span> <span class="n">bDot</span><span class="o">,</span> <span class="n">b1sq</span><span class="o">,</span> <span class="n">b2sq</span><span class="o">))</span> <span class="k">=&gt;</span>
</span><span class='line'>          <span class="o">(</span><span class="n">a1</span> <span class="o">+</span> <span class="n">b1</span><span class="o">,</span> <span class="n">a2</span> <span class="o">+</span> <span class="n">b2</span><span class="o">,</span> <span class="n">aDot</span> <span class="o">+</span> <span class="n">bDot</span><span class="o">,</span> <span class="n">a1sq</span> <span class="o">+</span> <span class="n">b1sq</span><span class="o">,</span> <span class="n">a2sq</span> <span class="o">+</span> <span class="n">b2sq</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">corr</span> <span class="k">=</span> <span class="n">pearsonCorr</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">sq1</span><span class="o">,</span> <span class="n">sq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span>
</span><span class='line'>      <span class="o">(</span><span class="n">col1Index</span><span class="o">,</span> <span class="n">col2Index</span><span class="o">,</span> <span class="n">d2d</span><span class="o">(</span><span class="n">corr</span><span class="o">))</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">res</span><span class="o">.</span><span class="n">toArray</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// correlation formula</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">pearsonCorr</span><span class="o">(</span><span class="n">size</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">sum1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sum2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">numerator</span> <span class="k">=</span> <span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">dotProduct</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">sum1</span> <span class="o">*</span> <span class="n">sum2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">denominator</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq1</span> <span class="o">-</span> <span class="n">sum1</span> <span class="o">*</span> <span class="n">sum1</span><span class="o">)</span> <span class="o">*</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq2</span> <span class="o">-</span> <span class="n">sum2</span> <span class="o">*</span> <span class="n">sum2</span><span class="o">)</span>
</span><span class='line'>    <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>MLlib Statistics</h2>

<p>By the way <a href="https://spark.apache.org/releases/spark-release-1-1-0.html">Spark Release 1.1.0</a> contains an algorithm for correlation computation, thus we now show you how to use that instead of the above one.
With <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/stat/Statistics.scala">Statistics</a> you can produce a correlation matrix from vectors. For obtaining the correlation coefficient pairs, we just need to get the upper triangular matrix without diagonal. It looks much simpler, isn&rsquo;t is?</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">computeCorrelation</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">vectors</span> <span class="k">=</span> <span class="n">input</span>
</span><span class='line'>      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)))</span>
</span><span class='line'>      <span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">corr</span><span class="k">:</span> <span class="kt">Matrix</span> <span class="o">=</span> <span class="nc">Statistics</span><span class="o">.</span><span class="n">corr</span><span class="o">(</span><span class="n">vectors</span><span class="o">,</span> <span class="s">&quot;pearson&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">num</span> <span class="k">=</span> <span class="n">corr</span><span class="o">.</span><span class="n">numRows</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// upper triangular matrix without diagonal</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">res</span> <span class="k">=</span> <span class="k">for</span> <span class="o">((</span><span class="n">x</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">corr</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">zipWithIndex</span> <span class="k">if</span> <span class="o">(</span><span class="n">i</span> <span class="o">/</span> <span class="n">num</span><span class="o">)</span> <span class="o">&lt;</span> <span class="n">i</span> <span class="o">%</span> <span class="n">num</span> <span class="o">)</span>
</span><span class='line'>    <span class="k">yield</span> <span class="o">((</span><span class="n">i</span> <span class="o">/</span> <span class="n">num</span><span class="o">),</span> <span class="o">(</span><span class="n">i</span> <span class="o">%</span> <span class="n">num</span><span class="o">),</span> <span class="n">d2d</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">res</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>For testing Spark jobs we use the Specs2 framework. We do not want to start a Spark context before every test case, so we just start/end it before/after steps.
In order to run Spark locally set master to &ldquo;local&rdquo;. In our example (for demonstration purposes) we do not turn off Spark logging (or set to warn level) but it is recommended.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">abstract</span> <span class="k">class</span> <span class="nc">SparkJobSpec</span> <span class="k">extends</span> <span class="nc">SpecificationWithJUnit</span> <span class="k">with</span> <span class="nc">BeforeAfterExample</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="nd">@transient</span> <span class="k">var</span> <span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span> <span class="o">=</span> <span class="k">_</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">beforeAll</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.driver.port&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.hostPort&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">afterAll</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">sc</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">sc</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
</span><span class='line'>      <span class="n">sc</span> <span class="k">=</span> <span class="kc">null</span>
</span><span class='line'>      <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.driver.port&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.hostPort&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">fs</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="nc">Fragments</span><span class="o">)</span> <span class="k">=</span> <span class="nc">Step</span><span class="o">(</span><span class="n">beforeAll</span><span class="o">)</span> <span class="o">^</span> <span class="k">super</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">fs</span><span class="o">)</span> <span class="o">^</span> <span class="nc">Step</span><span class="o">(</span><span class="n">afterAll</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In our test specification we check that both correlation implementations are correct or not.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="nd">@RunWith</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">JUnitRunner</span><span class="o">])</span>
</span><span class='line'><span class="k">class</span> <span class="nc">CorrelationJobTest</span> <span class="k">extends</span> <span class="nc">SparkJobSpec</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="s">&quot;Spark Correlation implementations&quot;</span> <span class="n">should</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&quot;1,2,9,5&quot;</span><span class="o">,</span> <span class="s">&quot;2,7,5,6&quot;</span><span class="o">,</span><span class="s">&quot;4,5,3,4&quot;</span><span class="o">,</span><span class="s">&quot;6,7,5,6&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">correctOutput</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mf">0.620299</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="o">-</span><span class="mf">0.627215</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.11776</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="o">-</span><span class="mf">0.70069</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.552532</span><span class="o">),</span>
</span><span class='line'>      <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mf">0.207514</span><span class="o">)</span>
</span><span class='line'>      <span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="s">&quot;case 1 : return with correct output (custom spark correlation)&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">customCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CustomCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">customCorr</span> <span class="n">must_==</span> <span class="n">correctOutput</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="s">&quot;case 2: return with correct output (stats spark correlation)&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">statCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatsCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">statCorr</span> <span class="n">must_==</span> <span class="n">correctOutput</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="s">&quot;case 3: equal to each other&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">inputRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">statCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatsCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">customCorr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CustomCorrelationJob</span><span class="o">().</span><span class="n">computeCorrelation</span><span class="o">(</span><span class="n">inputRDD</span><span class="o">,</span> <span class="n">sc</span><span class="o">)</span>
</span><span class='line'>      <span class="n">statCorr</span> <span class="n">must_==</span> <span class="n">customCorr</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To build and test the project use this command from our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> spark-correlation directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>You can run this correlation example in our free Docker based Apache Spark container as well. (with <a href="https://github.com/apache/spark/blob/master/bin/spark-submit">spark-submit</a> script). You can get the Spark container from the official <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Docker registry</a> or from our <a href="https://github.com/sequenceiq/docker-spark">GitHub</a> repository. The source code is available at <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-correlation">SequenceIQ&rsquo;s GitHub repository</a>.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing database upgrades with Liquibase and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process/"/>
    <updated>2014-09-26T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/26/database-upgrade-process</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we are <em>crazy</em> about automating everything &ndash; let it be the provisioning of a thousand nodes Hadoop
cluster using <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> or a simple database change.
We apply the same automated CI/CD process to all our projects, including plain old RDBMS schema changes &hellip; yes, though we are a <em>big data</em> technology
company sometimes we do use JPA as well.</p>

<p>As applications evolve their underlying data model change. New functionalities often need data model changes and the initial design
needs to be adapted to the ever changing demands. These changes usually are of two types : structural changes
(e.g.: addition/removal of tables, columns, constraints etc &hellip;)
and migration of the existing data to the new version of the datamodel.
As the data model gets more and more complex  &ndash; this will happen in-spite of trying to keep it as simple as possible &ndash;
the complexity of these tasks grow proportionally. This happens here at SequenceIQ too; the post is about how we address some of these problems.</p>

<!-- more -->


<h2>Directives</h2>

<ul>
<li>We need a process to follow each time such changes arise</li>
<li>Use appropriate tools that do the job (instead of reinventing the wheel)</li>
<li>Make the process <strong>automated</strong> as much as possible</li>
</ul>


<h3>The process</h3>

<p>The process &ndash; as the common sense suggests &ndash; could be split in the following steps:</p>

<ul>
<li>start from the initial version of the database (the version in production)</li>
<li>perform changes required by the new version of the application</li>
<li>capture and store differences between the two versions of the database</li>
<li>(automatically) apply changes to the initial database version</li>
<li>perform tests</li>
<li>apply changes to production</li>
</ul>


<h3>Tools</h3>

<ul>
<li>Dockerized (PostgreSQL) database</li>
<li>Dockerized Liquibase</li>
<li>Jenkins</li>
</ul>


<h3>Implementation</h3>

<h4>Start from the initial version of the database</h4>

<p>To start with, you need a database that&rsquo;s (structurally) identical with the production one. There are several ways to achieve this;we use Postgres and try to
keep it simple, so here&rsquo;s what we do:</p>

<ul>
<li>we always have a QA database which is identical with the production (obliviously the data is not the same)</li>
<li>we make a copy of the <em>data</em> folder of the postgres installation into an arbitrary location on the host</li>
<li>we pass it as a volume to a Docker container running Postgres</li>
</ul>


<p>We run the following command each time we need a fresh database:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d \
</span><span class='line'>  --name $CONTAINER_NAME \
</span><span class='line'>  -v /$WORKING_DIR/data:/data \
</span><span class='line'>  -p 5432:5432 \
</span><span class='line'>  -e "USER=$DB_USER" \
</span><span class='line'>  -e "PASS=$DB_PASS" \
</span><span class='line'>  -e "DB=$DB_NAME" \
</span><span class='line'>paintedfox/postgresql</span></code></pre></td></tr></table></div></figure>


<p>where the passed in variables are the following:</p>

<ul>
<li>CONTAINER_NAME &ndash; the name of the database Docker container</li>
<li>DB_USER &ndash; the database user name</li>
<li>DB_PASS &ndash; the database password</li>
<li>DB_NAME &ndash; the database schema</li>
</ul>


<p>We have a running database now (in less than a minute) &ndash; same as the prod; we can connect to it with a client on your localhost, port 5432 with the given username/password.</p>

<h4>Perform changes required by the new version of the application</h4>

<p>As expected, this is the most challenging part in the process: changes need to be implemented and also
captured so that they can be applied any time (preferably in an <strong>automated</strong> way)
As we&rsquo;re using JPA (with Hibernate as JPA provider) incremental structural changes are executed with the
SchemaUpdate tool. This can be done during the application startup or using <em>ant</em> or <em>maven</em>.
As we continuously test the application we choose to start the application configured to update the database based on the
changed data model (annotations). Alternatively we could regenerate the whole schema. (See the SchemaUpdate tool documentation:
<a href="http://docs.jboss.org/hibernate/core/3.6/reference/en-US/html/toolsetguide.html">here</a>)</p>

<p>At this point we have a database that aligns with the new version of the application. Please note here, that only <code>incremental</code> changes have been applied to
the database till now, meaning that for example new fields have been added,
 but old/deprecated fields haven&rsquo;t been deleted.</p>

<p>Other type of scripts need to be implemented manually:</p>

<ul>
<li><p>changes that couldn&rsquo;t be performed by the SchemaUpdate tool, such as cleanup (SQL) scripts. This being done, differences till this phase
can be automatically generated by running the Liquibase Docker container &ndash; see the next section. Differences are stored under version control,
in form of <em>Liquibase changelogs</em></p></li>
<li><p>data migration scripts, that adapt the existing data to the new structure. Think of cases
when for example a field becomes a new entity and instead of a value you need to store a reference to the new entity. We store these kind of scripts along
with the generated diff files under version control in form of <em>Liquibase changelogs</em></p></li>
</ul>


<h4>Dockerized Liquibase</h4>

<p>Speaking of tools, we found that <a href="http://www.liquibase.org/index.html">Liquibase</a> addresses many of our requirements, such as</p>

<ul>
<li>track database changes (changes being stored in VCS)</li>
<li>automatically generate diffs between two versions of the database</li>
<li>automatically update a database based on changelogs</li>
</ul>


<p>We have created a docker image with a <em>liquibase</em> installation. You can find the project <a href="https://github.com/sequenceiq/docker-liquibase">here</a></p>

<p>The image can be built locally with the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build -t sequenceiq/docker-liquibase .</span></code></pre></td></tr></table></div></figure>


<p>or from the project root, or pulled from the Docker repository:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/docker-liquibase</span></code></pre></td></tr></table></div></figure>


<p>Containers built from this image can be used to perform <em>liquibase</em> operations on any host.
This saves us a lot of time by having the installation and configuration shipped and helps us to automate most of the tasks.
You can use the container for performing liquibase tasks manually in a terminal, or you can start the container to
automatically perform specific tasks (and quit eventually). To start the container linked to the previously started database
container and perform manual operations, run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it \
</span><span class='line'>--name $LIQUIBASE_CONTAINER \
</span><span class='line'>--link $DB_CONTAINER:db \
</span><span class='line'>--entrypoint /bin/bash \
</span><span class='line'>-v /$LIQUIBASE_CHANGELOGS:/changelogs \
</span><span class='line'>$LIQUIBASE_DOCKER_IMAGE \
</span><span class='line'>/bin/bash</span></code></pre></td></tr></table></div></figure>


<p>See the description of the variables:</p>

<ul>
<li>LIQUIBASE_CONTAINER the name of the Liquibase Docker container</li>
<li>DB_CONTAINER the name of the database container the Liquibase container is to be linked to</li>
<li>LIQUIBASE_CHANGELOGS the folder holding the liquibase changelogs (Liquibase will read and write here)</li>
<li>LIQUIBASE_DOCKER_IMAGE the name of the dockerized Liquibase Docker image</li>
</ul>


<p>Some of the Liquibase tasks can be scripted. We scripted the diff generation and changelog application. Liquibase offers more advanced features too.</p>

<h4>Testing</h4>

<p>We write tests that can be run automatically to check the process. Each <code>changeset</code>, especially those related to data migration / transformation is covered.</p>

<h4>Apply liquibase changelogs to the production database</h4>

<p>After the application is tested upon applying the database changes &ndash; that ensures that changelogs are correct, it&rsquo;s easy to set up a <code>jenkins</code> job that:</p>

<ul>
<li>checks out the proper version of changelogs</li>
<li>starts a docker container linked to the (production) database and applies changelogs</li>
</ul>


<p>Obviously this step needs to be designed carefully and adapted to the custom application deployment needs.</p>

<h4>Notes</h4>

<ul>
<li>Thanks to Docker, all the work described here can be done offline (setting up the infrastructure can be done fast, on a dev&rsquo;s machine for example)</li>
<li>Liquibase changelogs can be executed individually or in group (by including subsets of changelogs) thus during the whole process we can adopt a
step-by step approach</li>
</ul>


<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>,
 <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Strata + Hadoop World 2014 Startup Showcase]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/25/strata-hadoop-world-2014/"/>
    <updated>2014-09-25T07:42:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/25/strata-hadoop-world-2014</id>
    <content type="html"><![CDATA[<p><a href="http://sequenceiq.com">SequenceIQ</a> is happy to announce that it has been selected among the top 10 leading big data startups by a team of investors, entrepreneurs, and industry analysts and will present live at the Startup Showcase at <strong>Strata + Hadoop World 2014, New York.</strong></p>

<p>We will have a space in the showcase to win over the judges, and pitch our company and innovative technology to the thousands of developers, founders, executives, investors and researchers that attend <a href="http://strataconf.com/stratany2014">Strata</a>.</p>

<p>Come and watch our showcase on Wednesday, October 15 or catch up with us during the conference (@sequenceiq).</p>

<p>Stay in touch with us through <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Euroventures invests in SequenceIQ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/25/euroventures-invests-in-sequenceiq/"/>
    <updated>2014-09-25T07:42:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/25/euroventures-invests-in-sequenceiq</id>
    <content type="html"><![CDATA[<p>We are happy to announce that Euroventures has invested in SequenceIQ. Euroventure&rsquo;s investment is intended to foster the company&rsquo;s growth in terms of staffing, technology stack and expansion into the U.S. market.</p>

<p>Euroventures&#8217; press release is available <a href="http://www.euroventures.hu/news.php?id=296">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Edit files in Docker containers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker/"/>
    <updated>2014-09-24T13:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/24/edit-files-docker</id>
    <content type="html"><![CDATA[<p>I wish I get 1 dollar each time I install vi in a docker container &hellip; I wanted
an easier way to edit files in a running docker container. First of all try to
<strong>avoid</strong> editing files at all, as it is against the container philosophy
(see the last paragraph).</p>

<p>But if you have a valid reason, here comes the how-to.</p>

<h2>Why Zedapp</h2>

<p>Most of the time I use either vi or <a href="https://atom.io/">Atom</a>, but a few months
ago I stumbled upon <a href="http://zedapp.org/">Zedapp</a> an opinionated editor. It aims to
reduce cognitive load while editing, by simplifying things, like deliberately
not using tabs.</p>

<p>It stands out with its <strong>first-class support of remote editing</strong> let it be a
remote server, or even directly editing github repositories.</p>

<p>Zedapp just reached version 1.0 and if you like it, consider help Zef Hemmel
at <a href="https://gratipay.com/zefhemel/">gratipay</a>, who was brave enough to quit his
regular job, and work full time on an open-source project!</p>

<h2>Install Zedapp</h2>

<p>You can use zedapp as a <em>chrome plugin</em> or a <em>standalone</em> app. Downloads are
available at: <a href="http://zedapp.org/download/">zedapp.org</a>. I recommend to
go for the <strong>standalone</strong> version.</p>

<!-- more -->


<h2>Install zedrem</h2>

<p>For <a href="http://zedapp.org/features/edit-remote-files/">remote editing</a>,
you need zedrem, a small process serving files to be edited in Zedapp.
Zedrem is packaged into a docker image:
<a href="https://github.com/sequenceiq/docker-zedapp">sequenceiq/zedapp</a></p>

<p>To start a local zed-server, and zed-client in the target container, there
is a helper script: <strong>zed</strong></p>

<p>To install the docker image and the shell script run this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm \
</span><span class='line'>  -v /usr/local/bin:/target \
</span><span class='line'>  -v /usr/local/bin/docker:/usr/local/bin/docker \
</span><span class='line'>  -v /var/run/docker.sock:/var/run/docker.sock \
</span><span class='line'>  sequenceiq/zedapp</span></code></pre></td></tr></table></div></figure>


<p>Actually there is only a single binary called <strong>zedrem</strong>, i just use the
terminology: zed-server and zed-client to
distinguish when you use it with or without the <code>--server</code> option.</p>

<p>Now you are ready to start a zedrem session, to edit files in Zedapp which are
inside of a Docker container&rsquo;s directory.</p>

<h2>Start a zedrem session</h2>

<p>To start a zedrem client in a container</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed &lt;container&gt; &lt;directory&gt;</span></code></pre></td></tr></table></div></figure>


<p>This will:
&ndash; start a <code>zedrem-server</code> if not already running.
&ndash; copy and start <code>zedrem-client</code> into the selected container and print out
  the zedrem session&rsquo;s <strong>remote-url</strong>.</p>

<p>Navigate to the project list window by: <code>Command-Shift-O</code>/<code>Ctrl-Shift-O</code>. Select
 <code>Remote Folder</code>, enter the remote-url into <code>Zedrem URL</code> input field and press
 <code>Open</code>.</p>

<p>Thats all enyoj! All the following paragraphs are for the curious only.</p>

<h2>Boot2docker helper function</h2>

<p>The <code>Install zedrem</code> step should have detected that you are using Boot2docker,
and instructed you to create a helper function, but in case you missed it, or
for reference:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed() { boot2docker ssh "sudo zed $@" ; }</span></code></pre></td></tr></table></div></figure>


<p>This is needed as the helper script called <code>zed</code> is installed inside of
Boot2docker, so you need the ususal <code>boot2docker ssh</code> workaround.</p>

<p>after that you can issue directly on OSX:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zed &lt;container&gt; &lt;directory&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Local zedrem server.</h2>

<p>By default when you want to use Zedapp for remote editing, you need two
other components then Zedapp:</p>

<ul>
<li><strong>zedrem-server</strong> Zedapp gets file content, and sends edit commands
on webservices protocol. It maintains sessions with zedrem-clients.</li>
<li><strong>zedrem-client</strong> a small process serving files from a specified directory.</li>
</ul>


<p>When you use zedrem-client via the official server, all the editing commands/content
travel around the blobe:</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-remote.png" alt="zedrem remote" /></p>

<p>Compare it with the dockerized local server setup, which is more quick and
secure:
<img src="https://raw.githubusercontent.com/sequenceiq/docker-zedapp/master/images/zed-docker.png" alt="zedrem docker" /></p>

<h2>nsenter</h2>

<p>You might wonder about the step: <strong>copy zedrem into the container</strong>. How is it
possible? Docker&rsquo;s <code>cp</code> command only supportts the other direction: copy from a
container into a local dir.</p>

<p>There is an <a href="https://github.com/docker/docker/issues/5846">open issue</a>, so it
will be fixed soon, but meanwhile you can use nsenter to the rescue. Jérôme
Petazzoni prepared us a canned <a href="https://github.com/jpetazzo/nsenter">nsenter</a>
with the helper script: <code>docker-enter</code>. We can missuse docker-enter to copy
a file from local fs into the container by:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat local-file | docker-enter $container sh -c 'cat&gt;/zedrem'</span></code></pre></td></tr></table></div></figure>


<p>btw: <code>docker exec</code> is already merged into the master branch, so it will replace
nsenter completely.</p>

<h2>Don&rsquo;t do this at all</h2>

<p>Let&rsquo;s make it clear, that most of the time you don&rsquo;t need this.
First of all editing files in a container, other than development or debug
considered bad practice.</p>

<p>You find yourself editing nginx config files? Don&rsquo;t do it, use the great generic
<a href="https://github.com/progrium/nginx-appliance">nginx appliance</a> from Jeff Lindsay.</p>

<p>If you <strong>really</strong> need to edit files in a docker container, just use volumes.</p>

<p>This process comes handy if you&rsquo;ve already started a container, and the file in
question doesn&rsquo;t sits on a volume. [16]</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TopK on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/"/>
    <updated>2014-09-23T17:53:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez</id>
    <content type="html"><![CDATA[<p>The Apache Tez community draw attention last week with their latest release <a href="http://tez.apache.org/releases/0.5.0/release-notes.txt">0.5.0</a>
of the application framework. At SequenceIQ we always try to find and provide the best solutions to our customers and share the experience we gain by
being involved in many open source Apache projects. We are always looking for the latest innovations, and try to apply them to our projects.
For a while we have been working hard on a new project called
<a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> which we&rsquo;ll open source in the near future. One handy feature of the projects is the ability to run the same pipes on <code>MR2</code>, <code>Spark</code> and <code>Tez</code> &ndash; your choice.
In the next couple of posts we&rsquo;ll compare these runtimes using different jobs and as the first example to implement we chose TopK. Before going into
details let&rsquo;s revisit what Apache Tez is made of.</p>

<h2>Apache Tez key concepts</h2>

<ul>
<li>One of the most important feature is that there is no heavy deployment phase which otherwise could go wrong in many ways &ndash; probably sounds familiar
for most of us. There is a nice <a href="http://tez.apache.org/install.html">install guide</a> on the project&rsquo;s page which you can follow, but basically
you have to copy a bunch of jars to HDFS and you&rsquo;re almost good to go.</li>
<li>Multiple versions of Tez can be used at the same time which solves a common problem, the rolling upgrades.</li>
<li>Distributed data processing jobs typically look like <code>DAGs</code> (directed acyclic graphs) and Tez relies on this concept to define your jobs.
DAGs are made from <code>Vertices</code> and <code>Edges</code>. Vertices in the graph represent data transformations while edges represent the data movement
from producers to consumers. The DAG itself defines the structure of the data processing and the relationship between producers and consumers.</li>
</ul>


<p>Tez provides faster execution and higher predictability because:</p>

<ul>
<li>Eliminates replicated write barriers between successive computations</li>
<li>Eliminates the job launch overhead</li>
<li>Eliminates the extra stage of map reads in every workflow job</li>
<li>Provides better locality</li>
<li>Capable to re-use containers which reduces the scheduling time and speeds up incredibly the short running tasks</li>
<li>Can share in-memory data across tasks</li>
<li>Can run multiple DAGs in one session</li>
<li>The core engine can be customized (vertex manager, DAG scheduler, task scheduler)</li>
<li>Provides an event mechanism to communicate between tasks (data movement events to inform consumers by the data location)</li>
</ul>


<p>If you&rsquo;d like to try Tez on a fully functional multi-node cluster we put together an Ambari based Docker image. Click
<a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a> for details.</p>

<!-- more -->


<h2>TopK</h2>

<p>The goal is to find the top K elements of a dataset. In this example&rsquo;s case is a simple CSV and we&rsquo;re looking for the top elements in a given column.
In order to do that we need to <code>group</code> and <code>sort</code> them to <code>take</code> the K elements. The implementation can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a> repository. The important part starts
with the <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L136">DAG creation</a>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">DataSourceDescriptor</span> <span class="n">dataSource</span> <span class="o">=</span> <span class="n">MRInput</span><span class="o">.</span><span class="na">createConfigBuilder</span><span class="o">(</span><span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="n">tezConf</span><span class="o">),</span>
</span><span class='line'>            <span class="n">TextInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">inputPath</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">DataSinkDescriptor</span> <span class="n">dataSink</span> <span class="o">=</span> <span class="n">MROutput</span><span class="o">.</span><span class="na">createConfigBuilder</span><span class="o">(</span><span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="n">tezConf</span><span class="o">),</span>
</span><span class='line'>            <span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">outputPath</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">tokenizerVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">TokenProcessor</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">columnIndex</span><span class="o">))))</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addDataSource</span><span class="o">(</span><span class="n">INPUT</span><span class="o">,</span> <span class="n">dataSource</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="kt">int</span> <span class="n">topK</span> <span class="o">=</span> <span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">top</span><span class="o">);</span>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">sumVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">SUM</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">SumProcessor</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">topK</span><span class="o">)),</span> <span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">numPartitions</span><span class="o">));</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Vertex</span> <span class="n">writerVertex</span> <span class="o">=</span> <span class="n">Vertex</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">WRITER</span><span class="o">,</span>
</span><span class='line'>            <span class="n">ProcessorDescriptor</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">Writer</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
</span><span class='line'>                    <span class="o">.</span><span class="na">setUserPayload</span><span class="o">(</span><span class="n">createPayload</span><span class="o">(</span><span class="n">topK</span><span class="o">)),</span> <span class="mi">1</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addDataSink</span><span class="o">(</span><span class="n">OUTPUT</span><span class="o">,</span> <span class="n">dataSink</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">OrderedPartitionedKVEdgeConfig</span> <span class="n">tokenSumEdge</span> <span class="o">=</span> <span class="n">OrderedPartitionedKVEdgeConfig</span>
</span><span class='line'>            <span class="o">.</span><span class="na">newBuilder</span><span class="o">(</span><span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span> <span class="n">IntWritable</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span>
</span><span class='line'>                    <span class="n">HashPartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">()).</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">UnorderedKVEdgeConfig</span> <span class="n">sumWriterEdge</span> <span class="o">=</span> <span class="n">UnorderedKVEdgeConfig</span>
</span><span class='line'>            <span class="o">.</span><span class="na">newBuilder</span><span class="o">(</span><span class="n">IntWritable</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">(),</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">()).</span><span class="na">build</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>First of all we define a <code>DataSourceDescriptor</code> which represents our dataset and a <code>DataSinkDescriptor</code> where we&rsquo;ll
write the results to. As you can see there are plenty of utility classes to help you define your DAGs. Now that the input and output is
ready let&rsquo;s define our <code>Vertices</code>. You&rsquo;ll see the actual data transformation is really easy as Hadoop will take care of the heavy
lifting. The first Vertex is a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L198">tokenizer</a>
which does nothing more than splitting the rows of the CSV and emit a record with the selected column as the key and <code>1</code> as the value.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">initialize</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>  <span class="kt">byte</span><span class="o">[]</span> <span class="n">payload</span> <span class="o">=</span> <span class="n">getContext</span><span class="o">().</span><span class="na">getUserPayload</span><span class="o">().</span><span class="na">deepCopyAsArray</span><span class="o">();</span>
</span><span class='line'>  <span class="n">ByteArrayInputStream</span> <span class="n">bis</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ByteArrayInputStream</span><span class="o">(</span><span class="n">payload</span><span class="o">);</span>
</span><span class='line'>  <span class="n">DataInputStream</span> <span class="n">dis</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DataInputStream</span><span class="o">(</span><span class="n">bis</span><span class="o">);</span>
</span><span class='line'>  <span class="n">columnIndex</span> <span class="o">=</span> <span class="n">dis</span><span class="o">.</span><span class="na">readInt</span><span class="o">();</span>
</span><span class='line'>  <span class="n">dis</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'>  <span class="n">bis</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">KeyValueWriter</span> <span class="n">kvWriter</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValueWriter</span><span class="o">)</span> <span class="n">getOutputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">WRITER</span><span class="o">).</span><span class="na">getWriter</span><span class="o">();</span>
</span><span class='line'>  <span class="n">KeyValuesReader</span> <span class="n">kvReader</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValuesReader</span><span class="o">)</span> <span class="n">getInputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">).</span><span class="na">getReader</span><span class="o">();</span>
</span><span class='line'>  <span class="k">while</span> <span class="o">(</span><span class="n">kvReader</span><span class="o">.</span><span class="na">next</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">Text</span> <span class="n">word</span> <span class="o">=</span> <span class="o">(</span><span class="n">Text</span><span class="o">)</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentKey</span><span class="o">();</span>
</span><span class='line'>    <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">Object</span> <span class="n">value</span> <span class="o">:</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentValues</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">sum</span> <span class="o">+=</span> <span class="o">((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">value</span><span class="o">).</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">kvWriter</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="n">sum</span><span class="o">),</span> <span class="n">word</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The interesting part here is the <code>initialize</code> method which reads the <code>UserPayload</code> to find out in which column we&rsquo;re looking for
the top K elements. What happens after the first Vertex is that Hadoop will <code>group</code> the records by key, so we&rsquo;ll have all the keys
with a bunch of 1s. In the next Vertex we
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L243">sum</a>
these values so we&rsquo;ll have all the words in the given column counted. We could emit all the values to make Hadoop sort them for us,
but we can optimize this process by reducing the data we need to send over the network. And how we are going to do that? If we
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L340">maintain</a>
a <code>local</code> top K of the data which the sum tasks process and emit only these values all we have left in the last Vertex is to
select the top K results from a much smaller data set. Another important improvement here is that we can use the <code>UnorderedKVEdgeConfig</code>
to avoid sorting the data at the task output and the merge sort at the next tasks input. We used the <code>OrderedPartitionedKVEdgeConfig</code>
between the first 2 Vertices to demonstrate the different edges and as partition can be a huge asset.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">Preconditions</span><span class="o">.</span><span class="na">checkArgument</span><span class="o">(</span><span class="n">getInputs</span><span class="o">().</span><span class="na">size</span><span class="o">()</span> <span class="o">==</span> <span class="mi">1</span><span class="o">);</span>
</span><span class='line'>        <span class="n">Preconditions</span><span class="o">.</span><span class="na">checkArgument</span><span class="o">(</span><span class="n">getOutputs</span><span class="o">().</span><span class="na">size</span><span class="o">()</span> <span class="o">==</span> <span class="mi">1</span><span class="o">);</span>
</span><span class='line'>        <span class="n">KeyValueWriter</span> <span class="n">kvWriter</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValueWriter</span><span class="o">)</span> <span class="n">getOutputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">WRITER</span><span class="o">).</span><span class="na">getWriter</span><span class="o">();</span>
</span><span class='line'>        <span class="n">KeyValuesReader</span> <span class="n">kvReader</span> <span class="o">=</span> <span class="o">(</span><span class="n">KeyValuesReader</span><span class="o">)</span> <span class="n">getInputs</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">TOKENIZER</span><span class="o">).</span><span class="na">getReader</span><span class="o">();</span>
</span><span class='line'>        <span class="k">while</span> <span class="o">(</span><span class="n">kvReader</span><span class="o">.</span><span class="na">next</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">Text</span> <span class="n">currentWord</span> <span class="o">=</span> <span class="o">(</span><span class="n">Text</span><span class="o">)</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentKey</span><span class="o">();</span>
</span><span class='line'>            <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">Object</span> <span class="n">value</span> <span class="o">:</span> <span class="n">kvReader</span><span class="o">.</span><span class="na">getCurrentValues</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">sum</span> <span class="o">+=</span> <span class="o">((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">value</span><span class="o">).</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="n">localTop</span><span class="o">.</span><span class="na">store</span><span class="o">(</span><span class="n">sum</span><span class="o">,</span> <span class="n">currentWord</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">localTop</span><span class="o">.</span><span class="na">getTopK</span><span class="o">();</span>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">top</span> <span class="o">:</span> <span class="n">result</span><span class="o">.</span><span class="na">keySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">IntWritable</span> <span class="n">topWritable</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="n">top</span><span class="o">);</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">string</span> <span class="o">:</span> <span class="n">result</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">top</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">word</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">string</span><span class="o">);</span>
</span><span class='line'>                <span class="n">kvWriter</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">topWritable</span><span class="o">,</span> <span class="n">word</span><span class="o">);</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>All we have left is to <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L318">take</a>
the first K elements of the reduced data set (from the local top Ks) and write it to HDFS and we&rsquo;re done. Except that we have to
define the data movements with edges.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">DAG</span> <span class="n">dag</span> <span class="o">=</span> <span class="n">DAG</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;topk&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">dag</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">tokenizerVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">sumVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addVertex</span><span class="o">(</span><span class="n">writerVertex</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addEdge</span><span class="o">(</span><span class="n">Edge</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">tokenizerVertex</span><span class="o">,</span> <span class="n">sumVertex</span><span class="o">,</span> <span class="n">tokenSumEdge</span><span class="o">.</span><span class="na">createDefaultEdgeProperty</span><span class="o">()))</span>
</span><span class='line'>    <span class="o">.</span><span class="na">addEdge</span><span class="o">(</span><span class="n">Edge</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">sumVertex</span><span class="o">,</span> <span class="n">writerVertex</span><span class="o">,</span> <span class="n">sumWriterEdge</span><span class="o">.</span><span class="na">createDefaultBroadcastEdgeProperty</span><span class="o">()));</span>
</span></code></pre></td></tr></table></div></figure>


<p>The execution of this DAG looks something like this:</p>

<p><img src="http://yuml.me/b6bf74a3" alt="" /></p>

<p>In the last Vertex we start collecting the grouped sorted data so we can take the first K elements. This part kills the parallelism as
we need to see the global picture here, that&rsquo;s why you can see that the parallelism is
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopK.java#L129">set</a> to <code>1</code>.
Leaving the parallelism undefined or -1 results that the number of launched tasks will be determined by the <code>ApplicationMaster</code>.</p>

<h3>TopK DataGen</h3>

<p>You also can generate an arbitrary size of dataset with the
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDataGen.java">TopKDataGen</a>
job. This is a special DAG which has only 1 Vertex and no Edges.</p>

<h3>How to run the examples</h3>

<p>First of all you will need a Tez cluster &ndash; we have put together a real one, you can get it from <a href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/">here</a>. Pull the container, and follow the instructions below.</p>

<p>Build the project <code>mvn clean install</code> which will generate a jar. Copy this jar to HDFS and you are good to go. In order to make this jar
runnable we also created a
<a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/tez-topk/src/main/java/com/sequenceiq/tez/topk/TopKDriver.java">driver</a>
class.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">hadoop</span> <span class="n">jar</span> <span class="n">tez</span><span class="o">-</span><span class="n">topk</span><span class="o">-</span><span class="mf">1.0</span><span class="o">.</span><span class="na">jar</span> <span class="n">topkgen</span> <span class="o">/</span><span class="n">data</span> <span class="mi">1000000</span>
</span><span class='line'><span class="n">hadoop</span> <span class="n">jar</span> <span class="n">tez</span><span class="o">-</span><span class="n">topk</span><span class="o">-</span><span class="mf">1.0</span><span class="o">.</span><span class="na">jar</span> <span class="n">topk</span> <span class="o">/</span><span class="n">data</span> <span class="o">/</span><span class="n">result</span> <span class="mi">0</span> <span class="mi">10</span>
</span></code></pre></td></tr></table></div></figure>


<h2>What&rsquo;s next</h2>

<p>In the next post we&rsquo;ll see how we can achieve the same with Spark and we&rsquo;ll do a performance comparison on a large dataset.
Cascading also works on the Tez integration, so we&rsquo;ll definitely report on that too.
If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Tez cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster/"/>
    <updated>2014-09-19T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/19/apache-tez-cluster</id>
    <content type="html"><![CDATA[<p>This week the <a href="http://tez.apache.org/">Apache Tez</a> community announced the release of the 0.5 version of the project. At <a href="http://sequenceiq.com/">SequenceIQ</a> first time we came across Tez was in 2013 &ndash; after <a href="http://hortonworks.com/">Hortonworks</a> launched the <code>Stinger Initiative</code>. Though we were not using Hive (that might change soon) we have quickly realized the <code>other</code> capabilities of Tez &ndash; the expressive data flow API, data movement patterns, dynamic graph reconfiguration, etc &ndash; to name a few.</p>

<p>We quickly became <code>fans</code> of Tez &ndash; and have started to run internal PoC projects, rewrite ML algorithms and legacy MR2 code to run/leverage Tez. The new release comes with a stable developer API and a proven stability track, and this has triggered a <code>major</code> re-architecture/refactoring project at SequenceIQ. While I don’t want to enter into deep details, we are building a Platform as a Service API &ndash; with the first stages of the project already released, open sourced and in public beta:</p>

<p><a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; our Docker based cloud agnostic Hadoop as a Service API (AWS, Azure, Google Cloud, DigitalOcean);
<a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; an SLA policy based autoscaling API for Hadoop YARN</p>

<p>One of the unreleased component is a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; a big data pipeline API (with 50+ pre-built data and job pipes), running on <strong>MR2, Tez and Spark</strong>.</p>

<p>With all these said, we have put together a <code>Tez Ready</code> Docker based Hadoop cluster to share our excitement and allow you to quickly start and get familiar with the nice features of the Tez API. The cluster is built on our widely used Apache Ambari Docker <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">container</a>, with some additional features. The containers are <code>service discovery</code> aware. You don’t need to setup anything beforehand, configure IP addresses or DNS names &ndash; the only thing you will need to do is just specify the number of nodes desired in your cluster, and you are ready to go. If you are interested on the underlying architecture (using Docker, Serf and dnsmasq) you can check my slides/presentation from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit</a>.</p>

<p>I&rsquo;d like to highlight one important feature of Tez &ndash; us being crazy about automation/DevOps &ndash; the simplicity and the capability of running multiple versions of Tez on the same YARN cluster. We are contributors to many Apache projects (Hadoop, YARN, Ambari, etc) and since we have started to use Tez we consider to contribute there as well (at the end of the day will be a core part of our platform). Adding new features, changing code or fixing bugs always introduce undesired <code>features</code> &ndash; nevertheless, the Tez binaries built by different colleagues can be tested at scale, using the same cluster without affecting each others work. Check Gopal V&rsquo;s good <a href="http://bit.ly/tez-devops">introduction</a> about Tez and DevOps.</p>

<h2>Apache Tez cluster on Docker</h2>

<p>The container’s code is available on our <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea-tez">GitHub</a> repository.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:1.7.0-ea-tez</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h3>Building the image</h3>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build --rm -t sequenceiq/ambari:1.7.0-ea-tez ambari-server/</span></code></pre></td></tr></table></div></figure>


<h2>Running the cluster</h2>

<p>We have put together a few shell functions to simplify your work, so before you start make sure you get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea-tez/ambari-functions">file</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari-tez && . .amb</span></code></pre></td></tr></table></div></figure>


<h3>Create your Apache Tez cluster</h3>

<p>You are almost there. The only thing you will need to do is to specify the number of nodes you need in your cluster. We will launch the containers, they will dynamically join the cluster and apply the Tez specific configurations.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-deploy-cluster 4</span></code></pre></td></tr></table></div></figure>


<p>Once the cluster is started you can <a href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/">enter</a> in the container and submit your custom Tez application or use one of the stock Tez examples.</p>

<p>Check back next week, as we are releasing <code>real world</code> examples running on three different big data fabrics: Tez, MR2 and Spark.</p>

<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak new provider implementation - Part I: Build your custom image]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc/"/>
    <updated>2014-09-18T07:42:58+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/18/custom-image-on-gcc</id>
    <content type="html"><![CDATA[<p>Not so long ago we have released <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the cloud agnostic, open source and Docker based Hadoop as a Service API (with support for <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">autoscaling</a> Hadoop clusters). As we have <code>dockerized</code> the whole Hadoop ecosystem, we are shipping the containers to different cloud providers, such as Amazon AWS, Microsoft Azure and Google Cloud Compute. Also Cloudbreak has an <a href="http://sequenceiq.com/cloudbreak/#add-new-cloud-providers">SDK</a> which allows you to quickly add your favorite cloud provider. In this post (series) we’d like to guide you trough the process, and show you how to create a custom image &ndash; on Google Cloud. We have chose Google Cloud as this is the least documented and has the smallest amount on default images (there are thousand for Amazon, and hundreds for Azure). Nevertheless on all cloud provider usually you’d like to have a custom image with your preferred OS, configuration and potentially installed applications.</p>

<!-- more -->


<h3>Why do we need custom images on every cloud?</h3>

<p>All the above are true for us as well &ndash; with some simplifications. We use Docker to run every process/application &ndash; for the benefits we have covered in other posts many times &ndash; and apart from Docker, our (or the customer’s) preferred OS and a few other helper/debugger things (such as <a href="https://registry.hub.docker.com/u/jpetazzo/nsenter/">nsenter</a>)
we are almost fine. We have made some PAM related fixes/contributions for Docker &ndash; and until they are not in the upstream we have built/derive from our base layer/containers &ndash; so with this and the actual containers included this is pretty much how a cloud base image looks like for us.</p>

<p>As usual we always automate everything &ndash; building custom cloud base images is part of the automation and our CI/CD process as well. For that we use <a href="http://www.ansible.com/home">Ansible</a> as the preferred IT automation tool. So the first step is to define your own <a href="http://docs.ansible.com/playbooks.html">playbook</a> to install everything on the virtual machine.</p>

<p>A simple playbook looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  - name: Install Docker
</span><span class='line'>    shell: curl -sL https://get.docker.io/ | sh
</span><span class='line'>    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
</span><span class='line'>
</span><span class='line'>  - name: Pull sequenceiq/ambari image
</span><span class='line'>    shell: docker pull sequenceiq/ambari:pam-fix
</span><span class='line'>
</span><span class='line'>  - name: Pull jpetazzo/nsenter image
</span><span class='line'>    shell: docker pull jpetazzo/nsenter
</span><span class='line'>
</span><span class='line'>  - name: Install bridge-utils
</span><span class='line'>    apt: name=bridge-utils state=latest
</span><span class='line'>    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
</span><span class='line'>
</span><span class='line'>  - name: install jq
</span><span class='line'>    shell: curl -o /usr/bin/jq http://stedolan.github.io/jq/download/linux64/jq && chmod +x /usr/bin/jq
</span></code></pre></td></tr></table></div></figure>


<p>Using Google cloud you have 2 choices:</p>

<ol>
<li> Create snapshots starting from a default image</li>
<li> Create a custom image</li>
</ol>


<h3>Image creation using snapshots</h3>

<p>We are using Debian as the host OS on Google Cloud, and have created a virtual machine using the default <a href="https://developers.google.com/compute/docs/operating-systems#backported_debian_7_wheezy">Debian</a> image. First thing first, you need to create a persistent disk:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gcloud compute disks create temporary-disk --zone ZONE</span></code></pre></td></tr></table></div></figure>


<p>Then create a virtual machine with the temporary-disk:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gcloud compute instances create example-instance \
</span><span class='line'>  --scopes storage-rw --image IMAGE \
</span><span class='line'>  --disk name=temporary-disk device-name=temporary-disk --zone ZONE</span></code></pre></td></tr></table></div></figure>


<p>And attach the disk to the google cloud instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gcloud compute instances attach-disk example-instance
</span><span class='line'>  --disk temporary-disk --device-name temporary-disk --zone ZONE</span></code></pre></td></tr></table></div></figure>


<p>When this is finished then you can <code>shh</code> to the <code>sample-instance</code>. You can now check your mounted volumes with this command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ls -l /dev/disk/by-id/google-*</span></code></pre></td></tr></table></div></figure>


<p>Now you need to create a folder which will contain your custom built image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo mkdir /mnt/tmp</span></code></pre></td></tr></table></div></figure>


<p>You have to format your partition before the image creation:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo /usr/share/google/safe_format_and_mount -m "mkfs.ext4 -F" /dev/sdb /mnt/tmp</span></code></pre></td></tr></table></div></figure>


<p>Now you can start building the image which will last about 10 minutes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo gcimagebundle -d /dev/sda -o /mnt/tmp/ --log_file=/tmp/imagecreation.log</span></code></pre></td></tr></table></div></figure>


<p>You have now an image in /tmp with a special hex number like <code>/tmp/HEX-NUMBER.image.tar.gz</code></p>

<p>Once you uploaded it to a Google bucket you are done, and ready to use it.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gsutil cp /mnt/tmp/IMAGE_NAME.image.tar.gz gs://BUCKET_NAME</span></code></pre></td></tr></table></div></figure>


<h3>Create a custom image &ndash; using your favorite OS</h3>

<p><a href="http://www.ubuntu.com/download/server">Ubuntu server 14.04</a> is many’s preferred Linux distribution &ndash; unluckily there is no default image using Ubuntu as the OS in the Google Cloud](<a href="https://developers.google.com/compute/docs/operating-systems">https://developers.google.com/compute/docs/operating-systems</a>). Luckily this is not that complicated &ndash; the process below works with any other OS as well. In order to start you should have <a href="https://www.virtualbox.org/">Virtualbox</a> installed. Download an Ubuntu server from <a href="http://www.ubuntu.com/server">Ubuntu’s</a> web page.
Install in into the <a href="https://www.virtualbox.org/">Virtualbox</a> box, start it and <code>ssh</code> into. Once you are inside you will have to install the <a href="https://developers.google.com/cloud/sdk/">Google Cloud SDK</a>. This is needed for the custom image, as contains some extra feature like <code>google-startup-scripts</code>. Remember that Ubuntu (and in general a few cloud providers) support <code>cloud-init</code> scripts, and this is why we need the Google Cloud SDK &ndash; as we ship these images to the <code>cloud</code>.</p>

<p>After the installation add the following kernel options into the <code>/etc/default/grub</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># to enable paravirtualization
</span><span class='line'>CONFIG_KVM_GUEST=y
</span><span class='line'>
</span><span class='line'># to enable the paravirtualized clock.
</span><span class='line'>CONFIG_KVM_CLOCK=y
</span><span class='line'>
</span><span class='line'># to enable paravirtualized PCI devices.
</span><span class='line'>CONFIG_VIRTIO_PCI=y
</span><span class='line'>
</span><span class='line'># to enable access to paravirtualized disks.
</span><span class='line'>CONFIG_SCSI_VIRTIO=y
</span><span class='line'>
</span><span class='line'># to enable access to the networking.
</span><span class='line'>CONFIG_VIRTIO_NET=y</span></code></pre></td></tr></table></div></figure>


<p>Now you are ready to prepare an <code>official</code> image into a tar file, by selecting the virtual box image file on your disk and convert it.
You can convert your <code>vmdk</code> file into the supported raw type by using:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>qemu-img convert -f vmdk -O raw VMDK_FILE_NAME.vmdk disk.img</span></code></pre></td></tr></table></div></figure>


<p>The .img file name has to be <code>disk.img</code>. After you have converted the image, you have to make a tar file:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar -Szcf &lt;image-tar-name&gt;.tar.gz disk.raw</span></code></pre></td></tr></table></div></figure>


<p>Same as before, you have to upload in to a Google Cloud Bucket:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gsutil cp &lt;image-tar-name&gt;.tar.gz gs://&lt;bucket-name&gt;</span></code></pre></td></tr></table></div></figure>


<p>Now you have an <code>official</code> image template but you have to create the image in Google Cloud:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gcutil addimage my-ubuntu gs://&lt;bucket-name&gt;/ubuntu_image.tar.gz</span></code></pre></td></tr></table></div></figure>


<p>Once this is done you have created your custom built Google Cloud image, and you are ready to start cloud instances using it. Let us know how it works for you, and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> for updates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark 1.1.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker/"/>
    <updated>2014-09-17T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/17/spark-1-1-0-docker</id>
    <content type="html"><![CDATA[<p>As you might be already familiar, we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well. For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>In this current post we’d like to help you to start with the <code>latest - 1.1.0</code> Spark release in minutes &ndash; using Docker. Docker and Spark are two technologies which are very <code>hyped</code> these days. At <a href="http://sequenceiq.com/">SequenceIQ</a> we use both quite a lot, thus we put together a Docker container and sharing it with the community.</p>

<p>The container’s code is available in our <a href="https://github.com/sequenceiq/docker-spark/blob/v1.1onHadoop-2.5.1/README.md">GitHub</a> repository.</p>

<h3>Pull the image from Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/spark:1.1.0</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Building the image</h2>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build --rm -t sequenceiq/spark:1.1.0 .</span></code></pre></td></tr></table></div></figure>


<h2>Running the image</h2>

<p>Once you have pulled or built the container, you are ready to start with Spark.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t -h sandbox sequenceiq/spark /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h3>Testing</h3>

<p>In order to check whether everything is OK, you can run one of the stock examples, coming with Spark. Check our previous blog posts and examples about Spark <a href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/">here</a> and <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">here</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'># run the spark shell
</span><span class='line'>./bin/spark-shell --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1
</span><span class='line'>
</span><span class='line'># execute the the following command which should return 1000
</span><span class='line'>scala&gt; sc.parallelize(1 to 1000).count()</span></code></pre></td></tr></table></div></figure>


<p>There are two deploy modes that can be used to launch Spark applications on YARN. In yarn-cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>

<p>Estimating Pi (yarn-cluster mode):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>
</span><span class='line'># execute the the following command which should write the "Pi is roughly 3.1418" into the logs
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar</span></code></pre></td></tr></table></div></figure>


<p>Estimating Pi (yarn-client mode):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>
</span><span class='line'># execute the the following command which should print the "Pi is roughly 3.1418" to the screen
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 ./lib/spark-examples-1.1.0-hadoop2.4.0.jar</span></code></pre></td></tr></table></div></figure>


<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.5.1 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/15/hadoop-2-5-1-docker/"/>
    <updated>2014-09-15T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/15/hadoop-2-5-1-docker</id>
    <content type="html"><![CDATA[<p>Following the release cycle of Hadoop, today we are releasing a new <code>2.5.1</code> version of our <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">Hadoop Docker container</a>. Up until today the container was only <code>CentOS</code> based, but during the last few months we got lots of requests to release a Hadoop container on <code>Ubuntu</code> as well. From now on we will have both released, supported and published to the official Docker repository. Enjoy.</p>

<h2>Centos</h2>

<h3>Build the image</h3>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker/tree/2.5.1">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker:2.5.1 .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h3>Pull the image</h3>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.5.1</span></code></pre></td></tr></table></div></figure>


<h3>Start a container</h3>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker:2.5.1 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Ubuntu</h2>

<h3>Build the image</h3>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/docker-hadoop-ubuntu/tree/2.5.1">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-ubuntu:2.5.1 .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h3>Pull the image</h3>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-ubuntu:2.5.1</span></code></pre></td></tr></table></div></figure>


<h3>Start a container</h3>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-ubuntu:2.5.1 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.1.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.5.0/view/files">Bintray repo</a>. Enjoy.</p>

<p>Should you have any questions let us know through our social channels as <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Drill on Docker - query as a service ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/11/apache-drill-docker/"/>
    <updated>2014-09-11T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/11/apache-drill-docker</id>
    <content type="html"><![CDATA[<p>As you might be already familiar, we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, etc in Docker containers &ndash; on bare metal and in the cloud as well. We have started to use (and contribute) to Docker quite a while ago, and beside the <code>mainstream</code> benefits of containers one feature was extremely appealing to us &ndash; <strong>the SOA way of DevOps</strong>. Before I go on and explore what we mean under this allow me to collect a few links for your reference (all open sourced under an <strong>Apache 2 license</strong>), in case you plan to use Hadoop in Docker containers.</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo dist. container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a> </td>
</tr>
</tbody>
</table>


<h2>Apache Drill at SequenceIQ</h2>

<p><a href="http://incubator.apache.org/drill/">Apache Drill</a> is an open source, low latency SQL query engine for Hadoop and NoSQL. It has many nice and interesting features, but one of the most interesting one (at least for us) is the <a href="https://cwiki.apache.org/confluence/display/DRILL/Storage+Plugin+Registration">storage plugin</a> and the tolerance/support for dynamic schemas. At <a href="http://sequenceiq.com/">SequenceIQ</a> the pre and post processed data ends up in different storage systems/layers. Obliviously we use HDFS, for low latency queries we use HBase and recently (with the emergence of Tez &ndash; which we consider the next big thing) we started to use Hive as well. Quite often there is a need to access the data from <code>legacy</code> systems &ndash; and more often we see <code>SQL</code> coming back in the picture. Just FYI, for SQL on HBase we are using <a href="http://phoenix.apache.org/">Apache Phoenix</a>, and of course we have released and open sourced a <a href="http://blog.sequenceiq.com/blog/2014/09/04/sql-on-hbase-with-apache-phoenix/">Docker container</a>.</p>

<p>As you see there are many storage systems use &ndash; and Drill helps us with aggregating these under one common <code>ANSI SQL syntax</code>. You can query data from HDFS, HBase, Hive, local or remote distributed file system &ndash; or write your own custom storage plugin.</p>

<!-- more -->


<h3>Lifecycle of a Drill query</h3>

<p>Let’s take a simple example (from the Drill samples), where we query a file, with a <code>WHERE</code> clause. Your statement is submitted in <code>Sqlline</code> &ndash; a very popular (used with our Phoenix container as well) Java interface which can talk to a JDBC driver. The <code>SELECT</code> statement is passed into <a href="http://optiq.incubator.apache.org/">Optiq</a>. Optiq is a library for query parsing and planning, and allows pluggable transformation rules. Optiq also has a cost-based query optimizer. At high level, based on the above the statements are converted into Drill <code>logical operators</code>, and form a Drill logical plan. This plan is then submitted into one <code>DrillBit service</code> &ndash; usually running on each datanode, to benefit on the data locality, during query execution. This logical plan is then transformed into a physical plan &ndash; a simple DAG  of physical operators &ndash; using a Drill’s <code>optimizer</code>. This physical plan is broken into a multi-level execution tree (hello MPP) that is executed by multiple DrillBits. The story goes on as there are statistics collected, endpoint affinities are checked (metadata based preferred endpoint selection) and the plan is broken in fragments, but at a high level this is the execution flow.
There are some interesting things going on under the hood which we can cover it one of the following posts &ndash; about writing our custom storage plugin.</p>

<h2>Apache Drill on Docker</h2>

<p>Now as you have a good overview about the capabilities of Drill, we’d like to expand on what we mean under <strong>SOA way of DevOps</strong>. Though Drill is a complex piece of software, essentially the provided service is extremely simple: <em>queries data</em>. We have created a <a href="https://registry.hub.docker.com/u/sequenceiq/drill/">Drill Docker</a> container and wrapped the <code>query</code> service inside. If you’d like to use Drill, the only thing you will have to do is to launch our Drill container &ndash; the <code>query service</code> is available <em>as a Service</em>. We have built the container in such a way that the data layer is separated from the <code>query service</code> &ndash; you can launch the container when and where you’d like to do, and attach the data using volumes. Once the data layer is attached, the only remaining thing is to let Drill know where to query &ndash; by either using one of the existing, or creating a new storage configuration.</p>

<h3>Pull the container</h3>

<p>The Drill container is available as a trusted build on Docker.io. You can get and start using it &ndash; the only prerequisite is to have Docker installed.</p>

<p><code>docker pull sequenceiq/drill</code></p>

<h3>Use the container</h3>

<p>Once the container is pulled you are ready to query your data by running:</p>

<p><code>docker run -it -v /data:/data sequenceiq/drill /etc/bootstrap.sh</code></p>

<p>Note that the <code>-v /data:/data</code> flag specifies that you are mounting your <code>/data</code> directory on the host into a <code>/data</code> directory inside the container. The files inside the directory will be available for Drill to query, by either using the default <code>dfs</code> storage plugin, or by a custom one. To check, or create a storage plugin or to access the Drill UI you should go to <code>http://CONTAINER_IP:8047</code>. You can find your container IP by using <code>docker inspect ID</code>.</p>

<p>In case you don&rsquo;t have any data, but would still like to explore Drill, start the contaier as:</p>

<p><code>docker run -it sequenceiq/drill /etc/bootstrap.sh</code></p>

<p>The sample data installed by default with Drill is available inside the container, thus you&rsquo;d be able to run all the Drill examples/tutorials.</p>

<h3>Drill Rest API</h3>

<p>Get Drillbit status: <code>http://localhost:8047/status</code>
Get all submitted queries: <code>http://localhost:8047/queries</code>
Get status of a given query:<code>http://localhost:8047/query/{QUERY_ID}</code></p>

<p>The next version of the container will be a fully distributed (based on our Hadoop container and Hazelcast) Apache Drill Docker container. Until then feel free to let us know how you <code>drill</code> and follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Schedulers demystified - Part 2: Fair]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/09/yarn-schedulers-demystified-part-2-fair/"/>
    <updated>2014-09-09T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/09/yarn-schedulers-demystified-part-2-fair</id>
    <content type="html"><![CDATA[<p>In our previous blog post we have been demystifying the <a href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/">Capacity scheduler internals</a> &ndash; as promised in this post is the Fair scheduler’s turn. You can check also our previous post to find out how fair is the Fair scheduler in real life <a href="http://blog.sequenceiq.com/blog/2014/08/16/fairplay/">here</a>.</p>

<p>You might ask why YARN schedulers are so important for us? Recently we have released and open sourced the industry&rsquo;s first SLA policy based autoscaling API for Hadoop clusters, called <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; and part of the project is based on schedulers, <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and our contribution to Apache YARN.</p>

<h2>The Fair Scheduler internals</h2>

<p>The FairScheduler&rsquo;s purpose is to assign resources to applications such that all apps get &ndash; on average &ndash; an equal share of resources over time.
By default the scheduler bases fairness decisions only on memory, but it can be configured otherwise. When only a single app is running
in the cluster it can take all the resources. When new apps are submitted resources that free up are assigned to the new apps,
so that each app eventually on gets roughly the same amount of resources. Queues can be weighted to determine the fraction of total
resources that each app should get.</p>

<h2>Configuration</h2>

<p>Although the CapacityScheduler is the default we can easily tell YARN to use the FairScheduler. In yarn-site.xml</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>      &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>      &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;/etc/hadoop/conf.empty/fair-scheduler.xml&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>The FairScheduler consists of 2 configuration files: scheduler-wide options can be placed into <code>yarn-site.xml</code> and queue settings in the
<code>allocation file</code> which must be in XML format. Click <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">here</a>
for a more detailed reference.</p>

<h3>Few things worth noting compared to CapacityScheduler regarding queues</h3>

<ul>
<li>Both CapacityScheduler and FairScheduler supports hierarchical queues and all queues descend from a queue named <code>root</code>.</li>
<li>Both uses a queue called <code>default</code> as well.</li>
<li>Applications can be submitted to leaf queues only.</li>
<li>Both CapacityScheduler and FairScheduler can create new queues at run time, the only difference is the how. In case of the CapacityScheduler
  the configuration file needed to be modified and we have to explicitly tell the ResourceManager to reload the configuration, while the
  FairScheduler does the same based on the queue placement policies which is less painful.</li>
<li>FairScheduler introduced scheduling policies which determines which job should get resources at each scheduling opportunity. The cool thing
  about this that besides the default ones (&ldquo;fifo&rdquo; &ldquo;fair&rdquo; &ldquo;drf&rdquo;) anyone can create new scheduling policies by extending the
  <code>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy</code> class and place it to the classpath.</li>
<li>FairScheduler allows different queue placement policies as mentioned earlier. These policies tell the scheduler where to place the incoming app
  among the queues. Placement can depend on users, groups or requested queue by the applications.</li>
<li>In FairScheduler applications can be submitted to non-existing queues if the <code>create</code> flag is set and it will create that queue, while the
  CapacityScheduler will instantly reject the submission.</li>
<li>From Hadoop 2.6.0 (<a href="https://issues.apache.org/jira/browse/YARN-1495">YARN-1495</a>) both schedulers will let users to manually move
  applications across queues.<br/>
  <code>Side note:</code> This feature allows us to re-prioritize and define SLAs on applications and place them to queues where they get the enforced
  resources. Our newly open sourced project <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> will add this
  capability for static clusters besides dynamic ones in the near future.</li>
</ul>


<!-- more -->


<h2>Messaging</h2>

<p>The event mechanism is the same as with CapacityScheduler &ndash; thus I&rsquo;m not going to take account on the events &ndash; and if you check the handler methods
(<a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L956">here</a>
and <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#L1134">here</a>)
you can notice that they look fairly the same.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">handle</span><span class="o">(</span><span class="n">SchedulerEvent</span> <span class="n">event</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">switch</span> <span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="na">getType</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_ADDED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">NodeAddedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">NodeAddedSchedulerEvent</span> <span class="n">nodeAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeAddedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="n">recoverContainersOnNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getContainerReports</span><span class="o">(),</span>
</span><span class='line'>          <span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_REMOVED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">NodeRemovedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">NodeRemovedSchedulerEvent</span> <span class="n">nodeRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">removeNode</span><span class="o">(</span><span class="n">nodeRemovedEvent</span><span class="o">.</span><span class="na">getRemovedRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_UPDATE:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">NodeUpdateSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">NodeUpdateSchedulerEvent</span> <span class="n">nodeUpdatedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeUpdateSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">nodeUpdate</span><span class="o">(</span><span class="n">nodeUpdatedEvent</span><span class="o">.</span><span class="na">getRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ADDED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">AppAddedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">AppAddedSchedulerEvent</span> <span class="n">appAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplication</span><span class="o">(</span><span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getApplicationId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getUser</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getIsAppRecovering</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_REMOVED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">AppRemovedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">AppRemovedSchedulerEvent</span> <span class="n">appRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">removeApplication</span><span class="o">(</span><span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getApplicationID</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getFinalState</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_ADDED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">AppAttemptAddedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">AppAttemptAddedSchedulerEvent</span> <span class="n">appAttemptAddedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplicationAttempt</span><span class="o">(</span><span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getTransferStateFromPreviousAttempt</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getIsAttemptRecovering</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_REMOVED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">AppAttemptRemovedSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">AppAttemptRemovedSchedulerEvent</span> <span class="n">appAttemptRemovedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptRemovedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">removeApplicationAttempt</span><span class="o">(</span>
</span><span class='line'>          <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptID</span><span class="o">(),</span>
</span><span class='line'>          <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getFinalAttemptState</span><span class="o">(),</span>
</span><span class='line'>          <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getKeepContainersAcrossAppAttempts</span><span class="o">());</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">CONTAINER_EXPIRED:</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!(</span><span class="n">event</span> <span class="k">instanceof</span> <span class="n">ContainerExpiredSchedulerEvent</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="s">&quot;Unexpected event type: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">);</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">ContainerExpiredSchedulerEvent</span> <span class="n">containerExpiredEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">ContainerExpiredSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">ContainerId</span> <span class="n">containerId</span> <span class="o">=</span> <span class="n">containerExpiredEvent</span><span class="o">.</span><span class="na">getContainerId</span><span class="o">();</span>
</span><span class='line'>      <span class="n">completedContainer</span><span class="o">(</span><span class="n">getRMContainer</span><span class="o">(</span><span class="n">containerId</span><span class="o">),</span>
</span><span class='line'>          <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">createAbnormalContainerStatus</span><span class="o">(</span>
</span><span class='line'>              <span class="n">containerId</span><span class="o">,</span>
</span><span class='line'>              <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">EXPIRED_CONTAINER</span><span class="o">),</span>
</span><span class='line'>          <span class="n">RMContainerEventType</span><span class="o">.</span><span class="na">EXPIRE</span><span class="o">);</span>
</span><span class='line'>      <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">default</span><span class="o">:</span>
</span><span class='line'>      <span class="n">LOG</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Unknown event arrived at FairScheduler: &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>NODE_ADDED &amp;&amp; NODE_REMOVED</h3>

<p>It&rsquo;s the same as in CapacityScheduler, adjusts the global resources based on whether a node joined or left the cluster.</p>

<h3>APP_ADDED</h3>

<p>Application submission is slightly different from CapacityScheduler (well not on client side as it&rsquo;s the same there), but because of
the queue placement policy. Administrators can define a <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementPolicy.java">QueuePlacementPolicy</a>
which will determine where to place the submitted application. A QueuePlacementPolicy stands from a list of <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java">QueuePlacementRules</a>.
These rules are ordered meaning that the first rule which can place the application into a queue will apply. If no rule can apply the
application submission will be rejected. Each rule accept a <code>create</code> argument in which case it&rsquo;s true the rule can create a queue if it is missing.
The following rules exist:</p>

<ul>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L124">User</a>:
  places the application into a queue with user&rsquo;s name e.g: root.chris</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L140">PrimaryGroup</a>:
  places the application into a queue with the user&rsquo;s primary group name e.g: root.hdfs</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L160">SecondaryGroupExistingQueue</a>:
  places the application into a queue with the user&rsquo;s secondary group name</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L188">NestedUserQueue</a>:
  places the application into a queue with the user&rsquo;s name under the queue returned by the nested rule</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L258">Specified</a>:
  places the application into a queue which was requested when submitted</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L282">Default</a>:
  places the application into the default queue</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementRule.java#L324">Reject</a>:
  it is a termination rule in the sequence of rules, if no rule applied before then it will reject the submission</li>
</ul>


<p>ACLs are also checked before creating and adding the application to the list of <code>SchedulerApplications</code> and updating the metrics.</p>

<h3>APP_REMOVED</h3>

<p>Simply stops the application and sets it&rsquo;s final state.</p>

<h3>APP_ATTEMPT_ADDED</h3>

<p>The analogy is the same with the CapacityScheduler that application attempts trigger the application to actually run. Based on the
allocation configuration mentioned above the <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/MaxRunningAppsEnforcer.java">MaxRunningAppsEnforcer</a>
will decide whether the app is placed into the <code>runnableApps</code> or the <code>nonRunnableApps</code> inside of the queue. <code>MaxRunningAppsEnforcer</code> also
keeps track of the runnable and non runnable apps per user. Attempt states are also transferred from one to another.</p>

<h3>APP_ATTEMPT_REMOVED</h3>

<p>Releases all the allocated, acquired, running containers (in case of <code>ApplicationMaster</code> restart the running containers won&rsquo;t get killed),
releases all reserved containers, cleans up pending requests and informs the queues. <code>MaxRunningAppsEnforcer</code> gets updated as well.</p>

<h3>NODE_UPDATE</h3>

<p>As we learned from CapacityScheduler <code>NodeUpdateSchedulerEvents</code> arrive every second. FairScheduler support asynchronous scheduling on a
different thread regardless of the <code>NodeManager's</code> <code>heartbeats</code> as well. We also learned the importance of the <code>Allocation</code> method which
issues the <code>ResourceRequests</code> of an application and in this case it does exactly the same as in case of CapacityScheduler. You can read
about the form of these requests there. At each node update the scheduler updates the capacities of the resources if it&rsquo;s changed, processes
the completed and newly launched containers, updates the metrics and tries to allocate resources to applications. Just like with CapacityScheduler
container reservation has the advantage thus it gets fulfilled first. If there is no reservation it tries to schedule in a queue which is
farthest below fair share. The scheduler first orders the queues and then the applications inside the queues using the configured
<a href="https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies">SchedulingPolicy</a>.
As I mentioned in the configuration section there are 3 default policies available:</p>

<ul>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java">FifoPolicy</a>
  (fifo) &ndash; Orders first by priorities and then by submission time.</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/DominantResourceFairnessPolicy.java">DominantResourceFairnessPolicy</a>
  (drf) &ndash; Orders by trying to equalize dominant resource usage.
  (dominant resource usage is the largest ratio of resource usage to capacity among the resource types it is using)</li>
<li><a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FairSharePolicy.java">FairSharePolicy</a>
  (fair) &ndash; Orders via weighted fair sharing. In addition, Schedulables below their min share get priority over those whose
  min share is met. Schedulables below their min share are compared by how far below it they are as a ratio. For example, if job A has 8
  out of a min share of 10 tasks and job B has 50 out of a min share of 100, then job B is scheduled next, because B is at 50% of its
  min share and A is at 80% of its min share. Schedulables above their min share are compared by (runningTasks / weight).</li>
</ul>


<p>SchedulingPolicies can be written and used by anyone without major investment to how to do it. All it takes is to extend a
<a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingPolicy.java">class</a>
and place the implementation to the classpath and restart the <code>ResourceManager</code>. Even though it&rsquo;s easy to do and it&rsquo;s not a major investment
the fairness will depend on it thus the effect will be major, so you should really consider it. After the decision of which application should
get resources first the game is pretty much the same as with the CapacityScheduler. First it tries to allocate container on a data local node
and after a delay on a rack local node and in the end falling back to an off switch node.</p>

<h3>CONTAINER_EXPIRED</h3>

<p>Cleans up the expired containers just like it would be a finished container.</p>

<h2>What&rsquo;s next?</h2>

<p>We might do a Part 3 post about the FIFOScheduler, though that&rsquo;s really straightforward &ndash; nevertheless, let us know if you&rsquo;d like to read about. As we have already mentioned, last week we released <a href="http://sequenceiq.com/periscope/">Periscope</a> &ndash; the industry’s first SLA policy based autoscaling API for Hadoop YARN &ndash; all these features we have blogged about are based on our contribution in Hadoop, YARN and Ambari -so stay tuned and follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> for updates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Ambari 1.7.0 early access]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/05/apache-ambari-1-7-0-ea/"/>
    <updated>2014-09-05T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/05/apache-ambari-1-7-0-ea</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use <a href="http://ambari.apache.org/">Apache Ambari</a> every day &ndash; it’s our tool to provision Hadoop clusters.</p>

<p>Beside that we are contributors to Ambari, we are so excited about the coming Apache Ambari 1.7.0 new <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30755705">features</a> that we could not help and put together an <strong>early access</strong> <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea">Ambari 1.7.0 Docker container</a>.</p>

<p>Give it a try, and provision an arbitrary number of Hadoop cluster on your laptop (or production environment), using our container and Ambari shell. Let us know how it works for you. Enjoy.</p>

<h3>Get the Docker container</h3>

<p>In case you don’t have Docker browse among our previous posts &ndash; we have a few posts about howto’s, examples and best practices in general for Docker and in particular about how to run the full Hadoop stack on Docker.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:1.7.0-ea</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p>Once you have the container you are almost ready to go &ndash; we always automate everything and <strong>over simplify</strong> Hadoop provisioning.</p>

<h3>Get ambari-functions</h3>

<p>Get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea/ambari-functions">file</a> from our GitHub.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari-170ea && . .amb</span></code></pre></td></tr></table></div></figure>


<h3>Create your cluster</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-deploy-cluster 4</span></code></pre></td></tr></table></div></figure>


<p><strong>Whaaat?</strong> No really, that’s it &ndash; we have just provisioned you a 4 node Hadoop cluster in less than 2 minutes. Docker, Apache Ambari and Ambari Shell combined is quite powerful, isn&rsquo;t it? You can always start playing with your desired services by changing the <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">blueprints</a> &ndash; the full Hadoop stack is supported.</p>

<p>If you’d like to play around and understand how this works check our previous blog posts &ndash; a good start is this first post about one of our contribution, the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari Shell</a>.</p>

<p>You have just seen how easy is to provision a Hadoop cluster on your laptop, if you’d like to see how we provision a Hadoop cluster in the cloud using the very same Docker image you can check our open source, cloud agnostic Hadoop as a Service API &ndash; <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a>. Last week we have released a project called <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; the industry&rsquo;s first open source autoscaling API for Hadoop.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL on HBase with Apache Phoenix]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/04/sql-on-hbase-with-apache-phoenix/"/>
    <updated>2014-09-04T12:24:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/04/sql-on-hbase-with-apache-phoenix</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use HBase to store large amounts of high velocity data and interact with them &ndash; many times we use native HBase interfaces but recently there was a need (internal and external) to access the data through an SQL interface.</p>

<h2>Introduction</h2>

<p>HBase is an open-source, distributed, versioned, non-relational database modeled after Google&rsquo;s Bigtable. It&rsquo;s designed to handle
billions of rows and millions of columns. However, using it as a relational database where you would store your data normalized,
split into multiple tables is not easy and most likely you will struggle with it as you would do in any other non-relational database.
Here comes <a href="http://phoenix.apache.org/">Apache Phoenix</a> in the picture. It&rsquo;s an SQL skin over HBase delivered as a
client-embedded JDBC driver targeting low latency queries. The project is in incubating state and under heavy development, but you
can already start embracing it.</p>

<h2>Installation</h2>

<p>Download the appropriate distribution from <a href="http://xenia.sote.hu/ftp/mirrors/www.apache.org/phoenix/">here</a>:</p>

<ul>
<li>Phoenix 3.x &ndash; HBase 0.94.x</li>
<li>Phoenix 4.x &ndash; HBase 0.98.1+</li>
</ul>


<p><em>Note the compatibilities between the HBase and Phoenix versions</em></p>

<p>Alternatively you can clone the <a href="https://github.com/apache/phoenix/tree/4.0">repository</a> and build it yourself (mvn clean install -DskipTests).
It should produce a jar file like this: phoenix-<code>version</code>-client.jar. Copy it to HBase&rsquo;s classpath (easiest way is to copy into
HBASE_HOME/lib). If you have multiple nodes it has to be there on every node. Restart the RegionServers and you are good to go. That&rsquo;s it?
Yes!</p>

<h2>Sample</h2>

<p>We&rsquo;ve pre-cooked a <a href="https://github.com/sequenceiq/phoenix-docker">Docker</a> image for you so you can follow this sample and play with it (the image is based on Hadoop 2.5, HBase 0.98.5, Phoenix 4.1.0):</p>

<h3>Normal launch</h3>

<p><code>docker run -it sequenceiq/phoenix:v4.1onHbase-0.98.5</code></p>

<h3>Alternative launch with sqlline</h3>

<p><code>docker run -it sequenceiq/phoenix:v4.1onHbase-0.98.5 /etc/bootstrap-phoenix.sh -sqlline</code></p>

<!-- more -->


<h3>Create tables</h3>

<p>The downloaded or built distribution&rsquo;s bin directory contains a pure-Java console based utility called sqlline.py. You can use this
to connect to HBase via the Phoenix JDBC driver. You need to specify the Zookeeper&rsquo;s QuorumPeer&rsquo;s address. If the default (2181) port is
used then type <em>sqlline.py localhost</em> (to quit type: !quit). Let&rsquo;s create two different tables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">CUSTOMERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="n">NAME</span> <span class="kt">VARCHAR</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">AGE</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">CITY</span> <span class="kt">CHAR</span><span class="p">(</span><span class="mi">25</span><span class="p">));</span>
</span><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">ORDERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="kt">DATE</span> <span class="kt">DATE</span><span class="p">,</span> <span class="n">CUSTOMER_ID</span> <span class="kt">INTEGER</span><span class="p">,</span> <span class="n">AMOUNT</span> <span class="kt">DOUBLE</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s worth checking which <a href="http://phoenix.apache.org/language/datatypes.html">datatypes</a> and
<a href="http://phoenix.apache.org/language/functions.html">functions</a> are currently supported. These tables will be translated into
HBase tables and the metadata is stored along with it and versioned, such that snapshot queries over prior versions will automatically
use the correct schema. You can check with HBase shell as <code>describe 'CUSTOMERS'</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="n">DESCRIPTION</span>                                                                                                                         <span class="n">ENABLED</span>
</span><span class='line'> <span class="s1">&#39;CUSTOMERS&#39;</span><span class="p">,</span> <span class="err">{</span><span class="n">TABLE_ATTRIBUTES</span> <span class="o">=&gt;</span> <span class="err">{</span><span class="n">coprocessor</span><span class="err">$</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">2</span> <span class="o">=&gt;</span> <span class="s1">&#39;|or true</span>
</span><span class='line'><span class="s1"> g.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">3</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.GroupedAggreg</span>
</span><span class='line'><span class="s1"> ateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">4</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">5</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apa</span>
</span><span class='line'><span class="s1"> che.phoenix.hbase.index.Indexer|1073741823|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.inde</span>
</span><span class='line'><span class="s1"> x.codec.class=org.apache.phoenix.index.PhoenixIndexCodec&#39;</span><span class="err">}</span><span class="p">,</span> <span class="err">{</span><span class="n">NAME</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">DATA_BLOCK_ENCODING</span> <span class="o">=&gt;</span> <span class="s1">&#39;FAST_DIFF&#39;</span><span class="p">,</span> <span class="n">BLOOMFILTER</span> <span class="o">=&gt;</span> <span class="s1">&#39;ROW&#39;</span>
</span><span class='line'> <span class="p">,</span> <span class="n">REPLICATION_SCOPE</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="n">COMPRESSION</span> <span class="o">=&gt;</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span> <span class="n">MIN_VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">TTL</span> <span class="o">=&gt;</span> <span class="s1">&#39;2147483647&#39;</span><span class="p">,</span> <span class="n">KEEP_DELETED_CELLS</span> <span class="o">=</span>
</span><span class='line'> <span class="o">&gt;</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">BLOCKSIZE</span> <span class="o">=&gt;</span> <span class="s1">&#39;65536&#39;</span><span class="p">,</span> <span class="n">IN_MEMORY</span> <span class="o">=&gt;</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">BLOCKCACHE</span> <span class="o">=&gt;</span> <span class="s1">&#39;true&#39;</span><span class="err">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see there are bunch of co-processors. Co-processors were introduced in version 0.92.0 to push arbitrary computation out
to the HBase nodes and run in parallel across all the RegionServers. There are two types of them: <code>observers</code> and <code>endpoints</code>.
Observers allow the cluster to behave differently during normal client operations. Endpoints allow you to extend the cluster’s
capabilities, exposing new operations to client applications. Phoenix uses them to translate the SQL queries to scans and that&rsquo;s
why it can operate so quickly. It is also possible to map an existing HBase table to a Phoenix table. In this case the binary
representation of the row key and key values must match one of the Phoenix data types.</p>

<h3>Load data</h3>

<p>After the tables are created fill them with data. For this purpose we&rsquo;ll use the <a href="http://www.jooq.org/">Jooq</a> library&rsquo;s fluent API.
The related sample project (Spring based) can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/phoenix-jooq">GitHub</a> repository. To connect you&rsquo;ll need Phoenix&rsquo;s
JDBC driver on your classpath (org.apache.phoenix.jdbc.PhoenixDriver). The url to connect to should be familiar as it uses the same Zookeeper QuorumPeer&rsquo;s address:
<code>jdbc:phoenix:localhost:2181</code>. Unfortunately Jooq&rsquo;s insert statement is not suitable for us since the JDBC driver only supports the
upsert statement so we cannot make use of the fluent API here.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">String</span> <span class="n">userSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into customers values (%d, &#39;%s&#39;, %d, &#39;%s&#39;)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">size</span><span class="o">()</span> <span class="o">-</span> <span class="mi">1</span><span class="o">))),</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">40</span><span class="o">)</span> <span class="o">+</span> <span class="mi">18</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">locales</span><span class="o">[</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">locales</span><span class="o">.</span><span class="na">length</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)].</span><span class="na">getDisplayCountry</span><span class="o">()));</span>
</span><span class='line'><span class="n">String</span> <span class="n">orderSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into orders values (%d, CURRENT_DATE(), %d, %d)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">1</span><span class="n">_000_000</span><span class="o">));</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">userSql</span><span class="o">);</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">orderSql</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Query</h3>

<p>On the generated data let&rsquo;s create queries:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">dslContext</span>
</span><span class='line'>          <span class="o">.</span><span class="na">select</span><span class="o">()</span>
</span><span class='line'>          <span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;customers&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;c&quot;</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;orders&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;o&quot;</span><span class="o">)).</span><span class="na">on</span><span class="o">(</span><span class="s">&quot;o.customer_id = c.id&quot;</span><span class="o">)</span>
</span><span class='line'>          <span class="o">.</span><span class="na">where</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;o.amount&quot;</span><span class="o">).</span><span class="na">lessThan</span><span class="o">(</span><span class="n">amount</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">orderBy</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;c.name&quot;</span><span class="o">).</span><span class="na">asc</span><span class="o">())</span>
</span><span class='line'>          <span class="o">.</span><span class="na">fetch</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>This query resulted the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">NAME</span>      <span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">AGE</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">CITY</span>     <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">DATE</span>    <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">CUSTOMER_ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">AMOUNT</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="n">Bogan</span><span class="o">,</span> <span class="n">Elias</span><span class="o">|</span>   <span class="mi">26</span><span class="o">|</span><span class="n">Japan</span>      <span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">976</span><span class="o">|</span>  <span class="mf">8664.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="n">Constrictor</span> <span class="o">|</span>   <span class="mi">29</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">827</span><span class="o">|</span>  <span class="mf">7856.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="n">Hardwire</span>    <span class="o">|</span>   <span class="mi">31</span><span class="o">|</span><span class="n">Tunisia</span>    <span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">672</span><span class="o">|</span>  <span class="mf">9292.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="n">Lady</span> <span class="n">Killer</span> <span class="o">|</span>   <span class="mi">37</span><span class="o">|</span><span class="n">Cyprus</span>     <span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">746</span><span class="o">|</span>  <span class="mf">1784.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="n">Lifeforce</span>   <span class="o">|</span>   <span class="mi">35</span><span class="o">|</span><span class="n">Switzerland</span><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">242</span><span class="o">|</span>  <span class="mf">5406.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="n">Topspin</span>     <span class="o">|</span>   <span class="mi">48</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">487</span><span class="o">|</span>  <span class="mf">6512.0</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span></code></pre></td></tr></table></div></figure>


<p>The same thing could&rsquo;ve been achieved with sqlline also.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">select</span> <span class="n">c</span><span class="p">.</span><span class="n">name</span> <span class="k">as</span> <span class="n">name</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="k">as</span> <span class="n">amount</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="kt">date</span> <span class="k">as</span> <span class="kt">date</span> <span class="k">from</span> <span class="n">customers</span> <span class="k">as</span> <span class="n">c</span> <span class="k">inner</span> <span class="k">join</span> <span class="n">orders</span> <span class="k">as</span> <span class="n">o</span> <span class="k">on</span> <span class="n">o</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="n">id</span> <span class="k">where</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Nested queries are not supported yet, but it will come soon.</p>

<h2>Summary</h2>

<p>As you saw it is pretty easy to get started with Phoenix both command line and programmatically. There are lots of lacking features, but
the contributors are dedicated and working hard to make this project moving forward. Next step? ORM for HBase? It is also ongoing.. :)</p>

<p>Follow up with <a href="https://www.linkedin.com/company/sequenceiq/">us</a> if you are interested in HBase and building an SQL interface on top.
Don&rsquo;t hesitate to contact us should you have any questions.</p>

<p><a href="http://sequenceiq.com/">SequenceIQ</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SLA policies for autoscaling Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope/"/>
    <updated>2014-09-01T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope</id>
    <content type="html"><![CDATA[<p>Last week we have <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">announced</a> and open sourced <a href="http://sequenceiq.com/periscope/">Periscope</a> &ndash; the industry’s first SLA policy based autoscaling API for Hadoop YARN clusters. In this post we’d like to come up with some examples, setting up alarms and attach scaling policies to your Hadoop cluster.</p>

<p>Periscope is built on existing (and coming/contributed by us) features provided by Apache Hadoop, YARN, Ambari, Docker containers and SequenceIQ’s <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Just FYI, <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> is our open source and cloud agnostic Hadoop as a Service API, built on Docker containers. While Periscope can attach scaling policies to <code>static</code> and <code>dynamic</code> clusters &ndash; in this post we’d like to emphasize Periscope’s capabilities while working with &mdash; `dynamic &ndash; cloud based Hadoop deployments  &ndash; such as Hadoop clusters deployed with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>SLAs policies are configured based on <code>alarms</code>, whereas an alarm is created based on <code>metrics</code> &ndash; these entities are explained below.</p>

<h2>Alarms</h2>

<p>An alarm watches a <code>metric</code> over a specified time period, and used by one or more action or scaling policy based on the value of the metric relative to a given threshold over the time period. A few of the supported <code>metrics</code> are listed below:</p>

<p>*<code>PENDING_CONTAINERS</code>&ndash; pending YARN containers</p>

<p>*<code>PENDING_APPLICATIONS</code> &ndash; pending/queued YARN applications</p>

<p>*<code>LOST_NODES</code> &ndash; cluster nodes lost</p>

<p>*<code>UNHEALTHY_NODES</code> &ndash; unhealthy cluster nodes</p>

<p>*<code>GLOBAL_RESOURCES</code> &ndash; global resources</p>

<!--more-->


<p>Measured <code>metrics</code> are compared with pre-configured values using operators. The <code>comparison operators</code> are: <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>LESS_OR_EQUAL_THAN</code>, <code>GREATER_OR_EQUAL_THAN</code>, <code>EQUALS</code>.
In order to avoid reacting for sudden spikes in the system and apply policies only in case of a sustained system stress, <code>alarms</code> have to be sustained over a <code>period</code> of time.  The <code>period</code> specifies the time period in minutes during the alarm has to be sustained. Also a <code>threshold</code> can be configured, which specifies the variance applied by the operator for the selected <code>metric</code>.</p>

<p>For the <code>alarm</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/alarms">API</a> documentation. Alarms can issue <code>notifications</code> as well &ndash; for example if a metric is reached for the configured time and threshold a notification event is raised &ndash; in the given example below this notification is an email.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># set metric alarms
</span><span class='line'>curl -X POST -H "Content-Type: application/json" -d '{"alarms":[{"alarmName":"pendingContainerHigh","description":"Number of pending containers is high","metric":"PENDING_CONTAINERS","threshold":10,"comparisonOperator":"GREATER_THAN","period":1},{"alarmName":"freeGlobalResourcesRateLow","description":"Low free global resource rate","metric":"GLOBAL_RESOURCES","threshold":1,"comparisonOperator":"EQUALS","period":1,"notifications":[{"target":[“mick.fanning@aspworldtour.com"],"notificationType":"EMAIL"}]}]}' localhost:8081/clusters/1/alarms | jq .
</span><span class='line'>curl -X PUT -H "Content-Type: application/json" -d '{"alarmName":"unhealthyNodesHigh","description":"Number of unhealthy nodes is high","metric":"UNHEALTHY_NODES","threshold":5,"comparisonOperator":"GREATER_OR_EQUAL_THAN","period":5}' localhost:8081/clusters/1/alarms | jq .</span></code></pre></td></tr></table></div></figure>


<h2>SLA scaling policies</h2>

<p>Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application.  When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.
Periscope will do the heavy lifting and based on the alarms and the scaling policy linked to them it executes the associated policy. By default a fully configured and running <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> cluster contains no SLA policies.  An SLA scaling policy can contain multiple <code>alarms</code>.</p>

<p>As an alarm is triggered a <code>scalingAdjustment</code> is applied, however to keep the cluster size within boundaries a <code>minSize</code> and <code>maxSize</code> is attached to the cluster &ndash; thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a <code>cooldown</code> period (minutes) &ndash; though an alarm is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered policies are applied in order.</p>

<p>Hosts can be added or removed from specific <code>hostgroups</code>. Periscope and Cloudbreak uses Apache Ambari to provision a Hadoop cluster. Ambari host groups are a set of machines with the same Hadoop “components” installed. You can set up a cluster having different hostgroups &ndash; and run different services, thus having a heterogenous cluster.</p>

<p>In the following example we downscale a cluster when the unused resources are high.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># set scaling policy
</span><span class='line'>curl -X POST -H "Content-Type: application/json" -d '{"minSize":2,"maxSize":10,"cooldown":30,"scalingPolicies":[{"name":"downScaleWhenHighResource","adjustmentType":"NODE_COUNT","scalingAdjustment":2,"hostGroup":"slave_1","alarmId":"101"},{"name":"upScaleWhenHighPendingContainers","adjustmentType":"PERCENTAGE","scalingAdjustment":40,"hostGroup":"slave_1","alarmId":"100"}]}' localhost:8081/clusters/1/policies | jq .</span></code></pre></td></tr></table></div></figure>


<p>For the <code>policy</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/scaling-policy">API</a> documentation.</p>

<p>Let us know how Periscope works for you &ndash; and for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infrastructure management with CloudFormation]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier/"/>
    <updated>2014-08-29T14:13:06+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier</id>
    <content type="html"><![CDATA[<p>Our Hadoop as a Service solution, <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> integrates with multiple cloud providers to deploy Hadoop clusters in the cloud. It means that every time a cluster is requested, Cloudbreak goes to the selected cloud provider and creates a new, separated infrastructure through the provider’s API. Building this infrastructure can be a real pain and can cause a lot of problems &ndash; it involves a lot of API calls, the polling of created building blocks, the management of failures and the necessary rollbacks to name a few. With the help of <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation</a> we were able to avoid most of these problems when integrating AWS in Cloudbreak.</p>

<h3>Problems with the traditional approach</h3>

<p>When Cloudbreak creates a Hadoop cluster it should first create the underlying infrastructure on the cloud provider. The building blocks are a bit different on every provider, the following resources are created on AWS:</p>

<ul>
<li>a virtual private cloud (VPC)</li>
<li>a subnet</li>
<li>an internet gateway</li>
<li>a route table</li>
<li>an auto scaling group and its launch configuration</li>
<li>a security group</li>
</ul>


<p>Although AWS has a pretty good API and great SDKs to communicate with it, we needed to deal with the above described problems if we would like to create all of these elements one by one through the Java SDK. The code would start with something like this with the creation of the VPC:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">AmazonEC2Client</span> <span class="n">amazonEC2Client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">AmazonEC2Client</span><span class="o">(</span><span class="n">basicSessionCredentials</span><span class="o">);</span>
</span><span class='line'><span class="n">amazonEC2Client</span><span class="o">.</span><span class="na">setRegion</span><span class="o">(</span><span class="n">region</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">CreateVpcRequest</span> <span class="n">vpcRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CreateVpcRequest</span><span class="o">().</span><span class="na">withCidrBlock</span><span class="o">(</span><span class="mf">10.0</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">24</span><span class="o">);</span>
</span><span class='line'><span class="n">CreateVpcResponse</span> <span class="n">vpcResponse</span> <span class="o">=</span> <span class="n">amazonEC2Client</span><span class="o">.</span><span class="na">createVpc</span><span class="o">(</span><span class="n">createVpcRequest</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//poll vpc creation until it’s state is available</span>
</span><span class='line'><span class="n">waitForVPC</span><span class="o">(</span><span class="n">amazonEC2Client</span><span class="o">,</span> <span class="n">vpcResponse</span><span class="o">.</span><span class="na">getVpc</span><span class="o">());</span>
</span><span class='line'>
</span><span class='line'><span class="n">ModifyVpcAttributeRequest</span> <span class="n">modifyVpcRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ModifyVpcAttributeRequest</span><span class="o">().</span><span class="na">withEnableDnsHostnames</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">withEnableDnsSupport</span><span class="o">(</span><span class="kc">true</span><span class="o">);</span>
</span><span class='line'><span class="n">amazonEC2Cient</span><span class="o">.</span><span class="na">modifyVpcAttribute</span><span class="o">(</span><span class="n">modifyVpcRequest</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p>The above code is only a taste of the whole thing. The VPC is one of the most simple resources with very few attributes to set. Also the polling of the creation process isn’t detailed here as well as failure handling. In addition the different resources would be scattered around the code making it impossible to have an overview of the whole stack and making it much harder to find bugs or to modify some attributes. With CloudFormation all of the above problems can be solved very easily.</p>

<h3>Introduction to CloudFormation</h3>

<p>According to the <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation documentation</a> it was designed to create and manage a collection of related AWS resources easily and provisioning and updating them in an orderly and predictable fashion. What it really means is that the resources can be described declaratively in a JSON document (a <em>template</em>) and the whole <em>stack</em> can be created/updated/deleted with a simple API call. AWS also handles failures, and rollbacks the whole stack if something goes wrong. Furthermore it is able to send notifications to <em>SNS topics</em> when some event occurs (e.g.: a resource creation started or the resource is completed), making the polling of resource creations unnecessary.</p>

<h3>Template structure</h3>

<p>We don’t want to give a detailed introduction on how the structure of a CloudFormation template look like, the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">AWS documentation</a> covers it really well and there are also a lot of <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-services-us-west-2.html">samples</a>.
Instead we’re trying to focus on the advantages that CloudFormation gave us while using it, so let’s jump in the middle and start with a simple example. The declaration of a VPC in a template looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;Resources&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;MY_VPC&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::EC2::VPC&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Properties&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;CidrBlock&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;10.0.0.0/16&quot;</span> <span class="p">},</span>
</span><span class='line'>      <span class="nt">&quot;EnableDnsSupport&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;EnableDnsHostnames&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;Tags&quot;</span> <span class="p">:</span> <span class="p">[</span>
</span><span class='line'>        <span class="p">{</span> <span class="nt">&quot;Key&quot;</span> <span class="p">:</span> <span class="s2">&quot;Application&quot;</span><span class="p">,</span> <span class="nt">&quot;Value&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::StackId&quot;</span> <span class="p">}</span> <span class="p">},</span>
</span><span class='line'>        <span class="p">{</span> <span class="nt">&quot;Key&quot;</span> <span class="p">:</span> <span class="s2">&quot;Network&quot;</span><span class="p">,</span> <span class="nt">&quot;Value&quot;</span> <span class="p">:</span> <span class="s2">&quot;Public&quot;</span> <span class="p">}</span>
</span><span class='line'>      <span class="p">]</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The JSON syntax can be a bit complicated sometimes, especially when dealing with a lot of references to other properties with the <em>&ldquo;Ref&rdquo;</em> keyword or some other built-in CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html">functions</a>, but it is much clearer than the Java code above.
Other than resources, there are other parts of a CloudFormation template (<em>Conditions</em>, <em>Mappings</em>, <em>Outputs</em>, <em>Intrinsic Functions</em>) but here we will cover only one more: <em>Parameters</em>.</p>

<p>Declaring a parameter means that there is no hard-coded value for a given attribute, rather it is given to the template when creating the stack. If you’d like to have an EC2 Instance  in your template but you don’t want to hardcode its type, you can have a parameter like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;Parameters&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;InstanceType&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;Description&quot;</span> <span class="p">:</span> <span class="s2">&quot;EC2 instance type&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;String&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Default&quot;</span> <span class="p">:</span> <span class="s2">&quot;m3.medium&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;AllowedValues&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;m3.medium&quot;</span><span class="p">,</span><span class="s2">&quot;m3.large&quot;</span><span class="p">,</span><span class="s2">&quot;m3.xlarge&quot;</span><span class="p">,</span><span class="s2">&quot;m3.2xlarge&quot;</span><span class="p">],</span>
</span><span class='line'>    <span class="nt">&quot;ConstraintDescription&quot;</span> <span class="p">:</span> <span class="s2">&quot;must be a valid EC2 instance type.&quot;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>After it’s declared, you can reference it from a resource with the <em>Ref</em> keyword:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;EC2Instance&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::EC2::Instance&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;Properties&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;SecurityGroups&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;InstanceSecurityGroup&quot;</span> <span class="p">}</span> <span class="p">],</span>
</span><span class='line'>    <span class="nt">&quot;InstanceType&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;InstanceType&quot;</span> <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;KeyName&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;KeyName&quot;</span> <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;ImageId&quot;</span> <span class="p">:</span> <span class="s2">&quot;ami-123456&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;EbsOptimized&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can reference not only parameters, but other resources as well. In the above code example there is a reference to <em>InstanceSecurityGroup</em> that is an <em>AWS::EC2::SecurityGroup</em> type resource and that is declared in an other part of the template.</p>

<h3>Creating the stack</h3>

<p>So we’ve declared a few resources, how can we tell AWS to create the stack? Let’s see how it looks like with the Java SDK (two parameters are passed to the template):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">CreateStackRequest</span> <span class="n">createStackRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CreateStackRequest</span><span class="o">()</span>
</span><span class='line'>    <span class="o">.</span><span class="na">withStackName</span><span class="o">(</span><span class="err">“</span><span class="n">MyCFStack</span><span class="s">&quot;)</span>
</span><span class='line'><span class="s">    .withTemplateBody(templateAsString)</span>
</span><span class='line'><span class="s">    .withNotificationARNs(notificationTopicARN)</span>
</span><span class='line'><span class="s">    .withParameters(</span>
</span><span class='line'><span class="s">        new Parameter().withParameterKey(&quot;</span><span class="n">InstanceCount</span><span class="s">&quot;).withParameterValue(“3&quot;</span><span class="o">),</span>
</span><span class='line'>        <span class="k">new</span> <span class="nf">Parameter</span><span class="o">().</span><span class="na">withParameterKey</span><span class="o">(</span><span class="s">&quot;InstanceType&quot;</span><span class="o">).</span><span class="na">withParameterValue</span><span class="o">(</span><span class="err">“</span><span class="n">m3</span><span class="o">.</span><span class="na">large</span><span class="err">&quot;</span><span class="o">));</span>
</span><span class='line'>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">createStack</span><span class="o">(</span><span class="n">createStackRequest</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that’s it. It’s every code that should written in Java to create the complete stack. It is pretty straightforward, the only thing that needs to be explained is the <em>notification ARN</em> part. It is the identifier of an <em>SNS topic</em> and it is detailed below.</p>

<h3>Callbacks</h3>

<p>CloudFormation is able to send notifications to SNS <em>topics</em> when an event occurs. An event is when a resource creation is started, finished or failed (and the same with delete). SNS is Amazon’s Simple Notification Service that enables endpoints to subscribe to a topic, and when a message is sent to a topic every subscriber receives that message. AWS supports a lot of endpoint types. It can send notifications by email or text message, to Amazon Simple Queue Service (SQS) queues, or to any HTTP/HTTPS endpoint. In the Cloudbreak project we’re using HTTP endpoints as callback URLs. We’re also creating topics and subscriptions from code but that could fill up another full blog post.</p>

<p>If you just like to try SNS, you can create a topic and a subscription from the AWS Console. After you have a confirmed subscription of an HTTP endpoint (e.g.: <em>example.com/sns</em>), you can very easily create an HTTP endpoint in Java (with some help from <a href="http://spring.io/">Spring</a>):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@RequestMapping</span><span class="o">(</span><span class="n">value</span><span class="o">=</span><span class="s">&quot;sns&quot;</span><span class="o">,</span> <span class="n">method</span> <span class="o">=</span> <span class="n">RequestMethod</span><span class="o">.</span><span class="na">POST</span><span class="o">)</span>
</span><span class='line'><span class="nd">@ResponseBody</span>
</span><span class='line'><span class="kd">public</span> <span class="n">ResponseEntity</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="nf">receiveSnsMessage</span><span class="o">(</span><span class="nd">@RequestBody</span> <span class="n">String</span> <span class="n">request</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// parse and handle request</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>For a more detailed example see the <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/controller/AmazonSnsController.java">controller class</a> in Cloudbreak.
So every time a CloudFormation stack event occurs, Cloudbreak receives a message that is parsed and handled correctly &ndash; there is no need to poll the creation of resources and dealing with timeouts.</p>

<h3>Failures and rollbacks</h3>

<p>It is always possible that something will go wrong when creating a stack with a lot of resources. With the traditional approach you must keep track of the resources that were created and you will have to implement some rollback logic that gets called when something unexpected happens and that rolls back the already created elements somehow. With CloudFormation these things are completely done by AWS.</p>

<p>The resources in the stack are tracked so the only thing you have to save is the identifier of the stack. If one of the resources fails to be created AWS rolls back every other resource and puts the stack in <em>ROLLBACK_COMPLETED</em> state. It also sends the failure message to the SNS topic with the exact cause of the failure.
The same is true if you’d like to delete the stack. The only call that you will have to send to the AWS API is the deletion of the stack (very similar to the creation in Java). CloudFormation will delete every resource one by one and will take care of failures.</p>

<h3>Notes</h3>

<p>The template we used in Cloudbreak is available <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/templates/aws-cf-stack.ftl">here</a>. It is not a pure CloudFormation template because of some limitations &ndash; the number of attached volumes cannot be specified dynamically and it is not possible to specify it as a parameter if spot priced instances are needed or not &ndash; we ended up generating the template with Freemarker.</p>

<h3>Terraform</h3>

<p>The <a href="http://www.hashicorp.com/products">company</a> that brought us <a href="http://www.vagrantup.com/">Vagrant</a>, <a href="http://www.packer.io/">Packer</a> and a few more useful things has recently announced a new project named <a href="http://www.terraform.io/intro/index.html">Terraform</a>. Terraform is inspired by tools like CloudFormation or <a href="https://wiki.openstack.org/wiki/Heat">OpenStack’s Heat</a>, but goes further as it supports multiple cloud platforms and their services can also be combined. If you’re interested in managing infrastructure from code and configuration you should check out that project too, we’ll keep an eye on it for sure.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Periscope - autoscaling for Hadoop YARN]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/"/>
    <updated>2014-08-27T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope</id>
    <content type="html"><![CDATA[<p><em>Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa&rsquo;s famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.</em></p>

<p><em>Periscope brings QoS and autoscaling to Hadoop YARN. Built on cloud resource management and YARN schedulers, allows to associate SLA policies to applications.</em></p>

<p>After the very positive reception of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; the first open source and cloud agnostic Hadoop as a Service API &ndash; today we are releasing the <code>public beta</code> version of our open source <strong>SLA policy based autoscaling API</strong> for Hadoop YARN clusters.</p>

<h2>Overview</h2>

<p>The purpose of Periscope is to bring QoS and autoscaling to a multi-tenant Hadoop YARN cluster, while allowing to apply SLA policies to individual applications.
At <a href="http://sequenceiq.com">SequenceIQ</a> working with multi-tenant Hadoop clusters for quite a while, we have always seen the same frustration and fight for resource between users.
The <strong>FairScheduler</strong> was partially solving this problem &ndash; bringing in fairness based on the notion of <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">Dominant Resource Fairness</a>.
With the emergence of Hadoop 2 YARN and the <strong>CapacityScheduler</strong> we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for predictable workloads, it often needs proper design ahead.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.</p>

<p>Periscope was designed around the idea of <code>autoscaling</code> clusters &ndash; without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.</p>

<!--more-->


<h2>How it works</h2>

<p>Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source <a href="https://github.com/sequenceiq/yarn-monitoring">monitoring project</a>, based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari&rsquo;s Nagios/Ganglia &ndash; and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health &ndash; they might not necessarily help on making decisions when applying different SLA policies on a multi-tenant cluster.
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.</p>

<p>Periscope works with two types of Hadoop clusters: <code>static</code> and <code>dynamic</code>. Periscope does not require any pre-installation &ndash; the only thing it requires is to be <code>attached</code> to an Ambari server&rsquo;s REST API.</p>

<h2>Clusters</h2>

<h3>Static clusters</h3>

<p>From Periscope point of view we consider a cluster <code>static</code> when the cluster capacity can&rsquo;t be increased horizontally.
This means that the hardware resources are already given &ndash; and the throughput can&rsquo;t be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Attempts</em> to enforce time based SLA (execution time, finish by, finish between, recurring)</li>
<li> <em>Attempts</em> to enforce guaranteed cluster capacity requests ( x % of the resources)</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>Autoscaling clusters</h3>

<p>From Periscope point of view we consider a cluster <code>dynamic</code> when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed on the fly &ndash; thus the cluster’s throughput can be increased or decreased based on the cluster load and scheduled applications.
Periscope works with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to add or remove nodes from the cluster based on the SLA policies and thus continuously provide a high <em>quality of service</em> for the multi-tenand Hadoop cluster.
Just to refresh memories &ndash; <a href="http://sequenceiq.com/products.html">Cloudbreak</a> is <a href="http://sequenceiq.com">SequenceIQ&rsquo;s</a> open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Enforce</em> time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput</li>
<li> Smart decommissioning &ndash; avoids HDFS storms, keeps <code>paid</code> nodes alive till the last minute</li>
<li> <em>Enforce</em> guaranteed cluster capacity requests ( x % of the resources)</li>
<li> <em>Private</em> cluster requests &ndash; supports provisioning of short lived private clusters with the possibility to merge them.</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>High level technical details</h3>

<p>When we have started to work on Periscope we checked different solutions &ndash; and we quickly realized that there are no any open source solutions available.
Apache YARN in general, and the scheduler API&rsquo;s in particular have solved few of the issues we had &ndash; and they have certainly bring some level of SLA to Hadoop.
At <a href="https://sequenceiq.com">SequenceIQ</a> we run all our different applications on YARN &ndash; and when we decided to create a heuristic scheduler we knew from very beginning that it has to be built on the functionality given by YARN.
In order to create Periscope we had to contribute code to YARN, Hadoop and Ambari &ndash; and were trying to add all the low level features directly into the YARN codebase.
Periscope has a <a href="http://docs.periscope.apiary.io/">REST API</a> and supports pluggable SLA policies.
We will follow up with technical details in coming blog posts, so make sure you subscribe to on of our social channels.</p>

<h3>Resources</h3>

<p>Get the code : <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></p>

<p>Documentation: <a href="http://sequenceiq.com/periscope">http://sequenceiq.com/periscope</a></p>

<p>API documentation: <a href="http://docs.periscope.apiary.io/">http://docs.periscope.apiary.io/</a></p>

<h3>What&rsquo;s next, etc</h3>

<p>This is the first <code>public beta</code> release of Periscope made available on our <a href="https://github.com/sequenceiq/periscope">GitHub</a> page.
While we are already using this internally we would like the community to help us battle test it, let us know if you find issues or raise feature requests. We are always happy to help.</p>

<p>Further releases will bring tighter integration with Ambari (especially around cluster resources), an enhanced (or potentially new) YARN scheduler and a Machine learning based job classification model.</p>

<p>We would like to say a big <em>thank you</em> for the YARN team &ndash; this effort would have not been possible without their contribution. Also we would like to thank them by supporting us with our contributions as well.
At SequenceIQ we are 100% committed to open source &ndash; and releasing Periscope under an <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2 licence</a> was never a question.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Submit a Spark job to YARN from code]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/"/>
    <updated>2014-08-22T08:09:28+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java</id>
    <content type="html"><![CDATA[<p>In our previous Apache Spark related post we showed you how to write a simple machine learning job. In this post we’d like to show you how to submit a Spark job from code. At SequenceIQ we submit jobs to different clusters &ndash; based on load, customer profile, associated SLAs, etc. Doing this the <code>documented</code> way was cumbersome so we needed a way to submit Spark jobs (and in general all of our jobs running in a YARN cluster) from code. Also due to the <code>dynamic</code> clusters, and changing job configurations we can’t use hardcoded parameters &ndash; in a previous <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">blog post</a> we highlighted how are we doing all these.</p>

<h2>Business as usual</h2>

<p>Basically as you from the <a href="https://spark.apache.org/docs/1.0.1/submitting-applications.html">Spark documentation</a>, you have to use the <a href="https://github.com/apache/spark/blob/master/bin/spark-submit">spark-submit</a> script to submit a job. In nutshell SparkSubmit is called
by the <a href="https://github.com/apache/spark/blob/master/bin/spark-class">spark-class</a> script with a lots of decorated arguments. In our example we examine only the YARN part of the submissions.
As you can see in <a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala">SparkSubmit.scala</a> the YARN <a href="https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala">Client</a> is loaded and its main method invoked (based on the arguments of the script).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// If we&#39;re deploying into YARN, use yarn.Client as a wrapper around the user class</span>
</span><span class='line'><span class="k">if</span> <span class="o">(!</span><span class="n">deployOnCluster</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">childMainClass</span> <span class="k">=</span> <span class="n">args</span><span class="o">.</span><span class="n">mainClass</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">isUserJar</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">childClasspath</span> <span class="o">+=</span> <span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">clusterManager</span> <span class="o">==</span> <span class="nc">YARN</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">childMainClass</span> <span class="k">=</span> <span class="s">&quot;org.apache.spark.deploy.yarn.Client&quot;</span>
</span><span class='line'>  <span class="n">childArgs</span> <span class="o">+=</span> <span class="o">(</span><span class="s">&quot;--jar&quot;</span><span class="o">,</span> <span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span><span class="o">)</span>
</span><span class='line'>  <span class="n">childArgs</span> <span class="o">+=</span> <span class="o">(</span><span class="s">&quot;--class&quot;</span><span class="o">,</span> <span class="n">args</span><span class="o">.</span><span class="n">mainClass</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="c1">// Here we invoke the main method of the Client</span>
</span><span class='line'><span class="k">val</span> <span class="n">mainClass</span> <span class="k">=</span> <span class="nc">Class</span><span class="o">.</span><span class="n">forName</span><span class="o">(</span><span class="n">childMainClass</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="n">loader</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">mainMethod</span> <span class="k">=</span> <span class="n">mainClass</span><span class="o">.</span><span class="n">getMethod</span><span class="o">(</span><span class="s">&quot;main&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="mi">0</span><span class="o">).</span><span class="n">getClass</span><span class="o">)</span>
</span><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">mainMethod</span><span class="o">.</span><span class="n">invoke</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="n">childArgs</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">case</span> <span class="n">e</span><span class="k">:</span> <span class="kt">InvocationTargetException</span> <span class="o">=&gt;</span> <span class="n">e</span><span class="o">.</span><span class="n">getCause</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="n">cause</span><span class="k">:</span> <span class="kt">Throwable</span> <span class="o">=&gt;</span> <span class="k">throw</span> <span class="n">cause</span>
</span><span class='line'>    <span class="k">case</span> <span class="kc">null</span> <span class="k">=&gt;</span> <span class="k">throw</span> <span class="n">e</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>It’s a pretty straightforward way to submit a Spark job to a YARN cluster, though you will need to change manually the parameters which as passed as arguments.</p>

<!--more-->


<h2>Submitting the job from Java code</h2>

<p>In case if you would like to submit a job to YARN from Java code, you can just simply use this Client class directly in your application.
(but you have to make sure that every environment variable what you will need is set properly).</p>

<h3>Passing Configuration object</h3>

<p>In the main method the org.apache.hadoop.conf.Configuration object is not passed to the Client class. A <code>Configuration</code> is created explicitly in the constructor, which is actually okay (then client configurations are loaded from $HADOOP_CONF_DIR/core-site.xml and $HADOOP_CONF_DIR/yarn-site.xml).
But what if you want to use (for example) an <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">Ambari Configuration Service</a> for retrieve your configuration, instead of using hardcoded ones?</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="o">...</span> <span class="c1">// Client class - constructor</span>
</span><span class='line'>  <span class="k">def</span> <span class="k">this</span><span class="o">(</span><span class="n">clientArgs</span><span class="k">:</span> <span class="kt">ClientArguments</span><span class="o">,</span> <span class="n">spConf</span><span class="k">:</span> <span class="kt">SparkConf</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>    <span class="k">this</span><span class="o">(</span><span class="n">clientArgs</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">(),</span> <span class="n">spConf</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="o">...</span> <span class="c1">// Client object - main method</span>
</span><span class='line'><span class="nc">System</span><span class="o">.</span><span class="n">setProperty</span><span class="o">(</span><span class="s">&quot;SPARK_YARN_MODE&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">args</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClientArguments</span><span class="o">(</span><span class="n">argStrings</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">)</span>
</span><span class='line'>  <span class="k">new</span> <span class="nc">Client</span><span class="o">(</span><span class="n">args</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">).</span><span class="n">run</span><span class="o">()</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">case</span> <span class="n">e</span><span class="k">:</span> <span class="kt">Exception</span> <span class="o">=&gt;</span> <span class="o">{</span>
</span><span class='line'>    <span class="nc">Console</span><span class="o">.</span><span class="n">err</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">getMessage</span><span class="o">)</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">exit</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="nc">System</span><span class="o">.</span><span class="n">exit</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Fortunately, the configuration can be passed here (there is a <code>Configuration</code> field in the Client), but you have to write your own main method.</p>

<h3>Code example</h3>

<p>In our example we also use the 2 client XMLs as configuration (for demonstration purposes only), the main difference here is that we read the properties from the XMLs and filling them in the Configuration. Then we pass the Configuration object to the Client (which is directly invoked here).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">config</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
</span><span class='line'>    <span class="n">fillProperties</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">getPropXmlAsMap</span><span class="o">(</span><span class="s">&quot;config/core-site.xml&quot;</span><span class="o">))</span>
</span><span class='line'>    <span class="n">fillProperties</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">getPropXmlAsMap</span><span class="o">(</span><span class="s">&quot;config/yarn-site.xml&quot;</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">setProperty</span><span class="o">(</span><span class="s">&quot;SPARK_YARN_MODE&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">cArgs</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClientArguments</span><span class="o">(</span><span class="n">args</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">new</span> <span class="nc">Client</span><span class="o">(</span><span class="n">cArgs</span><span class="o">,</span> <span class="n">config</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">).</span><span class="n">run</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To build the project use this command from the spark-submit directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>After building it you find the required jars in spark-submit-runner/build/libs (<code>uberjar</code> with all required dependencies) and spark-submit-app/build/libs. Put them in the same directory (do this also with this <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit/spark-submit-runner/src/main/resources">config folder</a> too). After that run this command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>java -cp spark-submit-runner-1.0.jar com.sequenceuq.spark.SparkRunner <span class="se">\</span>
</span><span class='line'>  --jar spark-submit-app-1.0.jar <span class="se">\</span>
</span><span class='line'>  --class com.sequenceiq.spark.Main <span class="se">\</span>
</span><span class='line'>  --driver-memory 1g <span class="se">\</span>
</span><span class='line'>  --executor-memory 1g <span class="se">\</span>
</span><span class='line'>  --executor-cores 1 <span class="se">\</span>
</span><span class='line'>  --arg hdfs://sandbox:9000/input/sample.txt <span class="se">\</span>
</span><span class='line'>  --arg /output <span class="se">\</span>
</span><span class='line'>  --arg 10 <span class="se">\</span>
</span><span class='line'>  --arg 10
</span></code></pre></td></tr></table></div></figure>


<p>During the submission note that: not just the app jar, but the spark-submit-runner jar is also uploaded (which is an <code>uberjar</code>) to the HDFS. To avoid this, you have to upload it to the HDFS manually and set the <strong>SPARK_JAR</strong> environment variable.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">SPARK_JAR</span><span class="o">=</span><span class="s2">&quot;hdfs:///spark/spark-submit-runner-1.0.jar&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you get &ldquo;Permission denied&rdquo; exception on submit, you should set the <strong>HADOOP_USER_NAME</strong> environment variable to root (or something with proper rights).</p>

<p>As usual for us we ship the code &ndash; you can get it from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit">GitHub</a> samples repository; the sample input is available <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/input.txt">here</a>.</p>

<p>If you would like to play with Spark, you can use our <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Spark Docker container</a> available as a trusted build on Docker.io repository.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.5.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/"/>
    <updated>2014-08-18T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker</id>
    <content type="html"><![CDATA[<p>A few weeks ago we have released an Apache Hadoop 2.3 Docker image &ndash; in a very short time this become the most <a href="https://registry.hub.docker.com/search?q=hadoop">popular</a> Hadoop image in the Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Following on the success of our Hadoop 2.3.0, 2.4.0 and 2.4.1 Docker <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">image</a>, the feedbacks and requests we have received and aligning with the Hadoop release cycle, we have released an Apache Hadoop 2.5.0 Docker image &ndash; same as the previous version this is available as a trusted and automated build on the official Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Please note that beside this Hadoop image, we have released and maintain a <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">pseudo-distributed</a> and <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">fully distributed</a> Hadoop Docker image provisioned with Apache Ambari. As they are provisioned with Ambari you have the option to change, add and remove Hadoop components using cluster blueprints.</p>

<p>Also we are happy to let you know that this release of Apache Hadoop contains a few of SequenceIQ&rsquo;s open source <strong>contributions</strong> and <a href="https://issues.apache.org/jira/browse/YARN-2250">fixes</a> around YARN schedulers.
We are working on an SLA enforcer for Hadoop &ndash; very soon to be open sourced &ndash; and part of that work we are contributing back to the community. Also there is a major contribution of ours coming in the next release of Hadoop &ndash; 2.6.0.
Stay tuned and follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<h2>Build the image</h2>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Pull the image</h2>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.5.0</span></code></pre></td></tr></table></div></figure>


<h2>Start a container</h2>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker:2.5.0 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.4.1/view/files">Bintray repo</a>. Enjoy.</p>

<p>Should you have any questions let us know through our social channels as <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
