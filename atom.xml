<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2015-01-29T12:35:46+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - new release available]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta/"/>
    <updated>2015-01-28T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/28/cloudbreak-last-beta</id>
    <content type="html"><![CDATA[<p>We are happy to announce that the <code>Release Candidate</code> version of Cloudbreak is almost around the corner. This major release is the last one in the <code>public beta program</code> and contains quite a few new features and architectural changes.</p>

<p>All theses new major features will be covered in the coming weeks on our blog, but in the meantime let us do a quick skim through.</p>

<h3>Accounts</h3>

<p>We have introduced the concept of <code>accounts</code> &ndash; after a user registers/signs in the first time will have the option to invite other people in the <code>account</code>. Being the administrator of the account, will have the option to activate, deactive and give admin rights for all the invited users.</p>

<p>Users can share <code>resources</code> (such as: cloud credentials, templates, blueprints, clusters) within the account by making it <code>public in account</code> but at the same time can create his own private resources as well. As you might be already aware, we use OAuth2 to make all these possible.</p>

<h3>Usage explorer</h3>

<p>We have built a unified (accross all cloud providers) usage explorer tool, where you can drill down into details to learn your (or in your account if you have admin rights) usage history. You can filter by date, users, cloud providers, region, etc &ndash; and generate a consolidated table/chart overview.</p>

<h3>Heterogenous clusters</h3>

<p>This was a feature many have asked &ndash; and we are happy to deliver it. Up till now all the nodes in your YARN clusters were built on the same cloud <code>instance types</code>. While this was an easy an convenient way to build a cluster (as far as we are aware all the Hadoop as a Service providers are doing it this way) back in the MR1 era, times changed now and with the emergence of <code>YARN</code> different workloads are running within a cluster.</p>

<p>While for example Spark jobs require a high memory instance a legacy MR2 code might require a high CPU instance, whereas a HBase RegionServer likes better a high I/O throughput one.</p>

<p>At SequenceIQ we have quickly realized this and the new release allows you to apply different <code>stack templates</code> to all these YARN services/components. We do the heavy lifting for you in the background &ndash; the only thing you will have to do is to associate stack templates to Ambari <code>hostgroups</code>.</p>

<p>This is a major step forward when you are using and running different workloads on your YARN cluster &ndash; and not just saving on costs but at the same time increasing your cluster throughput.</p>

<!--more-->


<h3>Hostgroup based autoscaling</h3>

<p>Cloudbreak now integrates with <a href="http://sequenceiq.com/periscope">Periscope</a> &ndash; and allows you to set up alarms and autoscaling SLA policies based on YARN metrics. Having done the heterogenous cluster integration, now it&rsquo;s time to apply <code>autoscaling</code> for those nodes based on Ambari Blueprints.</p>

<h3>Recipes</h3>

<p>While Cloudbreak and Ambari combined are a pretty powerful way to configure your Hadoop cluster, sometimes there are manuall steps required to reconfigure services, build dependent cluster architectures (e.g.: permanent and ephemeral clusters), etc &ndash; the list can be long.
Even a simple configuration on a large (thousands nodes) cluster is a tedious job &ndash; and usually people use Ansible, Chef, Puppet or Saltstack to do so &ndash; however these all have some drawback and are not integrated with Cloudbreak. As Cloudbreak under the hood uses Consul, we came up with a simple solution which facilitates creating, applying and running <code>recipes</code> on your already provisioned cluster &ndash; pre/post Ambari installation. A follow up blog post will be released in the coming days whch will explain the concept, architecture and gives you a few sample recipes.</p>

<h3>OpenStack</h3>

<p>This was one of the other highly desired features &ndash; and a perfect use case for Docker. You might be aware that we run the full Hadoop stack inside Docker container &ndash; and Cloudbreak&rsquo;s integration with the cloud provider is pretty thin. This gives us the option to add quick integration with a new cloud provider &ndash; the full OpenStack integration with Cloudbreak took few weeks only.</p>

<p>Long story short &ndash; Cloudbreak now support and automates provisioning of Hadoop clusters with custom blueprints on OpenStack. Give it a try and let us know how it works for you.</p>

<h3>Ambari 1.7 integration</h3>

<p>Shortly after Ambari 1.7 came out we have upgraded Cloudbreak to use this new version. Ambari 1.7 supports HDP and Bigtop stacks and an increased number of Hadoop services/components.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark 1.2.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/"/>
    <updated>2015-01-09T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker</id>
    <content type="html"><![CDATA[<p>In this current post we’d like to help you to start with the <code>latest - 1.2.0</code> Spark release in minutes &ndash; using Docker. Though we have released and pushed the container between the holidays into the official Docker repository, we were still due with the post. Here are the details &hellip;</p>

<p>Docker and Spark are two technologies which are very <code>hyped</code> these days. At <a href="http://sequenceiq.com/">SequenceIQ</a> we use both quite a lot, thus we put together a Docker container and sharing it with the community.</p>

<p>The container’s code is available in our <a href="https://github.com/sequenceiq/docker-spark/tree/v1.2.0onHadoop2.6.0">GitHub</a> repository.</p>

<h3>Pull the image from Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/spark:1.2.0</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Building the image</h2>

<p>Alternatively you can always build your own container based on our Dockerfile.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build --rm -t sequenceiq/spark:1.2.0 .</span></code></pre></td></tr></table></div></figure>


<h2>Running the image</h2>

<p>Once you have pulled or built the container, you are ready to start with Spark.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t -h sandbox sequenceiq/spark:1.2.0 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h3>Testing</h3>

<p>In order to check whether everything is OK, you can run one of the stock examples, coming with Spark. Check our previous blog posts and examples about Spark <a href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/">here</a> and <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">here</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'># run the spark shell
</span><span class='line'>./bin/spark-shell --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1
</span><span class='line'>
</span><span class='line'># execute the the following command which should return 1000
</span><span class='line'>scala&gt; sc.parallelize(1 to 1000).count()</span></code></pre></td></tr></table></div></figure>


<p>There are two deploy modes that can be used to launch Spark applications on YARN. In yarn-cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>

<p>Estimating Pi (yarn-cluster mode):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>
</span><span class='line'># execute the the following command which should write the "Pi is roughly 3.1418" into the logs
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 ./lib/spark-examples-1.2.0-hadoop2.4.0.jar</span></code></pre></td></tr></table></div></figure>


<p>Estimating Pi (yarn-client mode):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>
</span><span class='line'># execute the the following command which should print the "Pi is roughly 3.1418" to the screen
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 ./lib/spark-examples-1.2.0-hadoop2.4.0.jar</span></code></pre></td></tr></table></div></figure>


<p>Should you have any questions let us know through our social channels using <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker containers as Apache YARN containers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2015/01/07/yarn-containers-docker/"/>
    <updated>2015-01-07T09:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2015/01/07/yarn-containers-docker</id>
    <content type="html"><![CDATA[<p>The Hadoop 2.6 release contains a new <a href="https://issues.apache.org/jira/browse/YARN-1964">feature</a> that allows to launch Docker containers directly as YARN containers. Basically this solution let the developers package their applications and all of the dependencies into a Docker container in order to provide a consistent environment for execution and also provides isolation from other applications or softwares installed on host.</p>

<h2>Configuration</h2>

<p>To launch YARN containers as Docker containers the  <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html">DockerContainerExecutor</a> and the Docker client needs to be set up in the <code>yarn-site.xml</code> configuration file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.nodemanager.container-executor.class<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.nodemanager.docker-container-executor.exec-name<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>/usr/local/bin/docker<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>As the documentations states the DockerContainerExecutor requires the Docker daemon to be running on the NodeManagers and the Docker client must be also available, but at <a href="http://sequenceiq.com">SequenceIQ</a> we have already packaged and running the whole Hadoop ecosystem into Docker containers and therefore we already have a Docker daemon and Docker client, the only problem is that they are outside of our Hadoop container and therefore the NodeManager or any other process running inside the container does not have access to them. In one of our <a href="http://blog.sequenceiq.com/blog/2014/11/20/yarn-containers-and-docker/">earlier post</a> we have considered  to run Docker daemon inside Docker, but instead of running Docker in Docker it is much more simpler just to reuse the Docker daemon and Docker client what was used for launching the <a href="http://sequenceiq.com">SequenceIQ</a> containers.</p>

<!--more-->


<h2>Reuse of Docker daemon and client</h2>

<p>The problem what needs to be solved is to connect to Docker daemon (running on host) from a process running inside the container. It is possible to make the Docker daemon to listen on a specific TCP port, but it is not recommended due to security reasons since exposing the port might allow other clients to connect it and accidentally start/stop containers or even gain root access to the host where the daemon is running. Luckily the Docker daemon can listen on Unix domain socket on <code>unix:///var/run/docker.sock</code> to allow local connections and this can be mounted as Docker volume from the container.</p>

<p>For setting up the Docker client we also have multiple options, e.g. we can install it into the container or we can just mount the Docker client as a volume since it is just a single binary executable file.</p>

<p>These configuration has already been set up in the <a href="https://github.com/sequenceiq/hadoop-docker/tree/dce">dce branch of hadoop-docker repository</a> and also made available in the Docker Registry under the name sequenceiq/hadoop-docker:2.6.0-dce, therefore the command to try out the DockerContainerExecutor would look like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>docker run -i -t -v /usr/local/bin/docker:/usr/local/bin/docker -v /var/run/docker.sock:/var/run/docker.sock --net=host sequenceiq/hadoop-docker:2.6.0-dce /etc/bootstrap.sh -bash
</span><span class='line'>
</span><span class='line'># To verify it just launch the following command inside the container
</span><span class='line'>docker ps
</span><span class='line'>CONTAINER ID        IMAGE                                COMMAND ...
</span><span class='line'>c07914786e78        sequenceiq/hadoop-docker:2.6.0-dce   &quot;/etc/bootstrap.sh ...
</span></code></pre></td></tr></table></div></figure>


<p>From the first look this seems sufficient, but if you try to execute any Hadoop example it will fail, because the DockerContainerExecutor passes launch configuration and shares log files with YARN containers trough Docker volumes and in our configuration the Docker daemon is running outside of the sequenceiq/hadoop-docker container in other words it is not running on the same place where the NodeManager, therefore directories that are intended to be mounted as Docker volumes for YARN containers will not be there since the NodeManager is running inside a container and the directories are mounted from host. The workaround for this is to create the directories directly on host machine and mount them to sequenceiq/hadoop-docker container and the same directories will be mounted to YARN containers by DockerContainerExecutor.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'># On host machine where the Docker daemon is running (if you are using OS X then on boot2docker)
</span><span class='line'>mkdir -p /tmp/hadoop-root/nm-local-dir
</span><span class='line'>mkdir -p /usr/local/hadoop/logs/userlogs/
</span><span class='line'>
</span><span class='line'>docker run -i -t -v /usr/local/bin/docker:/usr/local/bin/docker -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/hadoop-root/nm-local-dir:/tmp/hadoop-root/nm-local-dir -v /usr/local/hadoop/logs/userlogs:/usr/local/hadoop/logs/userlogs --net=host sequenceiq/hadoop-docker:2.6.0-dce /etc/bootstrap.sh -bash
</span></code></pre></td></tr></table></div></figure>


<p>This is clearly a workaround and we are considering to create a patch for DockerContainerExecutor to make the volume sharing seamless by using Data Volume Container.</p>

<h2>Starting a MapReduce Job</h2>

<p>Starting a stock example also requires a few extra parameters like <code>mapreduce.map.env</code>, <code>mapreduce.reduce.env</code> and <code>yarn.app.mapreduce.am.env</code>, since the DockerContainerExecutor needs to know which Docker container shall be executed as YARN container.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'># run the grep with 2.6.0
</span><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'>
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep -Dmapreduce.map.env=&quot;yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.6.0-dce&quot; -Dmapreduce.reduce.env=&quot;yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.6.0-dce&quot; -Dyarn.app.mapreduce.am.env=&quot;yarn.nodemanager.docker-container-executor.image-name=sequenceiq/hadoop-docker:2.6.0-dce&quot; input output &#39;dfs[a-z.]+&#39;
</span><span class='line'>
</span><span class='line'># check the output of grep
</span><span class='line'>bin/hdfs dfs -cat output/*
</span></code></pre></td></tr></table></div></figure>


<p>As you can see the sequenceiq/hadoop-docker:2.6.0-dce image has been specified as parameters and not the sequenceiq/hadoop-docker:2.6.0, but basically there is no difference in this case, since when it is launched as YARN container then only the libraries are are used from the image and the bootstrap.sh or configuration files like yarn-site.xml are ignored.</p>

<p>In order to make it easier to understand you can take a look at the diagram which shows the relationship between containers, processes and volumes.</p>

<ul>
<li>green box: shows the container started by <code>docker run</code> command which is defined above</li>
<li>blue boxes: represent the containers started by DockerContainerExecutor</li>
<li>red boxes: processes started inside the individual containers or directly on host in case of Docker daemon</li>
<li>yellow boxes: mounted Docker volumes</li>
</ul>


<p> <img src="https://raw.githubusercontent.com/sequenceiq/blog-test/source/source/images/yarn-container/process_map.png" alt="" /></p>

<h2>Summary</h2>

<p>We hope that the above example provides you a good start to play with DockerContainerExecutor, but it is important to know that this new feature has been put to Hadoop 2.6 release only in the last minute and it is still in alpha state, therefore using it in production is not recommended.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Cloudbreak release - support for HDP 2.2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2/"/>
    <updated>2014-12-23T12:59:42+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/23/cloudbreak-on-hdp-2-dot-2</id>
    <content type="html"><![CDATA[<p>The last two weeks were pretty busy for us &ndash; we have <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">Dockerized</a> the new release of Ambari (1.7.0), <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">integrated</a> Periscope with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Cloudbreak</a> and just now we are announcing a new Cloudbreak <a href="https://cloudbreak.sequenceiq.com">release</a> which uses Ambari 1.7.0 and has full support for Hortonworks HDP 2.2 and Apache Bigtop stacks. But first &ndash; since this has been asked many times &ndash; see a <code>short</code> movie about Cloudbreak and Periscope in action.</p>

<h2>On-demand Hadoop cluster with autoscaling</h2>

<iframe width="640" height="480" src="//www.youtube.com/embed/E6bnEW76H_E" frameborder="0" allowfullscreen></iframe>




<!--more-->


<h2>Ambari 1.7.0</h2>

<p>The Ambari community recently released the 1.7.0 version which comes with lots of new features and bug fixes. We&rsquo;ve been testing the new version
internally for a while now and finally made it to Cloudbreak. Just to highlight the important ones:</p>

<ul>
<li>Ambari Views framework</li>
<li>Ambari Administration

<ul>
<li>Management of users/groups</li>
<li>Management of view instances</li>
<li>Management of cluster permissions</li>
</ul>
</li>
<li>Cancel/Abort background operation requests</li>
<li>Expose Ambari UI for config versioning, history and rollback</li>
<li>Ability to manage -env.sh configuration files</li>
<li>Recommendations and validations (via a &ldquo;Stack Advisor&rdquo;)</li>
<li>Export service configurations via Blueprint</li>
<li>Install + Manage Flume</li>
<li>HDFS Rebalance</li>
<li>ResourceManager HA</li>
</ul>


<p>These are nice features but for us one of the most important thing is that it allows you to install the latest versions of the Hadoop ecosystem.
As usual the Docker image is available for <em>local</em> deployments as well, described <a href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/">here</a>.</p>

<p><code>Note: There were small changes around the API so if you built an application on top of it check your REST calls. The Ambari Shell and the
underlying Groovy rest client have been updated and will go into the Apache repository once it's passed the reviews.</code></p>

<h2>Hadoop 2.6</h2>

<p>Since with Ambari 1.7.0 we&rsquo;re able to install Hadoop 2.6 let&rsquo;s see what happened in <code>YARN</code> in the last couple of months (it&rsquo;s stunning):</p>

<ul>
<li>Support for long running services &ndash; install <em>Slider</em> with Ambari and scale your Hadoop services!

<ul>
<li>Service Registry for applications</li>
</ul>
</li>
<li>Support for rolling upgrades &ndash; wow!

<ul>
<li>Work-preserving restarts of ResourceManager</li>
<li>Container-preserving restart of NodeManager</li>
</ul>
</li>
<li>Supports node labels during scheduling &ndash; label based scaling is on the way with <a href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/">Periscope</a></li>
<li>Support for time-based resource reservations in Capacity Scheduler (beta) &ndash; more on this awesome feature soon</li>
<li>Support running of applications natively in Docker containers (alpha) &ndash; <a href="http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker/">Docker in Docker</a></li>
</ul>


<p>I&rsquo;m excited about these great innovations (not, because we&rsquo;re involved in a few of them), but because people can leverage them by using Cloudbreak.</p>

<h2>HDP 2.2 blueprint</h2>

<p>I have created a blueprint which is not an <code>official</code> one, but it contains a few from the new services like: <code>SLIDER</code>, <code>KAFKA</code>, <code>FLUME</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "configurations": [
</span><span class='line'>  {
</span><span class='line'>    "nagios-env": {
</span><span class='line'>      "nagios_contact": "admin@localhost"
</span><span class='line'>    }
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "hive-site": {
</span><span class='line'>        "javax.jdo.option.ConnectionUserName": "hive",
</span><span class='line'>        "javax.jdo.option.ConnectionPassword": "hive"
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  ],
</span><span class='line'>  "host_groups": [
</span><span class='line'>    {
</span><span class='line'>      "name": "master_1",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "NAMENODE"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "ZOOKEEPER_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HBASE_MASTER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HDFS_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "YARN_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HCAT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "FALCON_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "FLUME_HANDLER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "KAFKA_BROKER"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "1"
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "name": "master_2",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "ZOOKEEPER_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HISTORYSERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HIVE_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "SECONDARY_NAMENODE"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HIVE_METASTORE"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HDFS_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HIVE_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "YARN_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "MYSQL_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "WEBHCAT_SERVER"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "1"
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "name": "master_3",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "RESOURCEMANAGER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "APP_TIMELINE_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "SLIDER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "ZOOKEEPER_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "1"
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "name": "master_4",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "OOZIE_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "ZOOKEEPER_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "1"
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "name": "slave_1",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "HBASE_REGIONSERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "NODEMANAGER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "DATANODE"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "FALCON_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "OOZIE_CLIENT"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "${slavesCount}"
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "name": "gateway",
</span><span class='line'>      "components": [
</span><span class='line'>        {
</span><span class='line'>          "name": "AMBARI_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "NAGIOS_SERVER"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "ZOOKEEPER_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "PIG"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "OOZIE_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HBASE_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HCAT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "SQOOP"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HDFS_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "HIVE_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "YARN_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "MAPREDUCE2_CLIENT"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "GANGLIA_MONITOR"
</span><span class='line'>        },
</span><span class='line'>        {
</span><span class='line'>          "name": "KNOX_GATEWAY"
</span><span class='line'>        }
</span><span class='line'>      ],
</span><span class='line'>      "cardinality": "1"
</span><span class='line'>    }
</span><span class='line'>    ],
</span><span class='line'>    "Blueprints": {
</span><span class='line'>      "blueprint_name": "hdp-multinode-sequenceiq",
</span><span class='line'>      "stack_name": "HDP",
</span><span class='line'>      "stack_version": "2.2"
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>What&rsquo;s next?</h2>

<blockquote><p>Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.</p></blockquote>


<p>We&rsquo;ve walked a long journey since we started the company almost a year ago to reach where we are now, but our products are not complete yet. We have big plans
with our product stacks. A couple of things from our roadmap:</p>

<h3>Cloudbreak</h3>

<ul>
<li>Cloudbreak currently supports homogeneous cluster deployments which we&rsquo;re going to change. The heterogeneous stack structure is more convenient
from Hadoop&rsquo;s perspective. The ability to define different type of cloud instances is a must, giving the users the option to use much more
powerful instances for the <code>ResourceManager</code> and <code>NameNodes</code>.</li>
<li>Service discovery and decentralization is always a key aspect. At the moment we&rsquo;re using Serf and dnsmasq, but we&rsquo;re already started the
integration with <a href="https://consul.io">Consul</a> which generally is a better fit. It provides service registration via DNS, key-value store and
decentralization across datacenters.</li>
<li>The deployment of Cloudbreak itself is going to change and use Consul with other side projects like <code>Consul templates</code> or <code>Registrator</code>. The
deployment is already based on Docker, but will be much more simplified.</li>
<li>Custom stack deployments with Ambari will be supported as <code>"recipes"</code>.</li>
<li>Generating reports of cloud instance usages and cost calculation.</li>
<li>Web hooks to subscribe to different cluster events.</li>
<li>Shared/company accounts.</li>
</ul>


<h3>Periscope</h3>

<ul>
<li>Add more <code>YARN</code> and <code>NameNode</code> related metrics.</li>
<li>Node label based scaling.</li>
<li>Pluggable metric system for custom metrics.</li>
<li>Application movement in Capacity Scheduler queues enforcing SLAs.</li>
<li>Time-based resource reservations in Capacity Scheduler for Applications.</li>
<li>Integration with <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">ELK</a>.</li>
</ul>


<h2>Happy Holidays</h2>

<p>We&rsquo;re taking a short break of writing new blog posts until next year. You can still reach us on the usual social sites, but you can expect
small delays for answering questions. <code>Happy Holidays everyone.</code></p>

<p><a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> <a href="https://twitter.com/sequenceiq">Twitter</a> <a href="https://www.facebook">Facebook</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak welcomes Periscope]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope/"/>
    <updated>2014-12-12T14:13:33+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/12/cloudbreak-got-periscope</id>
    <content type="html"><![CDATA[<p>Today we have pushed out a new release of <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> &ndash; our Docker container based and cloud agnostic Hadoop as a Service solution &ndash; containing a few major changes. While there are many significant changes (both functional and architectural) in this blog post we&rsquo;d like to describe one of the most expected one &ndash; the <code>autoscaling</code> of Hadoop clusters.</p>

<p>Just to quickly recap, Cloudbreak allows you to provision clusters &ndash; <code>full stacks</code> &ndash; in all major cloud providers using a unified API, UI or CLI/shell. Currently we support provisioning of clusters in <code>AWS</code>, <code>Google Cloud</code>, <code>Azure</code> and <code>OpenStack</code> (in private beta) &ndash; new cloud providers can be added quite easily (as everything runs in Docker) using our SDK.</p>

<p><a href="http://sequenceiq.com/periscope/">Periscope</a> allows you to configure SLA policies for your Hadoop cluster and scale up or down on demand. You are able to set alarms and notifications for different metrics like <code>pending containers</code>, <code>lost nodes</code> or <code>memory usage</code>, etc and set SLA scaling policies based on these alarms.</p>

<p>Today&rsquo;s <a href="http://cloudbreak.sequenceiq.com/">release</a> made available the integration between the two projects (they work independently as well) and allows subscribers to enable autoscaling for their already deployed or newly created Hadoop cluster.</p>

<p>We would like to guide you through the UI and help you to set up an autoscaling Hadoop cluster.</p>

<!--more-->


<h2>Using Periscope</h2>

<p>Once you have created your Hadoop clusters with Cloudbreak you will now how the option to configure autoscaling policies.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/select.png" alt="" /></p>

<p>In order to configure autoscaling for your cluster you should go to <code>autoscaling SLA policies</code> tab and hit the <code>enable</code> button.</p>

<h3>Alarms</h3>

<p>Periscope allows you to configure two types of <code>alarms</code>.</p>

<p><strong>Metric based</strong> alarms are alarms based on different <code>YARN</code> metrics. A plugin mechanism will be available in case you&rsquo;d like to plug your own metrics. As a quick note, we have another project called <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">Baywatch</a> where we collect around 400 Hadoop metrics &ndash; and those will be all pluggable in Periscope.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-metric.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>metrics &ndash; currently the default YARN metrics we support are: <code>pending containers</code>, <code>pending applications</code>, <code>lost nodes</code>, <code>unhealthy nodes</code> and <code>global resources</code></li>
<li>period &ndash;  the time that the metric has to be sustained in order for an alarm to be triggered</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<p><strong>Time based</strong> alarms allow autoscaling of clusters based on the configured time. We have <a href="http://blog.sequenceiq.com/blog/2014/11/25/periscope-scale-your-cluster-on-time/">blogged</a> about this new feature recently &ndash; with this new release of <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> this feature is available through UI as well.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/alarm-time.png" alt="" /></p>

<ul>
<li>alarm name &ndash; name of the alarm</li>
<li>description &ndash; description of the alarm</li>
<li>time zone &ndash; the timezone for the <code>cron</code> expression</li>
<li>cron expression &ndash; the cron expression</li>
<li>notification email (optional) &ndash; address where Periscope sends an email in case the alarm is triggered</li>
</ul>


<h2>Scaling policies</h2>

<p>Once you have an alarm you can configure scaling policies based on it. Scaling policies defines the actions you&rsquo;d like Periscope to take in case of a triggered alarm.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/scaling.png" alt="" /></p>

<ul>
<li>policy name &ndash; the name of the SLA scaling policy</li>
<li>scaling adjustment &ndash; the adjustment counted in <code>nodes</code>, <code>percentage</code> or <code>exact</code> numbers of cluster nodes</li>
<li>host group &ndash; the <code>autoscaled</code> Ambari hostgroup</li>
<li>alarm &ndash; the configured alarm</li>
</ul>


<h2>Cluster scaling configurations</h2>

<p>A cluster has a default configuration which Periscope scaling policies can&rsquo;t override. This is due to avoid over or under scaling a Hadoop cluster with policies and also to definde a cooldown time period between two scaling actions.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/cluster-config.png" alt="" /></p>

<ul>
<li>cooldown time &ndash; the time spent between two scaling actions</li>
<li>cluster size min. &ndash; the minimum size (in nodes) of a cluster</li>
<li>cluster size max. &ndash; the maximum size (in nodes) of a cluster</li>
</ul>


<p>It&rsquo;s that simple. Happy autoscaling.</p>

<p>In case you&rsquo;d like to test autoscaling and generate some load on your cluster you can use these <code>stock</code> Hadoop examples and the scripts below:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>
</span><span class='line'>export HADOOP_LIBS=/usr/lib/hadoop-mapreduce
</span><span class='line'>export JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient-2.4.0.2.1.2.0-402-tests.jar
</span><span class='line'>
</span><span class='line'>smalljobs(){
</span><span class='line'>  echo "############################################"
</span><span class='line'>  echo Running smalljobs tests..
</span><span class='line'>  echo "############################################"
</span><span class='line'>
</span><span class='line'>  CMD="hadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/hrt_qa/smallJobsBenchmark -numRuns 2 -maps 10 -reduces 5 -inputLines 10 -inputType ascending"
</span><span class='line'>  echo TEST 1: $CMD
</span><span class='line'>  su hdfs -c "$CMD" 1&gt; smalljobs-time.log 2&gt; smalljobs.log
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>smalljobs</span></code></pre></td></tr></table></div></figure>


<p>To test it you can run it with the following script:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>
</span><span class='line'>for i in {1..10}
</span><span class='line'>do
</span><span class='line'>nohup /test.sh &
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multinode cluster with Ambari 1.7.0 - in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0/"/>
    <updated>2014-12-04T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/04/multinode-ambari-1-7-0</id>
    <content type="html"><![CDATA[<p>Two days ago the latest <a href="http://ambari.apache.org/">version</a> of Ambari (1.7.0) has been released and now is time for us to release our automated process to deploy Hadoop clusters with Ambari in Docker containers.</p>

<p>The release contains lots of new features (follow <a href="http://ambari.apache.org/whats-new.html#">this</a> link) &ndash; we will highlight a few we consider important for us:</p>

<ul>
<li>Ambari Views &ndash; a systematic way to plug-in UI capabilities to surface custom visualization, management and monitoring features in Ambari Web.</li>
<li>Extended/new stack definitions &ndash; support for Hortonworks HDP and Apache Bigtop stacks</li>
<li>Apache Slider integration &ndash; ease deployments of existing applications into a YARN cluster</li>
</ul>


<p>As usual we have <code>dockerized</code> the whole Ambari 1.7.0 thus you can take the container and provision your arbitrary size Hadoop cluster.</p>

<h3>Get the Docker container</h3>

<p>In case you don’t have Docker browse among our previous posts &ndash; we have a few posts about howto’s, examples and best practices in general for Docker and in particular about how to run the full Hadoop stack on Docker.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:1.7.0</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p>Once you have the container you are almost ready to go &ndash; we always automate everything and <strong>over simplify</strong> Hadoop provisioning.</p>

<h3>Get ambari-functions</h3>

<p>Get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0/ambari-functions">file</a> from our GitHub.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari-170 && . .amb</span></code></pre></td></tr></table></div></figure>


<h3>Create your cluster &ndash; manually</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-start-cluster 3</span></code></pre></td></tr></table></div></figure>


<p>This will start a 3 node Ambari cluster where all the containers are preconfigured and the Ambari agants are running.</p>

<p>Now lets get familiar with a <a href="http://sequenceiq.com">SequenceIQ</a> contribution for Ambari: the <a href="https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Shell">Ambari Shell</a>.</p>

<p>Type the following command</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-shell</span></code></pre></td></tr></table></div></figure>


<p>This will start the Ambari shell. After the welcome screen
Now lets quickly create a cluster. Since two days ago Hortonworks released <a href="http://hortonworks.com/blog/available-now-hdp-2-2/">HDP 2.2</a> let set up an HDP 2.2 cluster. For that we will use this <a href="https://gist.github.com/matyix/aeb8837012b5fa253fa5">blueprint</a>.</p>

<p>In the shell type the following &ndash; note that throughout the process you can use <code>hint</code> or <code>help</code> for guidance and <code>tab completion</code> as well.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blueprint add --url https://gist.githubusercontent.com/matyix/aeb8837012b5fa253fa5/raw/3476b538c8ba0c16363dbfd9634f0b9fe88cb36e/multi-node-hdfs-yarn
</span><span class='line'>cluster build --blueprint multi-node-hdfs-yarn
</span><span class='line'>cluster autoAssign
</span><span class='line'>cluster create</span></code></pre></td></tr></table></div></figure>


<p>You can track the progress either from the shell or log into the Ambari UI. If you use <code>boot2docker</code> you should add routing from your host into the container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo route add -net 172.17.0.0/16 192.168.59.103</span></code></pre></td></tr></table></div></figure>


<p>In order to learn the Ambari UI IP address (IPAddres) use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker inspect --format<span class="o">=</span><span class="s1">&#39;{{.NetworkSettings.IPAddress}}&#39;</span> amb0
</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it.</p>

<h2>Create your cluster &ndash; automated</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>amb-deploy-cluster 3
</span></code></pre></td></tr></table></div></figure>


<p><strong>Whaaat?</strong> No really, that’s it &ndash; we have just provisioned you a 3 node Hadoop cluster in less than 2 minutes. Docker, Apache Ambari and Ambari Shell combined is quite powerful, isn&rsquo;t it? You can always start playing with your desired services by changing the <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">blueprints</a> &ndash; the full Hadoop stack is supported.</p>

<p>If you’d like to play around and understand how this works check our previous blog posts &ndash; a good start is this first post about one of our contribution, the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari Shell</a>.</p>

<p>You have just seen how easy is to provision a Hadoop cluster on your laptop, if you’d like to see how we provision a Hadoop cluster in the cloud using the very same Docker image you can check our open source, cloud agnostic Hadoop as a Service API &ndash; <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a>. Also we have released a project called <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; the industry&rsquo;s first open source autoscaling API for Hadoop.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Hadoop 2.6.0 in Docker containers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker/"/>
    <updated>2014-12-02T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/12/02/hadoop-2-6-0-docker</id>
    <content type="html"><![CDATA[<p>Yesterday the Hadoop community has released the <code>2.6.0</code> version of Hadoop &ndash; the 4th major release of this year &ndash; and one which contains quite a few new and interesting features:</p>

<ul>
<li>Rolling upgrades &ndash; the holly grail for enterprises to switch Hadoop versions</li>
<li>Long running services in YARN</li>
<li>Heterogeneous storage in HDFS</li>
</ul>


<p>These were the most popular features, though beside these there were quite a few extremely important ones &ndash; at least for us and our our <a href="http://sequenceiq.com/periscope/">Periscope</a> project. As you might be aware we are working on an SLA policy based autoscaling API for Apache YARN and we were closely following/been involved or contributed to these JIRA&rsquo;s below:</p>

<ul>
<li><a href="https://issues.apache.org/jira/browse/YARN-2248">YARN-2248</a> &ndash; CS changes for moving apps between queues</li>
<li><a href="https://issues.apache.org/jira/browse/YARN-1051">YARN-1051</a> &ndash; YARN Admission Control/Planner</li>
</ul>


<p>These tasks/subtasks (beside a few others) are all coming with the new major release and opening up brand new opportunities to make Hadoop YARN a more <code>dynamic</code> environment. Considering these and the <a href="http://slider.incubator.apache.org/index.html">Apache Slider</a> project it&rsquo;s pretty clear to see that exciting times are coming.</p>

<p>We have combined all these above with <a href="http://slider.incubator.apache.org/index.html">Docker</a>, our open source projects &ndash; <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> and <a href="http://sequenceiq.com/periscope/">Periscope</a> &ndash; and we are leveraging these new innovations, so stay tuned and get the code or our Docker containers to start with.</p>

<p>In the meanwhile (as usuall) we have <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">released</a> our Hadoop 2.6.0 container to ease your quick start with Hadoop.</p>

<h3>DIY &ndash; Build the image</h3>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker/tree/2.6.0">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker:2.6.0 .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h3>Pull the image</h3>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.6.0</span></code></pre></td></tr></table></div></figure>


<h3>Start the container</h3>

<p>In order to use the Docker image you have just built or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker:2.6.0 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop <code>nativelibs</code> on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.6.0/view/files">Bintray repo</a>. Enjoy.</p>

<p>Should you have any questions let us know through our social channels as <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Periscope: time based autoscaling]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/25/periscope-scale-your-cluster-on-time/"/>
    <updated>2014-11-25T14:13:33+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/25/periscope-scale-your-cluster-on-time</id>
    <content type="html"><![CDATA[<p><a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> allows you to configure SLA policies for your cluster
and scale up or down on demand. You are able to set alarms and notifications for different metrics like <code>pending containers</code>,
<code>lost nodes</code> or <code>memory usage</code>, etc . Recently we got a request to scale based on <code>time interval</code>. What does this mean? It means that you can tell
Periscope to shrink your cluster down to arbitrary number of nodes after work hours or at weekends and grow it back by the time people starts to work. We thought it would make a really useful feature so we quickly implemented it and made available. You can learn more about the Periscope API <a href="http://docs.periscope.apiary.io/">here</a>.</p>

<h3>Cost efficiency</h3>

<p>In this example we&rsquo;ll configure Pericope to downscale at 7PM and upscale at 8AM from Monday to Friday:</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/dowscale_diagram.png" alt="" /></p>

<p>Just to make things easier let&rsquo;s assume that our cluster is homogeneous. On AWS a c3.xlarge instance costs $0.210 per hour.
Now let&rsquo;s do the math:</p>

<ul>
<li>24 x 0.21 x 100                      = $504</li>
<li>(11 x 0.21 x 100) + (13 x 0.21 x 10) = $260</li>
</ul>


<p>In a month we can save <strong>$7560</strong> scaling from 100 to 10 and back &ndash; and the weekends are not even counted.</p>

<!--more-->


<h3>Cron based alarms</h3>

<p>In order to configure such actions you&rsquo;ll have to set some <code>time alarms</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;alarms&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;alarmName&quot;</span><span class="p">:</span> <span class="s2">&quot;worktime&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Number of nodes during worktime&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;timeZone&quot;</span><span class="p">:</span> <span class="s2">&quot;Europe/Budapest&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;cron&quot;</span><span class="p">:</span> <span class="s2">&quot;0 59 07 ? * MON-FRI&quot;</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;alarmName&quot;</span><span class="p">:</span> <span class="s2">&quot;after work&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Number of nodes after worktime&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;timeZone&quot;</span><span class="p">:</span> <span class="s2">&quot;Europe/Budapest&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;cron&quot;</span><span class="p">:</span> <span class="s2">&quot;0 59 18 ? * MON-FRI&quot;</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that the alarms are set we need to tell Periscope what to do when they are triggered. Let&rsquo;s define the <code>scaling policies</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;minSize&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;maxSize&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;cooldown&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;scalingPolicies&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;upscale&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;adjustmentType&quot;</span><span class="p">:</span> <span class="s2">&quot;EXACT&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;scalingAdjustment&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;hostGroup&quot;</span><span class="p">:</span> <span class="s2">&quot;slave_1&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;alarmId&quot;</span><span class="p">:</span> <span class="s2">&quot;150&quot;</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;downscale&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;adjustmentType&quot;</span><span class="p">:</span> <span class="s2">&quot;EXACT&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;scalingAdjustment&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;hostGroup&quot;</span><span class="p">:</span> <span class="s2">&quot;slave_1&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;alarmId&quot;</span><span class="p">:</span> <span class="s2">&quot;151&quot;</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>For those who are not familiar with the properties in the scaling JSON:</p>

<ul>
<li>minSize: defines the minimum size of the cluster</li>
<li>maxSize: defines the maximum size of the cluster</li>
<li>cooldown: defines the time between 2 scaling activity</li>
<li>adjustmentType: can be <code>NODE_COUNT</code>, <code>PERCENTAGE</code>, or <code>EXACT</code></li>
<li>scalingAdjustment: defines the number nodes of with to upscale or downscale and depends on the adjustment type as follows:

<ul>
<li><code>NODE_COUNT</code> can be -2 (downscale with 2 nodes) or +2 (upscale with 2 nodes)</li>
<li><code>PERCENTAGE</code> similarly can be 40% and -40%</li>
<li><code>EXACT</code> always a positive number which can mean both upscale or downscale based on the previous size of the cluster</li>
</ul>
</li>
<li>hostGroup: defines the Hadoop services installed on a host. In case of scaling we&rsquo;ll take or add hosts with these services.</li>
</ul>


<p>Many people reached us with their questions of how to scale down properly as they had some concerns about it.
Generally speaking downscaling is much harder to do than upscaling. Am I going to lose portion of my data? What will happen with the running applications? What will happen with say <code>RegionServers</code>? Luckily Hadoop services provide <code>graceful decommission</code>.</p>

<p><em>Note:When you are storing your data in a cloud object store (last week we have blogged about these <a href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/">here</a> and <a href="http://blog.sequenceiq.com/blog/2014/11/17/datalake-cloudbreak-2/">here</a>) this is less of an issue &ndash; and Periscope will not have to worry about HDFS data replications.</em></p>

<h3>Decommission flow</h3>

<p>Let&rsquo;s dive through an example: Periscope instructs <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; our
Hadoop as a service API &ndash; to shut down 10 nodes and Cloudbreak will make sure that nothing gets lost. First it will check which nodes are running <code>ApplicationMasters</code> to leave them out of the process. If it found all the 10 candidates for shutting down
it will decommission the necessary services from them and then it will shut down those nodes. Applications continue to run and Hadoop <code>master</code> services continue to run undisturbed.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/images/downscale_sequence.png" alt="" /></p>

<p>If you have questions like these don&rsquo;t hesitate to contact us we&rsquo;ll try to help you solve your problems.
Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.5.2 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/24/hadoop-252-docker/"/>
    <updated>2014-11-24T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/24/hadoop-252-docker</id>
    <content type="html"><![CDATA[<p>Following the release cycle of Hadoop -2.5.2 point release- today we are releasing a new <code>2.5.2</code> version of our <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">Hadoop Docker container</a>.</p>

<h2>Centos</h2>

<h3>Build the image</h3>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker/tree/2.5.2">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker:2.5.2 .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h3>Pull the image</h3>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.5.2</span></code></pre></td></tr></table></div></figure>


<h3>Start a container</h3>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker:2.5.2 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.5.2/view/files">Bintray repo</a>. Enjoy.</p>

<p>Should you have any questions let us know through our social channels as <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN containers as Docker containers in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/20/yarn-containers-and-docker/"/>
    <updated>2014-11-20T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/20/yarn-containers-and-docker</id>
    <content type="html"><![CDATA[<p>The new Hadoop 2.6 <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310240&amp;version=12327179">release</a> is almost here with an impressive set of new features and collaboration from the community &ndash; including <a href="http://sequenceiq.com">SequenceIQ</a> as well. It&rsquo;s not a new information that we use Docker quite a lot &ndash; and have <code>containerized</code> the full Hadoop ecosystem, and the <strong>2.6</strong> release will contain a new feature: <a href="https://issues.apache.org/jira/browse/YARN-1964">YARN-1964</a>.</p>

<h2>YARN containers as .. Docker containers</h2>

<p><a href="https://issues.apache.org/jira/browse/MAPREDUCE-279">Introduction</a> of YARN has revolutionized Hadoop &ndash; extending it with a new resource management, opening up Hadoop to different workloads, etc. &ndash; it&rsquo;s all history and we know it. With the emergence and wide adoption of Docker these days we are part of another <code>interesting</code> times again. Hadoop 2.6 will introduce (though in <code>alpha</code>) an analogy of YARN containers as Docker containers.</p>

<p>Just to remember, a container is the resource allocation which is the successful result of the ResourceManager granting a specific ResourceRequest. A Container grants rights to an application to use a specific amount of resources (memory, cpu etc.) on a specific host, and isolates it from other containers. Sounds familiar &ndash; well, among few others this is what exactly Docker does &ndash; with the additional benefit of <code>packaging</code> and <code>shipping</code> applications the easy way.</p>

<!--more-->


<p>Though there is still a long way to go in order to make this support the same features as YARN containers does today the future is very promising. All the networking, accessing external data, etc are already well-trodden paths. As an example <a href="https://registry.hub.docker.com/repos/sequenceiq/?&amp;s=downloads">SequenceIQ containers</a> available and open sourced on the official Docker registry all using some of these needed features.</p>

<h2>Wait, but you run Hadoop in Docker already</h2>

<p>OK, got it. YARN containers are Docker containers. But wait, all the Hadoop ecosystem at <a href="http://sequenceiq.com">SequenceIQ</a> is running inside a cluster of Docker containers, right?
Actually that&rsquo;s true &ndash; we run everything in Docker, and we will use these new feature to package some applications as Docker containers and run them inside YARN containers as Docker containers (err, how many times am I going to write down Docker in one sentence). Basically we will run <strong>Docker in Docker</strong>. How&rsquo;s that possible? Pretty easy and simple, Docker already does support this &ndash; the only requirement is that your Docker version should support the <code>--privileged</code> flag (0.6+).</p>

<p>We have put together a Docker container for you to give it a quick try &ndash; it&rsquo;s available on our <a href="https://github.com/sequenceiq/docker-dind">GitHub page</a>.</p>

<p>Built the container</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build -t sequenceiq/dind .</span></code></pre></td></tr></table></div></figure>


<p>Run the container</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it --rm --privileged -e LOG=file sequenceiq/dind</span></code></pre></td></tr></table></div></figure>


<p>It will start a Docker daemon in the background (unix:///var/run/docker.sock) and provide a bash session where you can play with a vanilla Docker environment. Once you are inside the container you will be able to pull or start any other Docker container &ndash; this is basically will work the same way as the how a YARN Docker container will run inside a Dokcer container.</p>

<p>Once the new <strong>2.6</strong> release will be out we will release it as a Hadoop <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">container</a> (as usuall) as well &ndash; to give you an easy way to try this new feature. Also we are putting together some examples in order to start experimenting this new cool feature &ndash; now that you understand how <strong>Docker in Docker</strong> works and you can start containers inside containers (inside containers, &hellip;) check back soon for some more serious stuff. In the meanwhile make sure you follow us through our <a href="http://blog.sequenceiq.com/">blog</a>, <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the data lake in the cloud - Part2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/17/datalake-cloudbreak-2/"/>
    <updated>2014-11-17T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/17/datalake-cloudbreak-2</id>
    <content type="html"><![CDATA[<p>Few weeks ago we had a <a href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/">post</a> about building a <code>data lake</code> in the cloud using a cloud based <code>object storage</code> as the primary file system.
In this post we&rsquo;d like to move forward and show you how to create an <code>always on</code> persistent datalake with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and create <code>ephemeral</code> clusters which can be scaled up and down based on configured SLA policies using <a href="http://sequenceiq.com/periscope/">Periscope</a>.</p>

<p>Just as a quick reminder &ndash; both are open source projects under Apache2 license and the documentation and code is available following these links below.</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<h2>Sample architecture</h2>

<p>For the sample use case we will create a <code>datalake</code> on <strong>AWS</strong> and <strong>Google Cloud</strong> as well &ndash; and use the most popular data warehouse software with an SQL interface &ndash; <a href="https://hive.apache.org/">Apache Hive</a>.</p>

<!--more-->


<p>From Hive perspective (simplified) while building the <code>datalake</code> there are tree main components:</p>

<ul>
<li>Hive warehouse &ndash; the location where the raw data is stored. Usually it&rsquo;s HDFS, in our case it&rsquo;s the <code>object store</code> &ndash; <strong>Amazon S3</strong> or <strong>Google Cloud Storage</strong></li>
<li>Hive metastore service &ndash; the Hive metastore service stores the metadata for Hive tables and partitions in a relational database &ndash; aka: <strong>metastore DB</strong>, and provides clients (including Hive) access to this information</li>
<li>Metastore database &ndash; a database implementation where the metastore information is stored and the local/remote metastore services talk to, over a JDBC interface</li>
</ul>


<p>The proposed sample architecture is shown on the diagram below &ndash; we have a <strong>permanent</strong> cluster which contains the <code>metastore database</code> and a local <code>metastore service</code>, an <strong>ephemeral</strong> cluster where the <code>metastore service</code> talks to a remote <code>metastore database</code> and the Hive <code>warehouse</code> with the data being stored in the cloud provider&rsquo;s <code>object store</code>.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/blog-test/source/source/images/hive-metastore/hive-permanent-ephemeral.jpg" alt="" /></p>

<p>Setting up a an architecture as such can be pretty complicated and involves a few steps &ndash; where many things could go wrong.</p>

<p>At <a href="http://sequenceiq.com">SequenceIQ</a> we try to automate all these steps and build into our product stack &ndash; and we did exactly the same with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. While a default Hive metastore cluster can be created in a fully automated manner using Cloudbreak <code>blueprints</code> in case of different cloud providers (remember we support AWS, Google Cloud and Azure, Open Stack in the pipeline) there are settings which you will need to apply on each nodes, reconfigure services, etc &ndash; and on a large cluster this is pretty awkward.
Because of these in the next release of Cloudbreak we introduce a new concept called <strong>recipes</strong>. A recipe will embed a full architectural representation of the Hadoop stack &ndash; incorporating all the necessary settings, service configurations &ndash; and allows the end user to bring up clusters as the one(s) discussed in this blog &ndash; with a push of a button, API call or CLI interface.</p>

<h2>Permanent cluster &ndash; on AWS and Google Cloud</h2>

<p>Both Amazon EC2 and Google Cloud allows you to set up a permanent cluster and use their <code>object store</code> for the Hive warehouse. You can set up these clusters with <a href="http://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; overriding the default configurations in the blueprints.</p>

<h4>Using AWS S3 as the Hive warehouse</h4>

<p>This setup will use the S3 Block FileSystem &ndash; as a quick note you need to remember that this is not interoperable with other S3 tools.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "core-site": {
</span><span class='line'>    "fs.s3.awsAccessKeyId": "YOUR ACCESS KEY",
</span><span class='line'>    "fs.s3.awsSecretAccessKey": "YOUR SECRET KEY"
</span><span class='line'>  }
</span><span class='line'>},
</span><span class='line'>{
</span><span class='line'>  "hive-site": {
</span><span class='line'>    "hive.metastore.warehouse.dir": "s3://siq-hadoop/apps/hive/warehouse"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>You will need to create an S3 <code>bucket</code> first &ndash; <code>siq-hadoop</code> in our example &ndash; that will contain the Hive warehouse. After the cluster is up you can start using Hive as usual. When you create a table its metadata will be stored in the MySQL database configured in the blueprint and if you load data in it, it will be moved to the warehouse location on S3. Note that in order to use the <code>LOAD DATA INPATH</code> hive command the source and target directories must be located on the same filesystem, so a file in local HDFS cannot be used.</p>

<h4>Using Google Storage as the Hive warehouse</h4>

<p>This setup will use the Google Storage &ndash; and the GS to HDFS connector.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  "global": {
</span><span class='line'>    "fs.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem",
</span><span class='line'>    "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
</span><span class='line'>    "fs.gs.project.id": "siq-haas",
</span><span class='line'>    "google.cloud.auth.service.account.enable": true,
</span><span class='line'>    "google.cloud.auth.service.account.email": "YOUR_ACCOUNT_ID@developer.gserviceaccount.com",
</span><span class='line'>    "google.cloud.auth.service.account.keyfile": "/mnt/fs1/&lt;PRIVATE_KEY_FILE&gt;.p12"
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>Note that in case of Google being used as an object store you will need to add your account details and the path towards your P12 file. You&rsquo;ll also have to copy the connector JAR to the classpath and the p12 file to every node as mentioned in our previous <a href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/">post</a>.</p>

<h2>Ephemeral cluster &ndash; on AWS and Google Cloud</h2>

<p>Ephemeral Hive clusters are using a very similar configuration: they also have to reach the object store as HDFS so the corresponding configurations must be there in the blueprint. The only <a href="http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.2.2/bk_installing_manually_book/content/rpm-chap6-3.html">additional parameters</a> needed are the ones that configure how the metastore service of the ephemeral cluster will reach the Hive <code>metastore DB</code> in the permanent cluster. Note: on the permanent cluster you will have to configure the <code>metastore DB</code> to allow connections from remote clusters.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "hive-site": {
</span><span class='line'>    "hive.metastore.warehouse.dir": "s3://siq-hadoop/apps/hive/warehouse",
</span><span class='line'>    "javax.jdo.option.ConnectionURL": "jdbc:mysql://$mysql.full.hostname:3306/$database.name?createDatabaseIfNotExist=true",
</span><span class='line'>    "javax.jdo.option.ConnectionDriverName": "com.mysql.jdbc.Driver",
</span><span class='line'>    "javax.jdo.option.ConnectionUserName": "dbusername",
</span><span class='line'>    "javax.jdo.option.ConnectionPassword": "dbpassword"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>As highlighted in this example, building a data lake or data warehouse is pretty simple and can be automated with <a href="http://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; also with the new <code>recipe</code> feature we are standardizing the provisioning of different Hadoop clusters. One of the coming posts will highlight the new architectural changes &ndash; and the components we use for service discovery/registry, failure detection, key/value store for dynamic configuration, feature flagging, coordination, leader election and more.</p>

<p>Make sure you check back soon to our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Extreme OLAP Engine running in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/13/kylin-on-docker/"/>
    <updated>2014-11-13T15:14:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/13/kylin-on-docker</id>
    <content type="html"><![CDATA[<p><em><a href="https://github.com/KylinOLAP/Kylin">Kylin</a> is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets.</em></p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are always interested in the latest emerging technologies, and try to offer those to our customers and the open source community. A few weeks ago <a href="http://www.ebayinc.com/">eBay Inc.</a> released <a href="https://github.com/KylinOLAP/Kylin">Kylin</a> as an open source product and made available for the community under an Apache 2 license. Since we share the approach towards <code>open source</code> software we have partnered with them to <code>Dockerize</code> Kylin &ndash; and made it extremely easy for people to deploy a Kylin locally or in the cloud, using our Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>While there is a pretty good <a href="http://www.kylin.io/document.html">documentation</a> available for Kylin we&rsquo;d like to give you a really short introduction and overview.</p>

<!--more-->


<h2>Overview</h2>

<p>For an overview and the used components and architecture please check this diagram.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/docker-kylin/master/img/kylin_diagram.png" alt="" /></p>

<p>For your reference you can also check the Ambari blueprint to learn the components used by Kylin. Both <a href="https://raw.githubusercontent.com/sequenceiq/docker-kylin/master/kylin-singlenode.json">singlenode</a> and <a href="https://raw.githubusercontent.com/sequenceiq/docker-kylin/master/kylin-multinode.json">multinode</a> blueprint templates are available.</p>

<h2>Kylin cluster running on Docker</h2>

<p>We have put together and fully <code>automated</code> the steps of creating a Kylin cluster. The only thing you will need to do is just pull the container from the <code>official</code> Docker repository by issuing the following command.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/kylin</span></code></pre></td></tr></table></div></figure>


<p>Once the container is pulled you are ready to start playing with Kylin. Get the following helper functions from our Kylin GitHub <a href="https://github.com/sequenceiq/docker-kylin/blob/master/ambari-functions">repository</a> &ndash; <em>(make sure you source it).</em></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> kylin-deploy-cluster 3</span></code></pre></td></tr></table></div></figure>


<p>You can specify the number of nodes you&rsquo;d like to have in your cluster (3 in this case). Once we installed all the necessary Hadoop
services we&rsquo;ll build Kylin on top of it and then you can reach the UI on:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http://&lt;container_ip&gt;:7070</span></code></pre></td></tr></table></div></figure>


<p>The default credentials to login are: <code>admin/KYLIN</code>. The cluster is pre-populated with sample data and is ready to build cubes as shown <a href="https://github.com/KylinOLAP/Kylin/wiki/Kylin-Cube-Creation-Tutorial">here</a>.</p>

<p>Keep up with the latest news with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New YARN features: Label based scheduling]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/10/new-yarn-features-part-1-label-based-scheduling/"/>
    <updated>2014-11-10T15:14:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/10/new-yarn-features-part-1-label-based-scheduling</id>
    <content type="html"><![CDATA[<p>The release of Hadoop 2.6.0 is upon us thus it&rsquo;s time to highlight a few upcoming features, especially those which we are building/planning to use in our Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and our SLA policy based autoscaling API &ndash; <a href="http://sequenceiq.com/periscope/">Periscope</a>.</p>

<p>Recently we explained how the
<a href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/">CapacityScheduler</a> and the <a href="http://blog.sequenceiq.com/blog/2014/09/09/yarn-schedulers-demystified-part-2-fair/">FairScheduler</a>
works and the upcoming release is about to add a few really interesting functionality to them which you should be aware as they might
change the way we think about resource scheduling. The first one which we are going to discuss is the <code>label based scheduling</code> although it&rsquo;s
not fully finished, yet. You can track its progress here: <a href="https://issues.apache.org/jira/browse/YARN-796">YARN-796</a>.</p>

<h2>Motivation</h2>

<p>Hadoop clusters are usually not fully homogeneous which means that different nodes can have different parameters. For example some nodes
have more memory than the others while others have better cpu&rsquo;s or better network bandwidth. At the moment YARN doesn&rsquo;t have the
ability to segregate nodes in a cluster based on their architectural parameters. Applications which are aware of their resource usages
cannot choose which nodes they want to run their containers on. Labels are about to solve this problem. Administrators will have
the ability to <code>mark</code> the nodes with different labels like: cpu, memory, network, rackA, rackB so applications can specify where they&rsquo;d
like to run.</p>

<h2>Cloud</h2>

<p>Things are different in cloud environments as the composition of the Hadoop clusters are more homogeneous. By the nature of cloud it&rsquo;s
easier and more convenient to request nodes with the exact same capabilities. <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a>
our Hadoop as a service API will address this problem, by giving the ability to the users to specify their needs. Take one example: on AWS
users can launch <code>spot price</code> instances which EC2 can <code>take away any time</code>. Labeling them as <code>spot</code> we can avoid spinning up the
<code>ApplicationMasters</code> on those nodes, thus operate safely and re-launch new containers on different nodes in case it happens.
Furthermore <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> with its autoscaling capabilities will be able
to scale out with nodes that are marked with <code>cpu</code>.</p>

<!--more-->


<h2>Terminology</h2>

<p>To start with let&rsquo;s declare the different types of labels and expressions:</p>

<ul>
<li>node label &ndash; describes a node, multiple labels can be specified</li>
<li>queue label &ndash; determines on which nodes the queue can schedule containers</li>
<li>application label &ndash; defines on which nodes the application want to run its containers</li>
<li>label expression &ndash; logical combination of labels (&amp;&amp;, ||, !) e.g: cpu &amp;&amp; rackA</li>
<li>queue label policy &ndash; resolve conflicts on different queue and application labels</li>
</ul>


<h2>Technical details</h2>

<p>Labeling nodes itself is not enough. Schedulers cannot rely only on application requirements as administrators can configure the queues
to act differently. As we discussed earlier, schedulers are defined in a configuration file where you can specify the queues. Initial labeling
can be done in these files:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.alpha.label<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>cpuheavy||rackA<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The value is a <code>label expression</code> that means applications which are submitted to this queue can run either on nodes labeled as
<code>cpuheavy</code> or <code>rackA</code>. As I said the configuration files can be used as an initial configuration, but changing dynamically queue labels
and node labels is also not a problem as the <code>RMAdminCLI</code> <a href="https://issues.apache.org/jira/browse/YARN-2504">allows</a> that.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'> <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-addToClusterNodeLabels&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[label1,label2,label3] (label splitted by \&quot;,\&quot;)&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;add to cluster node labels &quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-removeFromClusterNodeLabels&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[label1,label2,label3] (label splitted by \&quot;,\&quot;)&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;remove from cluster node labels&quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-replaceLabelsOnNode&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;[node1:port,label1,label2 node2:port,label1,label2]&quot;</span><span class="o">,</span>
</span><span class='line'>                  <span class="s">&quot;replace labels on nodes&quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;-directlyAccessNodeLabelStore&quot;</span><span class="o">,</span>
</span><span class='line'>              <span class="k">new</span> <span class="nf">UsageInfo</span><span class="o">(</span><span class="s">&quot;&quot;</span><span class="o">,</span> <span class="s">&quot;Directly access node label store, &quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot;with this option, all node label related operations&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; will not connect RM. Instead, they will&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; access/modify stored node labels directly.&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; By default, it is false (access via RM).&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; AND PLEASE NOTE: if you configured&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; yarn.node-labels.fs-store.root-dir to a local directory&quot;</span>
</span><span class='line'>                  <span class="o">+</span> <span class="s">&quot; (instead of NFS or HDFS), this option will only work&quot;</span>
</span><span class='line'>                  <span class="o">+</span>
</span><span class='line'>                  <span class="s">&quot; when the command run on the machine where RM is running.&quot;</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Declaring the labels is one thing, but how can the <code>ResourceManager</code> enforce that containers run on nodes where the application wants
it to? Let&rsquo;s think the other way around, how can the <code>ResourceManager</code> enforce that containers do not run on nodes where the
application doesn&rsquo;t want it to? The answer is already part of the RM. The <code>ApplicationMaster</code> can <strong>blacklist</strong> nodes. The
<code>AppSchedulingInfo</code> class can decide based on the <code>ApplicationLabelExpression</code> and the <code>QueueLabelExpression</code> whether the resource is
blacklisted or not.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="kd">synchronized</span> <span class="kd">public</span> <span class="kt">void</span> <span class="nf">updateBlacklist</span><span class="o">(</span>
</span><span class='line'>      <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">blacklistAdditions</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">blacklistRemovals</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Add to blacklist</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">blacklistAdditions</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">blacklist</span><span class="o">.</span><span class="na">addAll</span><span class="o">(</span><span class="n">blacklistAdditions</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// Remove from blacklist</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">blacklistRemovals</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">blacklist</span><span class="o">.</span><span class="na">removeAll</span><span class="o">(</span><span class="n">blacklistRemovals</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Okay, we know how to add labels to queues and nodes, but who is going to handle them? A new service will be introduced as part of
the RM called <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java">LabelManager</a>.
Its responsibilities are:</p>

<ul>
<li>load node labels and maintain an internal map of nodes and their labels</li>
<li>dynamically update the label &ndash; node associations (RMAdminCLI, queue configs are reloaded automatically on change)</li>
<li>evaluate label logical expressions for both queue and application</li>
<li>evaluate label expressions against nodes</li>
</ul>


<p>How can applications specify on which nodes they want to run? The <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationSubmissionContext.java#L77">ApplicationSubmissionContext</a>
has been extended with an <code>appLabelExpression</code> and <code>amContainerLabelExpression</code> thus when submitting the job we can specify them. If
we know that our application consumes too much memory and the labels are properly defined it shouldn&rsquo;t be a problem. Providing
an invalid label obviously our application will be rejected. Fairly complex expressions can be given, e.g: (highmemory &amp;&amp; rackA) || master.
Labels can be provided for every <a href="https://github.com/sequenceiq/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceRequest.java#L80">ResourceRequest</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="nd">@Public</span>
</span><span class='line'>  <span class="nd">@Stable</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kd">static</span> <span class="n">ResourceRequest</span> <span class="nf">newInstance</span><span class="o">(</span><span class="n">Priority</span> <span class="n">priority</span><span class="o">,</span> <span class="n">String</span> <span class="n">hostName</span><span class="o">,</span>
</span><span class='line'>      <span class="n">Resource</span> <span class="n">capability</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numContainers</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">relaxLocality</span><span class="o">,</span>
</span><span class='line'>      <span class="n">String</span> <span class="n">labelExpression</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">ResourceRequest</span> <span class="n">request</span> <span class="o">=</span> <span class="n">Records</span><span class="o">.</span><span class="na">newRecord</span><span class="o">(</span><span class="n">ResourceRequest</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setPriority</span><span class="o">(</span><span class="n">priority</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setResourceName</span><span class="o">(</span><span class="n">hostName</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setCapability</span><span class="o">(</span><span class="n">capability</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setNumContainers</span><span class="o">(</span><span class="n">numContainers</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setRelaxLocality</span><span class="o">(</span><span class="n">relaxLocality</span><span class="o">);</span>
</span><span class='line'>    <span class="n">request</span><span class="o">.</span><span class="na">setNodeLabelExpression</span><span class="o">(</span><span class="n">labelExpression</span><span class="o">);</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">request</span><span class="o">;</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>It only makes sense when the resource location is <code>ANY</code> or <code>rack</code> and not <code>data local</code>.</p>

<h2>Summary</h2>

<p>We&rsquo;re going to revisit this feature once it completely finished with a concrete example labeling multiple <code>docker</code> containers
and submit stock examples to see how it works in action. Besides labeling there are other important changes about to come to
the schedulers which will change the way we plan cluster capacities. The <code>CapacityScheduler</code> will be fully dynamic to create/remove/resize
queues, move applications on the fly to make room for the <code>AdmissionControl</code>.</p>

<p>Keep up with the latest news with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us
on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Securing Cloudbreak with OAuth2 - part 2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/06/securing-cloudbreak-with-oauth2-part-2/"/>
    <updated>2014-11-06T17:00:08+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/06/securing-cloudbreak-with-oauth2-part-2</id>
    <content type="html"><![CDATA[<p>A few weeks ago we&rsquo;ve published a <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">blog post</a> about securing our <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> infrastructure with OAuth2.
We&rsquo;ve discussed how we were setting up and configuring a new UAA OAuth2 identity server with Docker but we haven&rsquo;t detailed how to use this identity server in client applications.
And that&rsquo;s exactly what we&rsquo;ll do now: we&rsquo;ll show some code examples about how to obtain tokens from different clients and how to check these tokens in resource servers.</p>

<p>We&rsquo;re using almost every type of the OAuth2 flows in our infrastructure: Cloudbreak and <a href="http://sequenceiq.com/periscope/">Periscope</a> act as resource servers while <a href="https://github.com/sequenceiq/uluwatu">Uluwatu</a> and <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> for example are clients for these APIs.</p>

<h2>Obtaining an access token</h2>

<p>The main goal of an OAuth2 flow is to obtain an access token for the resource owner that can be used to access a resource server later.
There are multiple common flows depending on the client type, we&rsquo;ll have examples for three of them now: <em>implicit</em>, <em>authorization code</em> and <em>client credentials</em>.
If you&rsquo;re not familiar with the roles and expressions that take part in the OAuth2 flows I suggest to check out some <a href="http://aaronparecki.com/articles/2012/07/29/1/oauth2-simplified">&ldquo;Getting started&rdquo; resources</a> first before going forward with this post.</p>

<h3>Implicit flow</h3>

<p>This is not the most common flow with OAuth2 but it is the most simple one because only one request should be made to the identity server and the token will arrive directly in the response.
Two different types of this flow is supported by UAA. One for browser-based applications and one for those scenarios when there is no browser interaction (e.g.: CLIs).
The common part of these scenarios is that it would be useless to have a client secret because it couldn&rsquo;t be kept as a secret.</p>

<p>We are using the <em>implicit flow with credentials</em> in the <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak Shell</a>.
When using the shell you must provide your SequenceIQ credentials as environment variables and the shell uses those to obtain an access token.
Cloudbreak shell is written in Java but let&rsquo;s see a basic <code>curl</code> example instead &ndash; it does exactly the same as the Java code. (If you&rsquo;re still eager you can check out the code <a href="https://github.com/sequenceiq/cloudbreak-shell/blob/master/src/main/java/com/sequenceiq/cloudbreak/shell/configuration/ShellConfiguration.java#L122">here</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -iX POST -H "accept: application/x-www-form-urlencoded"  \
</span><span class='line'> -d 'credentials={"username":"admin","password":"periscope"}' \
</span><span class='line'> "http://localhost:8080/oauth/authorize?response_type=token&client_id=cli&scope.0=openid&redirect_uri=http://cli"</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p><em>notes:</em></p>

<ul>
<li>the <code>response_type=token</code> part tells the identity server to return a token <em>implicitly</em></li>
<li>UAA must be running on <code>localhost:8080</code></li>
<li>there is a registered client in UAA with <code>implicit</code> as <em>authorized_grant_type</em>, <code>cli</code> as <em>client ID</em>, and <code>http://cli</code> as <em>redirect URI</em> (it doesn&rsquo;t need to be a valid URL but has to match the one in the request)</li>
<li>the <code>cli</code> client is configured in UAA as <em>autoapproved</em></li>
<li>a user with <code>admin</code> as username and <code>periscope</code> as password is registered in UAA</li>
</ul>


<p>If you&rsquo;re having a browser-based application and would like to use the implicit flow it is very similar.
The main difference is that you won&rsquo;t have to provide the credentials in the request body but redirect the user to the same URL.
User authentication will happen through a login form and the <code>access_token</code> will appear as a parameter in the redirect URI.
You can simply try this out by opening the same URL in a browser. (Of course the redirect won&rsquo;t be successful with the URI above but the redirect URL will appear in the browser with the access token as a parameter if UAA is properly configured)</p>

<h3>Authorization code flow</h3>

<p>The authorization code flow is the most common one &ndash; it is used mostly by standard web applications that have some server side code besides the frontend.
It&rsquo;s main advantage against the implicit flow is that the token doesn&rsquo;t show up in the browser, only an authorization code is sent back by the identity server to the browser and it will be exchanged for an access token later in some kind of server side code.
We&rsquo;re using the authorization code flow with Uluwatu that&rsquo;s written in <em>node.js</em> so I&rsquo;ll show some <em>node.js</em> examples here</p>

<p>The first part of the authorization code flow is almost exactly the same as the browser-based implicit flow: you&rsquo;ll have to redirect the user to the <code>oauth/authorize</code> endpoint, but with a different <code>response_type</code> (<em>code</em>). The response is a redirect again but instead of the access token an authorization code is sent back as a parameter. You can still try it out in a browser &ndash; of course the redirect won&rsquo;t be successful if there&rsquo;s nothing listening on the redirect URI but the code will appear in the browser.</p>

<p>A somewhat simplified version of the code we&rsquo;re using <a href="https://github.com/sequenceiq/uluwatu/blob/master/server.js#L123">in Uluwatu</a> to start the process looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  var authUrl = uaaAddress
</span><span class='line'>    + 'oauth/authorize?response_type=code'
</span><span class='line'>    + '&client_id=' + clientId
</span><span class='line'>    + '&scope=' + clientScopes
</span><span class='line'>    + '&redirect_uri=' + redirectUri
</span><span class='line'>  if (!req.session.token){
</span><span class='line'>    res.redirect(authUrl)
</span><span class='line'>  } else {
</span><span class='line'>    res.render('index');
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li>the <code>response_type=code</code> part tells the identity server to return an authorization code instead of an access token</li>
<li>UAA must be available on the address specified in the uaaAddress variable</li>
<li>there is a registered client in UAA with <code>authorization_code</code> as <em>authorized_grant_type</em>, and its <em>client ID</em> and <em>redirect URI</em> parameters must be specified in the <code>clientId</code> and <code>redirectUri</code> variables</li>
<li>we&rsquo;re using the <a href="http://expressjs.com">Express</a> web framework for node.js.</li>
</ul>


<p>The second part is about exchanging the authorization code for an access token. To try it out you&rsquo;ll need a web server that will handle the redirect URI. In our case it is done by the <a href="https://github.com/sequenceiq/uluwatu/blob/master/server.js#L100">Uluwatu backend</a> on the <code>/authorize</code> endpoint.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var optionsAuth = { user: clientId, password: clientSecret },
</span><span class='line'>    identityServerClient = new restClient.Client(optionsAuth);
</span><span class='line'>
</span><span class='line'>identityServerClient.registerMethod("retrieveToken", uaaAddress + "oauth/token", "POST");
</span><span class='line'>
</span><span class='line'>app.get('/authorize', function(req, res, next){
</span><span class='line'>  var args = {
</span><span class='line'>    headers: { "Content-Type": "application/x-www-form-urlencoded" },
</span><span class='line'>    data:
</span><span class='line'>      'grant_type=authorization_code'
</span><span class='line'>      + '&redirect_uri=' + redirectUri
</span><span class='line'>      + '&code=' + req.query.code
</span><span class='line'>  }
</span><span class='line'>  identityServerClient.methods.retrieveToken(args, function(data, response){
</span><span class='line'>    req.session.token=data.access_token;
</span><span class='line'>    res.redirect('/');
</span><span class='line'>  });
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li>the <em>POST</em> request must be made to the <code>oauth/token</code> endpoint</li>
<li>the client must authenticate himself by putting the <em>client id</em> and <em>client secret</em> in a standard basic authentication header. The base64 encoding is done by the client library we&rsquo;re using.</li>
<li>the access token arrives in the response body along with a refresh token that can be used to renew the access token when it expires</li>
</ul>


<h3>Client credentials flow</h3>

<p>This one is a bit different from the previous ones because this flow is used when a client would like to access some resources by itself, not on behalf of a user.
A common use case with UAA is when we&rsquo;d like to access the <a href="http://www.simplecloud.info/">SCIM</a> endpoints for describing or registering users.
<a href="https://github.com/sequenceiq/sultans">Sultans</a> is the user management service for the SequenceIQ platform, it uses the client credentials flow in its <a href="https://github.com/sequenceiq/sultans/blob/master/main.js#L216">source</a> to obtain a token that&rsquo;s used for example to register new users later.
Just to keep it simple that&rsquo;s how it looks like in <code>curl</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -u &lt;CLIENT_ID&gt;:&lt;CLIENT_SECRET&gt; -d 'grant_type=client_credentials' http://localhost:8080/oauth/token</span></code></pre></td></tr></table></div></figure>


<p>And that&rsquo;s how a correct sample response looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{"access_token":"&lt;ACCESS_TOKEN&gt;","token_type":"bearer","expires_in":43199,"scope":"password.write scim.write scim.read uaa.resource","jti":"&lt;JTI&gt;"}</span></code></pre></td></tr></table></div></figure>


<p><em>notes:</em></p>

<ul>
<li><p>the scopes of a client is described in the authorities property in the UAA configuration.</p></li>
<li><p>an UAA server must be available on localhost:8080</p></li>
<li><p>a client must be registered in UAA&rsquo;s database with <code>client_credentials</code> as <em>grant_type</em></p></li>
</ul>


<h2>Using the access token to make requests to a resource server</h2>

<p>The <a href="http://self-issued.info/docs/draft-ietf-oauth-v2-bearer.html">Bearer Token Usage part</a> of the OAuth 2.0 specification talks about how to include the access token in a request. According to the specification there are several ways to send the token:</p>

<ul>
<li>in the Authorization request header field</li>
<li>in a form-encoded body parameter</li>
<li>in a URI query parameter</li>
</ul>


<p>Only the first of these (the Authorization header) <em>must</em> be supported by resource servers, the others are only optional.
Here&rsquo;s how the Authorization header should look like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /resource HTTP/1.1
</span><span class='line'>Host: server.example.com
</span><span class='line'>Authorization: Bearer mF_9.B5f-4.1JqM</span></code></pre></td></tr></table></div></figure>


<h2>Checking the token in the resource server</h2>

<p>Now that we are able to deploy and configure an UAA identity server, obtain tokens from it in client applications and send these in resource server requests there&rsquo;s only one thing left: how should we implement the resource server part of our infrastructure to handle the token requests. The OAuth 2.0 specification leaves it up to the implementor but with UAA there is one recommended way, the <code>/check_token</code> <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#oauth2-token-validation-service-post-check_token">endpoint</a>.</p>

<p>The boundaries of resource servers and OAuth2 providers are often blurred and they are in the same application therefore checking a token can be implemented in place by going directly to a token store or decoding the JWT token. If the components are correctly separated this can only be done if the token is encrypted with a shared secret between the provider and the resource server. If it&rsquo;s not the case the resource server must reach out to the identity server to check the validity of the token. In case of UAA this can be achieved with the help of the <code>check_token</code> endpoint.</p>

<p>Our resource servers are implemented in Java and are using Spring. Spring has great support for <a href="http://projects.spring.io/spring-security-oauth/">OAuth</a> but it could feel like <strong>magic</strong> if you don&rsquo;t know what&rsquo;s behind it.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@Configuration
</span><span class='line'>@EnableResourceServer
</span><span class='line'>protected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter {
</span><span class='line'>
</span><span class='line'>    @Bean
</span><span class='line'>    RemoteTokenServices remoteTokenServices() {
</span><span class='line'>        RemoteTokenServices rts = new RemoteTokenServices();
</span><span class='line'>        rts.setClientId(clientId);
</span><span class='line'>        rts.setClientSecret(clientSecret);
</span><span class='line'>        rts.setCheckTokenEndpointUrl(identityServerUrl + "/check_token");
</span><span class='line'>        return rts;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    @Override
</span><span class='line'>    public void configure(ResourceServerSecurityConfigurer resources) throws Exception {
</span><span class='line'>        resources.resourceId("cloudbreak");
</span><span class='line'>        resources.tokenServices(remoteTokenServices());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    @Override
</span><span class='line'>    public void configure(HttpSecurity http) throws Exception {
</span><span class='line'>        http
</span><span class='line'>            .authorizeRequests()
</span><span class='line'>            .antMatchers("/user/blueprints").access("#oauth2.hasScope('cloudbreak.blueprints')")
</span><span class='line'>            .antMatchers("/user/templates").access("#oauth2.hasScope('cloudbreak.templates')");
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>With these few lines you&rsquo;ll have a fully functioning resource server that checks every incoming token on the two endpoints defined in the second <code>configure</code> method.
The <code>EnableResourceServer</code> annotation will include a new <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/authentication/OAuth2AuthenticationProcessingFilter.java#L95">filter</a> in the security filter chain that will use the <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/token/RemoteTokenServices.java#L95">RemoteTokenServices class</a> to make a request to the <code>check_token</code> endpoint. If the response doesn&rsquo;t contain errors it uses a custom <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/authentication/OAuth2AuthenticationManager.java#L77">authentication manager</a> to put the authentication in the Spring authentication context (username will be available through the <code>Principal</code> object). It is also very easy to configure which scopes are needed for specific endpoints &ndash; the expressions used in the configuration are processed by the <a href="https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/provider/expression/OAuth2SecurityExpressionMethods.java">OAuth2SecurityExpressionMethods class</a>.</p>

<p>The <code>check_token</code> endpoint in UAA uses basic authentication with the <strong>resource server&rsquo;s</strong> client id and client secret as username and password. That&rsquo;s why the <em>resource server must be configured in UAA as a client</em> as well. The access token must be included in the request body:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>POST /check_token HTTP/1.1
</span><span class='line'>Host: server.example.com
</span><span class='line'>Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==
</span><span class='line'>Content-Type: application/x-www-form-encoded
</span><span class='line'>
</span><span class='line'>token=eyJ0eXAiOiJKV1QiL</span></code></pre></td></tr></table></div></figure>


<p>A successful response will include the decoded parts of the JWT token such as <code>exp</code>, <code>scope</code>, <code>user_name</code> or <code>client_id</code>. See an example <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-APIs.rst#oauth2-token-validation-service-post-check_token">here</a>.</p>

<h2>Resources</h2>

<p>I haven&rsquo;t included an example for the password grant type because we are not using it in our projects but you can check it out in the <a href="https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-Tokens.md#getting-started"><strong>Tokens part</strong></a> of the UAA documentation. If you&rsquo;d like to learn more about UAA, check out its <a href="https://github.com/cloudfoundry/uaa/tree/master/docs">documentation</a> or the source code of our projects in our <a href="https://github.com/sequenceiq/">Github repo</a>. Also feel free to ask anything in the comments section.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Timeline Service]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/04/yarn-timeline-service-tez/"/>
    <updated>2014-11-04T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/04/yarn-timeline-service-tez</id>
    <content type="html"><![CDATA[<p>As you may know from our earlier <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">blogposts</a> we are continuously monitoring and trying to find out what happens inside our YARN clusters, let it be MapReduce jobs, TEZ DAGs, etc&hellip; We&rsquo;ve analyzed our clusters from various aspects so far; now it&rsquo;s the time to take a look at the information provided by the built YARN <code>timeline</code> service.</p>

<p>This post is about how to set up a YARN cluster so that the Timeline Server is available and how to configure applications running in the cluster to report information to it. As an example we&rsquo;ve chosen to run a simple TEZ example. (MapReduce2 also reports to the <code>timeline</code> service)</p>

<p>As a playground we will use a multinode cluster set up on the local machine; alternatively one could do the same on a cluster provisioned with <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a>. Cluster nodes run in Docker containers, YARN / TEZ provisioning and configuration is done with <a href="http://ambari.apache.org/">Apache Ambari</a>.</p>

<h2>Building a multinode cluster</h2>

<p>To build a multinode cluster we use a set of commodity functions that you can install by running the following in a terminal:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari && . .amb</span></code></pre></td></tr></table></div></figure>


<p>(The commodity functions use our docker-ambari image: sequenceiq/ambari:1.6.0)</p>

<!-- more -->


<p>With the functions installed, you can start your cluster by running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-start-cluster 3</span></code></pre></td></tr></table></div></figure>


<p>After a couple of seconds you&rsquo;ll have a running 3-node Ambari cluster.</p>

<h2>Create an Ambari blueprint with the Timeline Server configuration entries</h2>

<p>To provision and configure Hadoop services we use Ambari and Ambari blueprints. Check this <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">blogpost</a> about how to setup an multi-node Hadoop cluster.</p>

<p>To enable the Timeline Server in the cluster, we&rsquo;ve created a blueprint which contains a few overrides of the related configuration properties. A detailed description of the configuration settings for the Timeline Server are described <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">here</a> and <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.5/bk_system-admin-guide/content/ch_application-timeline-server.html">here</a>.</p>

<p>We used <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/timeline-server/blueprints/multi-node-hdfs-yarn-tez-timeline-service.json">this</a> blueprint for the experiment.</p>

<p>Please note, that the blueprint here only contains those configuration entries that differ from the defaults; the assumption is that the other defaults are similar to those described in the documentation. It&rsquo;s always possible to override any of the defaults by adding them to the blueprint, or using the Ambari UI.</p>

<h1>Create the YARN cluster with ambari-shell</h1>

<p>Now it&rsquo;s time to provision our cluster with YARN, TEZ and the Timeline Server enabled. For this let&rsquo;s start the <code>ambari-shell</code>, which, surprisingly runs in a docker container as well.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-shell</span></code></pre></td></tr></table></div></figure>


<p>Following the instructions below you can provision the Timeline Server enabled cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#add the blueprint
</span><span class='line'>blueprint add --url https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/timeline-server/blueprints/multi-node-hdfs-yarn-tez-timeline-service.json
</span><span class='line'>#build the cluster
</span><span class='line'>cluster build --blueprint multi-node-hdfs-yarn-tez-timeline-service
</span><span class='line'>#auto assign nodes
</span><span class='line'>cluster autoAssign
</span><span class='line'>#create the cluster
</span><span class='line'>cluster create</span></code></pre></td></tr></table></div></figure>


<p>After services start, you can reach the Timeline Server on the port 8188 of the ambari host.</p>

<p>There is some more configuration needed for the Timeline Server to work properly, we have to set the following entries to the address where the timeline service is running. You can get the proper value from Ambari &ndash;  the Timeline Server runs where the resource manager is.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yarn.timeline-service.webapp.address
</span><span class='line'>yarn.timeline-service.webapp.https.address
</span><span class='line'>yarn.timeline-service.address</span></code></pre></td></tr></table></div></figure>


<p>This can be done from the Ambari web UI; a restart of the YARN services is needed after the values are saved.</p>

<h1>Check the information in the Timeline Server</h1>

<p>With the cluster and the Timeline Server set up every MR2 and TEZ application starts reporting to the <code>timeline</code> service. Information is made available at <code>http://&lt;ambari-host:8188&gt;</code>. You can also inspect application related information using the command line, as described in the aforementioned documentation.</p>

<p>As we mentioned at the beginning of this post, we choose TEZ to show you how to use the Timeline Server. After running the Tez application in the Timeline Server web UI you will have fine grained generic information about the application, application attempt, containers used by the application, etc.</p>

<p>You can find a few screenshots of the web ui  <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/timeline-server/ts-screenshots/gen-app-logs">here</a>.</p>

<p>If you&rsquo;d like to have a vizualized view of the application   you can use the <em>swimlanes</em> tez tool. Based on the information provided by the Timeline Server this generates images similar to <a href="https://github.com/sequenceiq/sequenceiq-samples/blob/master/timeline-server/ts-screenshots/swimlanes/topk_topk_stark_application_1415093602516_0016.svg">this</a></p>

<p>If you are curious what framework related information have been logged, you can access the Timeline Server RESTful interface.
You can get very deep details similar to the ones in <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/timeline-server/ts-screenshots/framework-logs">these</a> screenshots</p>

<p>For further details follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark on Tez execution context - running in Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez/"/>
    <updated>2014-11-02T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/11/02/spark-on-tez</id>
    <content type="html"><![CDATA[<p>Last week Hortonworks <a href="http://hortonworks.com/blog/improving-spark-data-pipelines-native-yarn-integration/">announced</a> improvements for running Apache Spark at scale by introducing a new pluggable <code>execution context</code> and has <a href="https://github.com/hortonworks/spark-native-yarn-samples">open sourced</a> it.</p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are always trying to work and offer the latest technology solutions for our clients and help them to choose their favorite technology/option. We are running a project called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> &ndash; to be open sourced soon &ndash; with the goal (among many others) to abstract and allow our customers to use their favorite big data runtime: MR2, Spark or Tez. Along this process we have <code>dockerized</code> most of the Hadoop ecosystem &ndash; we are running MR2, Spark, Storm, Hive, HBase, Pig, Oozie, Drill etc in Docker containers &ndash; on bare metal and in the cloud as well (all of these containers have made <strong>top</strong> downloads on the official Docker repository). For details you can check these older posts/resources:</p>

<table>
<thead>
<tr>
<th></th>
<th> Name                  </th>
<th> Description </th>
<th> Documentation </th>
<th> GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Apache Hadoop  </td>
<td> Pseudo distributed container </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/">http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/hadoop-docker">https://github.com/sequenceiq/hadoop-docker</a></td>
</tr>
<tr>
<td></td>
<td> Apache Ambari   </td>
<td> Multi node &ndash; full Hadoop stack, blueprint based </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/</a> </td>
<td> <a href="https://github.com/sequenceiq/docker-ambari">https://github.com/sequenceiq/docker-ambari</a></td>
</tr>
<tr>
<td></td>
<td> Cloudbreak         </td>
<td> Cloud agnostic Hadoop as a Service </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/</a> </td>
<td> <a href="https://github.com/sequenceiq/cloudbreak">https://github.com/sequenceiq/cloudbreak</a></td>
</tr>
<tr>
<td></td>
<td> Periscope          </td>
<td> SLA policy based autoscaling for Hadoop clusters </td>
<td> <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/</a> </td>
<td> <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></td>
</tr>
</tbody>
</table>


<p>We have always been big fans on Apache Spark &ndash; due to the simplicity of development and at the same time we are big fans of Apache Tez, for reasons we have <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">blogged before</a>.</p>

<p>When the <a href="https://issues.apache.org/jira/browse/SPARK-3561">SPARK-3561</a> has been submitted we were eager to get our hands on the WIP and early implementation &ndash; and this time we&rsquo;d like to help you with a quick ramp-up and easy solution to have a Spark Docker <a href="https://github.com/sequenceiq/docker-spark-native-yarn">container</a> where the <code>execution context</code> has been changed to <a href="http://tez.apache.org/">Apache Tez</a> and everything is preconfigured. The only thing you will need to do is to follow these easy steps.</p>

<h3>Pull the image from the Docker Repository</h3>

<p>We suggest to always pull the container from the official Docker repository &ndash; as this is always maintained and supported by us.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/spark-native-yarn</span></code></pre></td></tr></table></div></figure>


<p>Once you have pulled the container you are ready to run the image.</p>

<h3>Run the image</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t -h sandbox sequenceiq/spark-native-yarn /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>You have now a fully configured Apache Spark, where the <code>execution context</code> is <a href="http://tez.apache.org/">Apache Tez</a>.</p>

<h3>Test the container</h3>

<p>We have pushed sample data and tests from the <a href="https://github.com/hortonworks/spark-native-yarn-samples">code repository</a> into the Docker container, thus you can start experimenting right away without writing one line of code.</p>

<h4>Calculate PI</h4>

<p>Simplest example to test with is the <code>PI calculation</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local/spark
</span><span class='line'>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-examples-1.1.0.2.1.5.0-702-hadoop2.4.0.2.1.5.0-695.jar</span></code></pre></td></tr></table></div></figure>


<p>You should expect something like the following as the result:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Pi is roughly 3.14668</span></code></pre></td></tr></table></div></figure>


<h4>Run a KMeans example</h4>

<p>Run the <code>KMeans</code> example using the sample dataset.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.KMeans --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/kmeans_data.txt</span></code></pre></td></tr></table></div></figure>


<p>You should expect something like the following as the result:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Finished iteration (delta = 0.0)
</span><span class='line'>Final centers:
</span><span class='line'>DenseVector(0.15000000000000002, 0.15000000000000002, 0.15000000000000002)
</span><span class='line'>DenseVector(9.2, 9.2, 9.2)
</span><span class='line'>DenseVector(0.0, 0.0, 0.0)
</span><span class='line'>DenseVector(9.05, 9.05, 9.05)</span></code></pre></td></tr></table></div></figure>


<h4>Other examples (Join, Partition By, Source count, Word count)</h4>

<p>Join</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.Join --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/join1.txt /sample-data/join2.txt</span></code></pre></td></tr></table></div></figure>


<p>Partition By</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.PartitionBy --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/partitioning.txt</span></code></pre></td></tr></table></div></figure>


<p>Source count</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.SourceCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt</span></code></pre></td></tr></table></div></figure>


<p>Word count</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-submit --class sample.WordCount --master execution-context:org.apache.spark.tez.TezJobExecutionContext --conf update-classpath=true ./lib/spark-native-yarn-samples-1.0.jar /sample-data/wordcount.txt 1</span></code></pre></td></tr></table></div></figure>


<p>Note that the last argument (1) is the number of <code>reducers</code>.</p>

<h3>Using the Spark Shell</h3>

<p>The Spark shell works out of the box with the new Tez <code>executor context</code>, the only thing you will need to do is run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./bin/spark-shell --master execution-context:org.apache.spark.tez.TezJobExecutionContext</span></code></pre></td></tr></table></div></figure>


<h3>Summary</h3>

<p>Right after the next day that <a href="https://github.com/hortonworks/spark-native-yarn-samples">SPARK-3561</a> has been made available we have started to test at scale using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and run performance tests by using the same Spark jobs developed in Banzai (over 50 individual jobs) using the same input sets, cluster size and Scala code &ndash; but changing the default <code>Spark context</code> to a <code>Tez context</code>. Follow up with us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a> as we will release these test results and the lessons we have learned in the coming weeks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying a Hadoop Cluster - DevOps way]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops/"/>
    <updated>2014-10-30T12:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/30/cloudbreak-devops</id>
    <content type="html"><![CDATA[<p>A while ago we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider &ndash; record the process and automate it.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Are we <code>obsessed with automation</code>? Definitely yes &ndash; all the step which are candidates of doing it twice we script or automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/cloudbreak-shell.git
</span><span class='line'>cd cloudbreak-shell
</span><span class='line'>mvn clean package</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p><em>Note: In case you use the hosted version of Cloudbreak you should use the <code>latest-release.sh</code> to get the right version of the CLI.
In case you build your own Cloudbreak from the <code>master</code> branch you should use the <code>latest-snap.sh</code> to get the right version of the CLI.</em></p>

<!--more-->


<h2>Sign in and connect to Cloudbreak</h2>

<p>There are several different ways to use the shell. First of all you&rsquo;ll need a Cloudbreak instance you can connect to. The easiest way is to use our hosted solution &ndash; you can access it with your SequenceIQ credentials. If you don&rsquo;t have an account, you can subscribe <a href="https://accounts.sequenceiq.com/register">here</a>.</p>

<p>Alternatively you can host your own Cloudbreak instance &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. If you&rsquo;re hosting your own Cloudbreak server you can still use your SequenceIQ credentials and our identity server, but then you&rsquo;ll have to configure your Cloudbreak installation with proper client credentials that will be accepted by our identity server. It is currently not supported to register your Cloudbreak application through an API (but it is planned), so contact us if you&rsquo;d like to use this solution.</p>

<p>The third alternative is to deploy our whole stack locally in your organization along with <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">Cloudbreak</a>, our OAuth2 based <a href="http://blog.sequenceiq.com/blog/2014/10/16/using-uaa-as-an-identity-server/">Identity Server</a>, and our user management application, <a href="https://github.com/sequenceiq/sultans">Sultans</a>.</p>

<p>We suggest to try our hosted solution as in case you have any issues we can always help you. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).</p>

<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
</span><span class='line'>  java -jar cloudbreak-shell-0.2-SNAPSHOT.jar --cmdfile=&lt;FILE&gt; : Cloudbreak executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --cloudbreak.address=&lt;http[s]://HOSTNAME:PORT&gt;  Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].
</span><span class='line'>  --identity.address=&lt;http[s]://HOSTNAME:PORT&gt;    Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].
</span><span class='line'>  --sequenceiq.user=&lt;USER&gt;                        Username of the SequenceIQ user [default: user@sequenceiq.com].
</span><span class='line'>  --sequenceiq.password=&lt;PASSWORD&gt;                Password of the SequenceIQ user [default: password].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  You should specify at least your username and password.</span></code></pre></td></tr></table></div></figure>


<p>Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use <code>hint</code>. You can always use <code>TAB</code> for completion. Note that all commands are <code>context aware</code> &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential select --id #ID of the credential</span></code></pre></td></tr></table></div></figure>


<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template createEC2 --name awstemplate --description aws-template  --region EU_WEST_1 --instanceType M3Xlarge --volumeSize 100 --volumeCount 2</span></code></pre></td></tr></table></div></figure>


<p>You can check whether the template was created successfully by using the <code>template list</code> command. Check the template and select it if you are happy with it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template show --id #ID of the template
</span><span class='line'>
</span><span class='line'>template select --id #ID of the template</span></code></pre></td></tr></table></div></figure>


<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stack create --name “myStackName" --nodeCount 10</span></code></pre></td></tr></table></div></figure>


<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blueprint list
</span><span class='line'>
</span><span class='line'>blueprint select --id #ID of the blueprint</span></code></pre></td></tr></table></div></figure>


<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster create --description “my cluster desc"</span></code></pre></td></tr></table></div></figure>


<p>You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the data lake in the cloud - Part1]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak/"/>
    <updated>2014-10-28T13:56:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/28/datalake-cloudbreak</id>
    <content type="html"><![CDATA[<p>A while ago we have released our cloud agnostic and Docker container based Hadoop as a Service API &ndash; <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Though the purpose of <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> is to quickly provision arbitrary sized Hadoop clusters in the cloud, the project emerged from bare metal Hadoop provisioning in Docker containers. We were (still doing it) <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">provisioning</a> Hadoop on bare metal using Docker &ndash; and because of this legacy the data was always stored in HDFS. Recently we have been asked to run a proof-of-concept project and build an <code>always on</code> data lake using a cloud <code>object storage</code>.</p>

<p>This post is the first in this series and will cover the connectivity, interoperability and access of data from an <code>object storage</code> and work with that in Hadoop. For this post we choose to create a <code>data lake</code> on Google Cloud Compute and guide you through the steps, run performance tests and understand the benefits/drawbacks of such a setup.</p>

<p><em>Next post will be about sharing the <code>data lake</code> among multiple clusters, using <a href="http://hortonworks.com/hadoop/hcatalog/">Apache HCatalog</a>.</em></p>

<h2>Object storage</h2>

<p>An object storage usually is an <code>internet service</code> to store data in the cloud and comes with a programming interface which allows to retrieve data in a secure, durable and highly-scalable way. The most well know object storage is <strong>Amazon S3</strong> &ndash; with a pretty well covered literature, thus in this example we will use the <strong>Google Cloud Storage</strong>. Google Cloud Storage enables application developers to store their data on Google’s infrastructure with very high reliability, performance and availability, and can be used to distribute large data objects &ndash; like HDFS. In many occasions companies stores their data in objects storages &ndash; but for analytics they would like to access it from their Hadoop cluster. There are several options available:</p>

<ul>
<li>replicate the full dataset in HDFS</li>
<li>read and write from <code>object storage</code> at start/stop of the flow and use HDFS for intermediary data</li>
<li>use a connector such as Google Cloud Storage Connector for Hadoop</li>
</ul>


<h2>Google Cloud Storage Connector for Hadoop</h2>

<p>Using <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">this</a> connector developed by Google allows you to choose <code>Google Cloud Storage</code> as the default file system for Hadoop, and run all your jobs on top (we will come up with MR2 and Spark examples). Using the connector can have several benefits, to name a few:</p>

<ul>
<li>Direct data access &ndash; data is stored in GCS, no need to transfer it into HDFS</li>
<li>HDFS compatibility &ndash; data stored in HDFS can be accessed through the connector</li>
<li>Data accessibility &ndash; data is always accessible, even when the Hadoop cluster is shut down</li>
<li>High data availability &ndash; data is highly available and globally replicated</li>
</ul>


<!-- more -->


<h2>DIY &ndash; build your data lake</h2>

<p>Follow these steps in order to create your own <code>data lake</code>.</p>

<ol>
<li>Create your <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak account</a></li>
<li>Configure your Google Cloud account following these <a href="http://sequenceiq.com/cloudbreak/#accounts">steps</a></li>
<li>Copy the appropriate version of the <a href="https://cloud.google.com/hadoop/google-cloud-storage-connector">connector jar</a> to the Hadoop classpath and the key file for auth on every node of the cluster &ndash; use this <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/copyscripts.sh">script</a> to automate the process</li>
<li>Use this Ambari <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/data-lake/gcs-con-multi-node-hdfs-yarn.blueprint">blueprint</a> to configure the connector</li>
<li>Restart the following services: HDFS, YARN and MapReduce2</li>
</ol>


<p>That&rsquo;s it &ndash; you are done, you can work on your data stored in Google Storage. The next release of <a href="https://github.com/sequenceiq/cloudbreak">Cloudbreak</a> will incorporate and automate these steps for you &ndash; and will use HCatalog to allow you to configure an <code>always on</code> data store using object storages.</p>

<h2>Performance results</h2>

<p>We configured two identical clusters with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> on Google Cloud with the following parameters</p>

<ul>
<li>Number of nodes: 1 master node + 10 slave nodes</li>
<li>2 * 200 GB rotating HDD (where appropriate)</li>
<li>2 Virtual CPU</li>
<li>7.5 GB of memory</li>
</ul>


<p>First of all we run all the Hadoop and the certification tests in order to validate the correctness of the setups. For the tests we have provisioned an <strong>Hortonwork&rsquo;s HDP 2.1</strong> cluster.</p>

<p>After these steps we have switched to the <code>standard</code> performance test &ndash; <strong>TeraGen, TeraSort and TeraValidate</strong>. Please see the results below.</p>

<table>
<thead>
<tr>
<th></th>
<th> File System           </th>
<th> TeraGen </th>
<th> TeraSort </th>
<th> TeraValidate</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> HDFS                  </td>
<td>58mins, 58sec</td>
<td>4hrs, 59mins, 6sec</td>
<td>35mins, 58sec</td>
</tr>
<tr>
<td></td>
<td> Google Cloud Storage  </td>
<td>34mins, 36sec</td>
<td>4hrs, 34mins, 52sec</td>
<td> 29mins, 22sec</td>
</tr>
</tbody>
</table>


<h2>Summary</h2>

<p>There is a pretty good literature about HDFS and object storages and lots of debates around. At <a href="http://sequenceiq.com">SequenceIQ</a> we support both &ndash; and we also believe that each and every company or use case has his own rationale behind choosing one of them. When we came up with the mission statement of simplifying how people work with Hadoop and stated that we&rsquo;d like to give the broadest available options to developers we were pretty serious about.</p>

<p><a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> was designed around being cloud agnostic &ndash; running on Docker and being able to ship those containers to bare metal or any cloud provider with a very easy integration process: currently we support <strong>Amazon AWS, Microsoft Azure and Google Cloud</strong> in public beta and <strong>OpenStack, Digital Ocean</strong> integration in progress/private beta.
As for the supported Hadoop distribution we provision <strong>Apache Hadoop and Hortonworks HDP</strong> in public and <strong>Cloudera CDH</strong> in private beta.</p>

<p>All the private betas will emerge into public programs and will be in GA &ndash; and open sourced under an Apache2 license during Q4.</p>

<p><a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> will be released quite soon &ndash; stay tuned &ndash; will support one API/representation of your big data pipeline and running on multiple runtimes: <strong>MR2, Spark and Tez</strong>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or
<a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark RDD operation examples]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/23/spark-operations-overview/"/>
    <updated>2014-10-23T14:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/23/spark-operations-overview</id>
    <content type="html"><![CDATA[<p>Recently we blogged about how you can write simple Apache Spark jobs and how to test them. Now we&rsquo;d like to introduce all basic RDD operations with easy examples (our goal is to come up with examples as simply as possible). The Spark <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations">documentation</a> explains well what each operations is doing in detail. We made tests for most of the RDD operations with good ol&#8217; <code>TestNG</code>. e.g.:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="nd">@Test</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">testRightOuterJoin</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input1</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="mi">4</span><span class="o">)))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input2</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">makeRDD</span><span class="o">(</span><span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="sc">&#39;1&#39;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="sc">&#39;2&#39;</span><span class="o">)))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">expectedOutput</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="mi">4</span><span class="o">),</span> <span class="sc">&#39;1&#39;</span><span class="o">)),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="o">(</span><span class="nc">None</span><span class="o">,</span> <span class="sc">&#39;2&#39;</span><span class="o">)))</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">rightOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">Assert</span><span class="o">.</span><span class="n">assertEquals</span><span class="o">(</span><span class="n">output</span><span class="o">.</span><span class="n">collect</span><span class="o">(),</span> <span class="n">expectedOutput</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Sample</h2>

<p>Get the code from our GitHub repository <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> and build the project inside the <code>spark-samples</code> directory. For running the examples, you do not need any pre-installed Hadoop/Spark clusters or anything else.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/sequenceiq/sequenceiq-samples.git
</span><span class='line'><span class="nb">cd </span>sequenceiq-samples/spark-samples/
</span><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>All the other RDD operations are covered in the example (makes no sense listing them here).</p>

<h2>Spark on YARN</h2>

<p>Should you want to run your Spark code on a YARN cluster you have several options.</p>

<ul>
<li>Use our Spark Docker <a href="https://github.com/sequenceiq/docker-spark">container</a></li>
<li>Use our multi-node Hadoop <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">cluster</a></li>
<li>Use <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to provision a YARN cluster on your favorite cloud provider</li>
</ul>


<p>In order to help you get on going with Spark on YARN read our previous blog post about how to <a href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/">submit a Spark</a> job into a cluster.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cascading on Apache Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez/"/>
    <updated>2014-10-20T16:00:00+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/10/20/cascading-on-tez</id>
    <content type="html"><![CDATA[<p>In one of our previous <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez/">posts</a> we showed how to do a TopK using directly the Apache Tez API. In this post we’d like to show how to do a similarly complex algorithm with Cascading &ndash; running on Apache Tez.
At <a href="http://sequenceiq.com">SequenceIQ</a> we use Scalding, Cascading and Spark to write most of our jobs. For a while our big data pipeline API called <a href="http://docs.banzai.apiary.io/">Banzai Pipeline</a> offers a unified API over different runtimes: MR2, Spark and Tez; recently Cascading has announced support for Apache Tez and we’d like to show you that by writing a detailed example.</p>

<h2>Cascading Application &ndash; GroupBy, Each, Every</h2>

<p>Cascading data flows are to be constructed from Source taps (input), Sink taps (output) and Pipes.
At first, we have to setup our properties for the Cascading flow.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="n">AppProps</span><span class="o">.</span><span class="na">appProps</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setJarClass</span><span class="o">(</span><span class="n">Main</span><span class="o">.</span><span class="na">class</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">buildProperties</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">properties</span> <span class="o">=</span> <span class="n">FlowRuntimeProps</span><span class="o">.</span><span class="na">flowRuntimeProps</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setGatherPartitions</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">buildProperties</span><span class="o">(</span><span class="n">properties</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then in order to use Apache Tez, setup the Tez specific <code>Flow Connector</code>.</p>

<!-- more -->




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">FlowConnector</span> <span class="n">flowConnector</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hadoop2TezFlowConnector</span><span class="o">(</span><span class="n">properties</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>After that we do the algorithm part of the flow. We need an input and output which comes as command-line arguments.
We are going to work on CSV files for the sake of simplicity, so we will use the <code>TextDelimited</code> scheme. Also we need to define our input pipe and taps (<code>source/sink</code>).
Suppose that we want to count the occurrences of users and keep them only if they occur more than once. We can compute this with 2 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N205A3">GroupBy</a>, 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Every</a> and 1 <a href="http://docs.cascading.org/cascading/2.5/userguide/html/ch03s03.html#N20438">Each</a> operation.
First, we group by user ids (count them with every operation), then in the second grouping we need to sort on the whole data set (by <code>count</code>) and use the <a href="http://docs.cascading.org/cascading/2.5/javadoc/cascading/operation/Filter.html">Filter</a> operation to remove the unneeded lines. (here we grouping by <code>Fields.NONE</code>, that means we take all data into 1 group, in other words we force to use 1 reducer)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="kd">final</span> <span class="n">String</span> <span class="n">inputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">String</span> <span class="n">outputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">fields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">,</span> <span class="s">&quot;data1&quot;</span><span class="o">,</span> <span class="s">&quot;data2&quot;</span><span class="o">,</span> <span class="s">&quot;data3&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Scheme</span> <span class="n">scheme</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TextDelimited</span><span class="o">(</span><span class="n">fields</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Pipe</span> <span class="n">inPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Pipe</span><span class="o">(</span><span class="s">&quot;inPipe&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Tap</span> <span class="n">inTap</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hfs</span><span class="o">(</span><span class="n">scheme</span><span class="o">,</span> <span class="n">inputPath</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">groupFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Pipe</span> <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GroupBy</span><span class="o">(</span><span class="s">&quot;usersWithCount&quot;</span><span class="o">,</span> <span class="n">inPipe</span><span class="o">,</span> <span class="n">groupFields</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Every</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">groupFields</span><span class="o">,</span> <span class="k">new</span> <span class="n">Count</span><span class="o">(),</span> <span class="n">Fields</span><span class="o">.</span><span class="na">ALL</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GroupBy</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">Fields</span><span class="o">.</span><span class="na">NONE</span><span class="o">,</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="s">&quot;userId&quot;</span><span class="o">),</span> <span class="kc">true</span><span class="o">);</span>
</span><span class='line'>    <span class="n">usersPipe</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Each</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">),</span> <span class="k">new</span> <span class="n">RegexFilter</span><span class="o">(</span> <span class="s">&quot;^(?:[2-9]|(?:[1-9][0-9]+))&quot;</span> <span class="o">));</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Fields</span> <span class="n">resultFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">&quot;userId&quot;</span><span class="o">,</span> <span class="s">&quot;count&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="kd">final</span> <span class="n">Scheme</span> <span class="n">outputScheme</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TextDelimited</span><span class="o">(</span><span class="n">resultFields</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="n">Tap</span> <span class="n">sinkTap</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Hfs</span><span class="o">(</span><span class="n">outputScheme</span><span class="o">,</span> <span class="n">outputPath</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Finally, setup the flow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="n">FlowDef</span> <span class="n">flowDef</span> <span class="o">=</span> <span class="n">FlowDef</span><span class="o">.</span><span class="na">flowDef</span><span class="o">()</span>
</span><span class='line'>            <span class="o">.</span><span class="na">setName</span><span class="o">(</span><span class="s">&quot;Cascading-TEZ&quot;</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">inPipe</span><span class="o">,</span> <span class="n">inTap</span><span class="o">)</span>
</span><span class='line'>            <span class="o">.</span><span class="na">addTailSink</span><span class="o">(</span><span class="n">usersPipe</span><span class="o">,</span> <span class="n">sinkTap</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Flow</span> <span class="n">flow</span> <span class="o">=</span> <span class="n">flowConnector</span><span class="o">.</span><span class="na">connect</span><span class="o">(</span><span class="n">flowDef</span><span class="o">);</span>
</span><span class='line'>    <span class="n">flow</span><span class="o">.</span><span class="na">complete</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see the codebase is a bit simpler than using directly the Apache Tez API, however you loose the low level features of the expressive data flow API. Basically it&rsquo;s up to the personal preference of a developer whether to use and build directly on top of the Tez API or use Cascading (we have our own internal debate among colleagues) &ndash; as Apache Tez improves the performance by multiple times.</p>

<p>Get the code from our GitHub repository <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub examples</a> and build the project inside the <code>cascading-tez-sample</code> directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>Once your jar is ready upload it onto a Tez cluster and run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>hadoop jar cascading-tez-sample-1.0.jar /input /output
</span></code></pre></td></tr></table></div></figure>


<p>Sample data can be generated in the same way as in <a href="http://blog.sequenceiq.com/blog/2014/09/23/topn-on-apache-tez">this</a> example.</p>

<p>We have put together a Tez enabled Docker container, you can get it from <a href="https://github.com/sequenceiq/docker-tez">here</a>. Pull the container, and follow the instructions.</p>

<p>If you have any questions or suggestions you can reach us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
